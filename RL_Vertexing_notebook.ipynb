{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import Intersection_finder_absoluteCoordinates_Module as I\n",
    "import VertexObject as VO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"DQN class with h input nodes and output output nodes\"\"\"\n",
    "    def __init__(self, h, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fcn1 = nn.Linear(h,512)\n",
    "        self.fcn2 = nn.Linear(512,256)\n",
    "        self.fcn3 = nn.Linear(256,128)\n",
    "        self.fcn4 = nn.Linear(128,64)\n",
    "        #self.fcn6 = nn.Linear(64,10)\n",
    "        self.fcn5 = nn.Linear(64,outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fcn1(x))\n",
    "        x = F.relu(self.fcn2(x))\n",
    "        x = F.relu(self.fcn3(x))\n",
    "        x = F.relu(self.fcn4(x))\n",
    "        x = self.fcn5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def select_action_DQN(state):\n",
    "    \"\"\"Selects an action either based on policy or randomly\"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    a = 0\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # return the index of the max in output tensor\n",
    "            state = state.to(device)\n",
    "            a = (policy_net(state.flatten()).argsort())\n",
    "    else:\n",
    "        # return random bool\n",
    "        a = np.random.choice(n_actions, n_actions,replace=False)\n",
    "    #print(\"Action selected:\", a)\n",
    "    return [int(i) for i in a]\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Inits weights of m by random for linear layers\"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "        m.bias.data.fill_(0.00)\n",
    "\n",
    "# maybe change reward such that it can be calculated after the experience\n",
    "# this way training can be made faster by keeping experiences\n",
    "# num steps is a bad thing as it is not         \n",
    "def optimise_model_memory(batch_size):\n",
    "    \"\"\"\n",
    "    This function performs one training step on policy net.\n",
    "    \n",
    "    State action value Q(s,a) is compared to r + Q_t(s',a')\n",
    "    and a step of the optimiser is taken.\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    action_batch = []\n",
    "    batch_size = min(len(memory), batch_size)\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        x_batch.append(state.flatten())\n",
    "        state = state.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        y_target = reward if done else reward + gamma * float((target_net(next_state.flatten()).max()).to('cpu'))\n",
    "        action_batch.append(int(action))\n",
    "        y_batch.append(y_target)\n",
    "        #.type(torch.FloatTensor)\n",
    "    with torch.no_grad():\n",
    "        x_batch = torch.cat(x_batch).reshape((batch_size, n_inputs)).to(device)\n",
    "        y_batch = torch.tensor(y_batch).type(torch.FloatTensor).squeeze().to(device)\n",
    "        action_batch = torch.tensor(action_batch).reshape((batch_size,1)).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = policy_net(x_batch).reshape((batch_size, n_actions))\n",
    "    out = out.gather(1, action_batch).squeeze()\n",
    "    loss = F.smooth_l1_loss(out, y_batch)\n",
    "    #loss = F.mse_loss(out, y_batch)\n",
    "    loss.backward()\n",
    "    # clip error values to values between -1 and 1\n",
    "    #for param in policy_net.parameters():\n",
    "     #   param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# set up training as testing data\n",
    "\n",
    "# load data in torch format\n",
    "X_data = np.load('PFC_data_pocas_wrtJ.npy')\n",
    "y_data = np.load('SV_true_pocas_wrtJ.npy')\n",
    "print(\"Data loaded\")\n",
    "np.load = np_load_old\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PFCatts = 'pt', 'eta', 'phi', 'charge', 'dxy', 'dz', 'pv_x', 'pv_y', 'pv_z',\n",
    "\n",
    "            0     1     2        3          4    5    6         7       8 \n",
    "            \n",
    "           'theta', 'chi2', 'normalizedChi2','ndof', 'nPixelHits', 'deltaEta',\n",
    "           \n",
    "              9        10            11         12       13            14\n",
    "              \n",
    "           'deltaPhi', 'jetPtFrac', \n",
    "           \n",
    "                15            16 \n",
    "                \n",
    "           'ptError', 'etaError', 'phiError',  'dxyError', 'dzError'\n",
    "           \n",
    "               17          18            19         20         21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jets: (70191, 8, 17)\n",
      "[46976 50157 16081 ... 37422  8365 41504]\n",
      "Number of jets: (70191, 8, 17)\n"
     ]
    }
   ],
   "source": [
    "num_pfc_cut = 8 # maxp\n",
    "# do not use the most complicated parameters\n",
    "X_data = X_data[:,:num_pfc_cut,:17] # do not include error for track params\n",
    "print(f\"Number of jets: {X_data.shape}\")\n",
    "\n",
    "# assign training and testing data\n",
    "# randomize data\n",
    "idx = np.random.choice(X_data.shape[0], X_data.shape[0],replace=False)\n",
    "print(idx)\n",
    "X_data = X_data[idx]\n",
    "y_data = y_data[idx]\n",
    "\n",
    "print(f\"Number of jets: {X_data.shape}\")\n",
    "# split data to training, testing, validation data\n",
    "X_train = X_data[:50000]\n",
    "y_train = y_data[:50000]\n",
    "\n",
    "X_test = X_data[50000:60000]\n",
    "y_test = y_data[50000:60000]\n",
    "if len(X_test) == 0:\n",
    "    print(\"To much training data\")\n",
    "\n",
    "X_val = X_data[60000:]\n",
    "y_val = y_data[60000:]\n",
    "if len(X_val) == 0:\n",
    "    print(\"To much testing data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# actions: 17\n",
      "# inputs: 80\n"
     ]
    }
   ],
   "source": [
    "#print(X_train.shape)\n",
    "Env_test = VO.TrackEnvironment(X_train[0])\n",
    "n_actions = 1 + 2 * Env_test.state.shape[0]\n",
    "print(f\"# actions: {n_actions}\")\n",
    "\n",
    "n_inputs = Env_test.state.shape[0] * Env_test.state.shape[1]\n",
    "print(f\"# inputs: {n_inputs}\")\n",
    "\n",
    "\n",
    "#for i in range(X_train[:50].shape[0]):\n",
    " #   print(X_train[i, :5, :14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN initialised\n"
     ]
    }
   ],
   "source": [
    "memory = []\n",
    "\n",
    "policy_net = DQN(n_inputs, n_actions).to(device)\n",
    "target_net = DQN(n_inputs, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "#target_net.load_state_dict(nn.init.zeros_(target_net.weight.size()))\n",
    "optimizer = optim.Adam(policy_net.parameters(),lr=0.01)\n",
    "\n",
    "print(\"NN initialised\")\n",
    "\n",
    "gamma = 0\n",
    "GAMMA = 0.4 #0.9\n",
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 10000\n",
    "steps_done = 0\n",
    "\n",
    "MINI_BATCH = 200\n",
    "TARGET_UPDATE = 100\n",
    "\n",
    "epochs = 1\n",
    "num_episodes = 1000 #number of jets to train on\n",
    "num_test_episodes = 500# number of jets to test on\n",
    "max_episode_length = 25 #when to end a vertex attempt\n",
    "run_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnv = VO.TrackEnvironment(X_train[12000])\\ny_true = y_train[12000,:3]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Env = VO.TrackEnvironment(X_train[12000])\n",
    "y_true = y_train[12000,:3]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnv.take_action(2)\\noptimise_model_memory(MINI_BATCH)\\nx = Env.vertex.x\\nprint(Env.state)\\nprint(Env.vertex.track_indices)\\nprint(LA.norm(x-y_true))\\nprint(policy_net(Env.state.flatten().to(device)))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Env.take_action(2)\n",
    "optimise_model_memory(MINI_BATCH)\n",
    "x = Env.vertex.x\n",
    "print(Env.state)\n",
    "print(Env.vertex.track_indices)\n",
    "print(LA.norm(x-y_true))\n",
    "print(policy_net(Env.state.flatten().to(device)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.hist([memory[i][2] for i in range(len(memory))], bins=np.arange(-2500, 5, 100))\\nplt.show()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.hist([memory[i][2] for i in range(len(memory))], bins=np.arange(-2500, 5, 100))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimise_model_memory(MINI_BATCH)\\nprint(policy_net(Env.state.flatten().to(device)))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "optimise_model_memory(MINI_BATCH)\n",
    "print(policy_net(Env.state.flatten().to(device)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif reward > -9:\\n    reward = 1\\n    if dflag:\\n        reward += 1\\nelse:\\n    reward = -1\\n    if type(vertex_x) != np.ndarray:\\n        reward -= 1\\n    if pflag:\\n        reward -= 20\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            # clip rewards\n",
    "            \"\"\"\n",
    "            if reward > -9:\n",
    "                reward = 1\n",
    "                if dflag:\n",
    "                    reward += 1\n",
    "            else:\n",
    "                reward = -1\n",
    "                if type(vertex_x) != np.ndarray:\n",
    "                    reward -= 1\n",
    "                if pflag:\n",
    "                    reward -= 20\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Currently at event: 0\n",
      "Steps done: 0\n",
      "SV: [ 0.04980092 -0.1992443   0.93090886]\n",
      "Reward for action 4: -128.947696593653\n",
      "[0, 1, 4]\n",
      "Steps done: 1\n",
      "SV: [ 0.04980092 -0.1992443   0.93090886]\n",
      "Reward for action 20: -124.947696593653\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 1\n",
      "Steps done: 2\n",
      "SV: [ 1.0124997  0.745912  -4.073312 ]\n",
      "Reward for action 20: -58.17350442404067\n",
      "[0, 1]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 2\n",
      "Steps done: 3\n",
      "SV: [-1.6873754   0.70178646 -0.39066747]\n",
      "Reward for action 3: -107.22534390039922\n",
      "[0, 1, 3]\n",
      "Steps done: 4\n",
      "SV: [-1.6873754   0.70178646 -0.39066747]\n",
      "Reward for action 10: -285.5330813573991\n",
      "[1, 3]\n",
      "Steps done: 5\n",
      "SV: [-1.6873754   0.70178646 -0.39066747]\n",
      "Reward for action 2: -114.67580943403297\n",
      "[1, 3, 2]\n",
      "Steps done: 6\n",
      "SV: [-1.6873754   0.70178646 -0.39066747]\n",
      "Reward for action 0: -87.05162403596232\n",
      "[1, 3, 2, 0]\n",
      "Steps done: 7\n",
      "SV: [-1.6873754   0.70178646 -0.39066747]\n",
      "Reward for action 11: -179.42440011492783\n",
      "[3, 2, 0]\n",
      "Steps done: 8\n",
      "SV: [-1.6873754   0.70178646 -0.39066747]\n",
      "Reward for action 20: -175.42440011492783\n",
      "[3, 2, 0]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 3\n",
      "Steps done: 9\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 3: -41.27429470672398\n",
      "[0, 1, 3]\n",
      "Steps done: 10\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 8: -6.3540345020975915\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 11\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 7: -13.519178458446666\n",
      "[0, 1, 3, 8, 7]\n",
      "Steps done: 12\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 11: -13.081069424513478\n",
      "[0, 3, 8, 7]\n",
      "Steps done: 13\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 9: -20.621046746860795\n",
      "[0, 3, 8, 7, 9]\n",
      "Steps done: 14\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 4: -16.482354318674332\n",
      "[0, 3, 8, 7, 9, 4]\n",
      "Steps done: 15\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 5: -16.753630931051315\n",
      "[0, 3, 8, 7, 9, 4, 5]\n",
      "Steps done: 16\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 18: -29.2309323001849\n",
      "[0, 3, 7, 9, 4, 5]\n",
      "Steps done: 17\n",
      "SV: [-0.944534    0.71097916 -0.1539724 ]\n",
      "Reward for action 20: -25.2309323001849\n",
      "[0, 3, 7, 9, 4, 5]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 4\n",
      "Steps done: 18\n",
      "SV: [ 0.13795798 -0.02740638  0.38653708]\n",
      "Reward for action 20: 0.314179476023976\n",
      "[0, 1]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 5\n",
      "Steps done: 19\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 4: -74.18060626287847\n",
      "[0, 1, 4]\n",
      "Steps done: 20\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 2: -174.31335459761914\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 21\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 11: -706.4100567688583\n",
      "[0, 4, 2]\n",
      "Steps done: 22\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 12: -748.0602220995814\n",
      "[0, 4]\n",
      "Steps done: 23\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 1: -74.18060626287847\n",
      "[0, 4, 1]\n",
      "Steps done: 24\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 10: -741.7514398225599\n",
      "[4, 1]\n",
      "Steps done: 25\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 3: -18.039508470255928\n",
      "[4, 1, 3]\n",
      "Steps done: 26\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 0: -12.670573328727144\n",
      "[4, 1, 3, 0]\n",
      "Steps done: 27\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 13: -74.18060626287847\n",
      "[4, 1, 0]\n",
      "Steps done: 28\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 14: -845.9532814394859\n",
      "[1, 0]\n",
      "Steps done: 29\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 3: -661.7032512821172\n",
      "[1, 0, 3]\n",
      "Steps done: 30\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 2: -158.63016992536572\n",
      "[1, 0, 3, 2]\n",
      "Steps done: 31\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 4: -7.211675314182028\n",
      "[1, 0, 3, 2, 4]\n",
      "Steps done: 32\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 10: -11.928913230791863\n",
      "[1, 3, 2, 4]\n",
      "Steps done: 33\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 14: -253.85635849777208\n",
      "[1, 3, 2]\n",
      "Steps done: 34\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 12: -581.6373873921838\n",
      "[1, 3]\n",
      "Steps done: 35\n",
      "SV: [-1.1487257   0.24927182 -3.1033444 ]\n",
      "Reward for action 20: -577.6373873921838\n",
      "[1, 3]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 6\n",
      "Steps done: 36\n",
      "SV: [-0.11730007 -0.00056272  0.4210398 ]\n",
      "Reward for action 20: -11.130113143408689\n",
      "[0, 1]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 7\n",
      "Steps done: 37\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 5: -1.151470077635069\n",
      "[0, 1, 5]\n",
      "Steps done: 38\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 2: -0.6191632611922488\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 39\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 10: -0.9292396879815865\n",
      "[1, 5, 2]\n",
      "Steps done: 40\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 15: -0.5205093357781906\n",
      "[1, 2]\n",
      "Steps done: 41\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 4: -8.150730435743721\n",
      "[1, 2, 4]\n",
      "Steps done: 42\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 9: -5.791191499288245\n",
      "[1, 2, 4, 9]\n",
      "Steps done: 43\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 11: -14.26429535969956\n",
      "[2, 4, 9]\n",
      "Steps done: 44\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 14: -7.059188675558674\n",
      "[2, 9]\n",
      "Steps done: 45\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 1: -2.1019828024259453\n",
      "[2, 9, 1]\n",
      "Steps done: 46\n",
      "SV: [ 0.00899438  0.01251355 -0.170102  ]\n",
      "Reward for action 20: 1.8980171975740547\n",
      "[2, 9, 1]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 8\n",
      "Steps done: 47\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 6: -2.959123076192533\n",
      "[0, 1, 6]\n",
      "Steps done: 48\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 7: -7.776368616392019\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 49\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 16: -0.15069377593446273\n",
      "[0, 1, 7]\n",
      "Steps done: 50\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 4: -2.6751538342804926\n",
      "[0, 1, 7, 4]\n",
      "Steps done: 51\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 5: -15.08424856747937\n",
      "[0, 1, 7, 4, 5]\n",
      "Steps done: 52\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 15: -2.6751538342804926\n",
      "[0, 1, 7, 4]\n",
      "Steps done: 53\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 11: -1.0506233329263512\n",
      "[0, 7, 4]\n",
      "Steps done: 54\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 14: -3.2837779691324327\n",
      "[0, 7]\n",
      "Steps done: 55\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 6: -2.916545166724935\n",
      "[0, 7, 6]\n",
      "Steps done: 56\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 2: -1.7994091876133556\n",
      "[0, 7, 6, 2]\n",
      "Steps done: 57\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 5: -1.6673481074047438\n",
      "[0, 7, 6, 2, 5]\n",
      "Steps done: 58\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 17: -13.550003657332436\n",
      "[0, 6, 2, 5]\n",
      "Steps done: 59\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 1: -1.7801098410136655\n",
      "[0, 6, 2, 5, 1]\n",
      "Steps done: 60\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 11: -13.550003657332436\n",
      "[0, 6, 2, 5]\n",
      "Steps done: 61\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 16: -1.8074302353681033\n",
      "[0, 2, 5]\n",
      "Steps done: 62\n",
      "SV: [-0.22161023 -0.25354585 -0.15132336]\n",
      "Reward for action 20: 2.1925697646318967\n",
      "[0, 2, 5]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 9\n",
      "Steps done: 63\n",
      "SV: [-0.00302611  0.08215763 -0.53536016]\n",
      "Reward for action 9: -18.81197531027531\n",
      "[0, 1, 9]\n",
      "Steps done: 64\n",
      "SV: [-0.00302611  0.08215763 -0.53536016]\n",
      "Reward for action 11: -48.6474407850359\n",
      "[0, 9]\n",
      "Steps done: 65\n",
      "SV: [-0.00302611  0.08215763 -0.53536016]\n",
      "Reward for action 6: -38.45677930532608\n",
      "[0, 9, 6]\n",
      "Steps done: 66\n",
      "SV: [-0.00302611  0.08215763 -0.53536016]\n",
      "Reward for action 20: -34.45677930532608\n",
      "[0, 9, 6]\n",
      "Done flag\n",
      "Trying to plot\n",
      "Closed figure\n",
      "Episode ended naturally\n",
      "Currently at event: 10\n",
      "Steps done: 67\n",
      "SV: [-1.4882338  -0.59926367  1.2225192 ]\n",
      "Reward for action 2: -185.52662667890428\n",
      "[0, 1, 2]\n",
      "Steps done: 68\n",
      "SV: [-1.4882338  -0.59926367  1.2225192 ]\n",
      "Reward for action 10: -184.2164714430882\n",
      "[1, 2]\n",
      "Steps done: 69\n",
      "SV: [-1.4882338  -0.59926367  1.2225192 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 0: -185.52662667890422\n",
      "[1, 2, 0]\n",
      "Steps done: 70\n",
      "SV: [-1.4882338  -0.59926367  1.2225192 ]\n",
      "Reward for action 20: -181.52662667890422\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 11\n",
      "Steps done: 71\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 4: -36.55496555307452\n",
      "[0, 1, 4]\n",
      "Steps done: 72\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 11: -54.72613227223304\n",
      "[0, 4]\n",
      "Steps done: 73\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 1: -36.55496555307452\n",
      "[0, 4, 1]\n",
      "Steps done: 74\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 11: -54.72613227223304\n",
      "[0, 4]\n",
      "Steps done: 75\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 1: -36.55496555307452\n",
      "[0, 4, 1]\n",
      "Steps done: 76\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 2: -35.23906011402096\n",
      "[0, 4, 1, 2]\n",
      "Steps done: 77\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 14: -29.906642867228854\n",
      "[0, 1, 2]\n",
      "Steps done: 78\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 11: -33.7707803352279\n",
      "[0, 2]\n",
      "Steps done: 79\n",
      "SV: [ 0.09742707 -0.16998705 -0.7996769 ]\n",
      "Reward for action 20: -29.770780335227897\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 12\n",
      "Steps done: 80\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 2: -68.44254203578932\n",
      "[0, 1, 2]\n",
      "Steps done: 81\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 12: -136.14988499865728\n",
      "[0, 1]\n",
      "Steps done: 82\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 2: -68.44254203578932\n",
      "[0, 1, 2]\n",
      "Steps done: 83\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 12: -136.14988499865728\n",
      "[0, 1]\n",
      "Steps done: 84\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 2: -68.44254203578932\n",
      "[0, 1, 2]\n",
      "Steps done: 85\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 10: -50.50525841302288\n",
      "[1, 2]\n",
      "Steps done: 86\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 0: -68.44254203578932\n",
      "[1, 2, 0]\n",
      "Steps done: 87\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 10: -50.50525841302288\n",
      "[1, 2]\n",
      "Steps done: 88\n",
      "SV: [-0.2832068   0.42393312 -1.374586  ]\n",
      "Reward for action 20: -46.50525841302288\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 13\n",
      "Steps done: 89\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 7: -128.85811408075756\n",
      "[0, 1, 7]\n",
      "Steps done: 90\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 10: -298.74243467582926\n",
      "[1, 7]\n",
      "Steps done: 91\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 5: -331.57717089839934\n",
      "[1, 7, 5]\n",
      "Steps done: 92\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 11: -310.7034877250899\n",
      "[7, 5]\n",
      "Steps done: 93\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 6: -156.00061515440999\n",
      "[7, 5, 6]\n",
      "Steps done: 94\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 4: -197.2112884303371\n",
      "[7, 5, 6, 4]\n",
      "Steps done: 95\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 17: -207.0108556933096\n",
      "[5, 6, 4]\n",
      "Steps done: 96\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 15: -160.6721291135251\n",
      "[6, 4]\n",
      "Steps done: 97\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 0: -90.23277055538657\n",
      "[6, 4, 0]\n",
      "Steps done: 98\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 14: -144.30773526959916\n",
      "[6, 0]\n",
      "Steps done: 99\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 1: -94.82498386228144\n",
      "[6, 0, 1]\n",
      "Did target update\n",
      "Steps done: 100\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 2: -154.93745131053632\n",
      "[6, 0, 1, 2]\n",
      "Steps done: 101\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 11: -211.8875381076326\n",
      "[6, 0, 2]\n",
      "Steps done: 102\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 4: -149.32453993286018\n",
      "[6, 0, 2, 4]\n",
      "Steps done: 103\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 5: -207.26925070354122\n",
      "[6, 0, 2, 4, 5]\n",
      "Steps done: 104\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 16: -276.17646809436167\n",
      "[0, 2, 4, 5]\n",
      "Steps done: 105\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 14: -458.56190180387006\n",
      "[0, 2, 5]\n",
      "Steps done: 106\n",
      "SV: [-0.8519184  0.4980475 -1.9515046]\n",
      "Reward for action 20: -454.56190180387006\n",
      "[0, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 14\n",
      "Steps done: 107\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 2: -79.14221272018067\n",
      "[0, 1, 2]\n",
      "Steps done: 108\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 4: -27.595435796386106\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 109\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 7: -33.175201323087194\n",
      "[0, 1, 2, 4, 7]\n",
      "Steps done: 110\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 14: -59.55541397228078\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 111\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 17: -79.14221272018067\n",
      "[0, 1, 2]\n",
      "Steps done: 112\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 5: -102.79004079825913\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 113\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 9: -99.60264831188402\n",
      "[0, 1, 2, 5, 9]\n",
      "Steps done: 114\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 4: -95.53091799679088\n",
      "[0, 1, 2, 5, 9, 4]\n",
      "Steps done: 115\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 10: -57.02246277295894\n",
      "[1, 2, 5, 9, 4]\n",
      "Steps done: 116\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 15: -32.91212644307629\n",
      "[1, 2, 9, 4]\n",
      "Steps done: 117\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 3: -43.56612857680378\n",
      "[1, 2, 9, 4, 3]\n",
      "Steps done: 118\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 0: -59.33167691082825\n",
      "[1, 2, 9, 4, 3, 0]\n",
      "Steps done: 119\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 12: -44.156457955678064\n",
      "[1, 9, 4, 3, 0]\n",
      "Steps done: 120\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 5: -71.1158903599227\n",
      "[1, 9, 4, 3, 0, 5]\n",
      "Steps done: 121\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 6: -90.39079657777755\n",
      "[1, 9, 4, 3, 0, 5, 6]\n",
      "Steps done: 122\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 10: -81.19513917642112\n",
      "[1, 9, 4, 3, 5, 6]\n",
      "Steps done: 123\n",
      "SV: [-0.19001588  0.5220682   0.6289986 ]\n",
      "Reward for action 20: -77.19513917642112\n",
      "[1, 9, 4, 3, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 15\n",
      "Steps done: 124\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 7: -4.2926310071205185\n",
      "[0, 1, 7]\n",
      "Steps done: 125\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 11: -208.4134079615382\n",
      "[0, 7]\n",
      "Steps done: 126\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 8: -22.66720634977165\n",
      "[0, 7, 8]\n",
      "Steps done: 127\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 4: -4.8759632757253275\n",
      "[0, 7, 8, 4]\n",
      "Steps done: 128\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 6: -3.238668265790065\n",
      "[0, 7, 8, 4, 6]\n",
      "Steps done: 129\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 9: -2.719455626233654\n",
      "[0, 7, 8, 4, 6, 9]\n",
      "Steps done: 130\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 16: -3.4466746996030393\n",
      "[0, 7, 8, 4, 9]\n",
      "Steps done: 131\n",
      "SV: [ 0.08876977 -0.14143293  0.26929018]\n",
      "Reward for action 20: 0.5533253003969607\n",
      "[0, 7, 8, 4, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 16\n",
      "Steps done: 132\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 4: -5.012193830064621\n",
      "[0, 1, 4]\n",
      "Steps done: 133\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 9: -6.119170209206629\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 134\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 2: -8.626214896369222\n",
      "[0, 1, 4, 9, 2]\n",
      "Steps done: 135\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 11: -7.468388315063335\n",
      "[0, 4, 9, 2]\n",
      "Steps done: 136\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 7: -6.080386856879047\n",
      "[0, 4, 9, 2, 7]\n",
      "Steps done: 137\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 8: -5.503147570263179\n",
      "[0, 4, 9, 2, 7, 8]\n",
      "Steps done: 138\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 1: -5.769883447653639\n",
      "[0, 4, 9, 2, 7, 8, 1]\n",
      "Steps done: 139\n",
      "SV: [ 0.15924281 -0.07670204  0.17795284]\n",
      "Reward for action 20: -1.7698834476536387\n",
      "[0, 4, 9, 2, 7, 8, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 17\n",
      "Steps done: 140\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 3: -4.201936371240027\n",
      "[0, 1, 3]\n",
      "Steps done: 141\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n",
      "Reward for action 11: -4.154110539568995\n",
      "[0, 3]\n",
      "Steps done: 142\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n",
      "Reward for action 8: -106.4958739583766\n",
      "[0, 3, 8]\n",
      "Steps done: 143\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n",
      "Reward for action 1: -53.967804083096986\n",
      "[0, 3, 8, 1]\n",
      "Steps done: 144\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n",
      "Reward for action 9: -1.1674643864907348\n",
      "[0, 3, 8, 1, 9]\n",
      "Steps done: 145\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n",
      "Reward for action 11: -0.49302413830094377\n",
      "[0, 3, 8, 9]\n",
      "Steps done: 146\n",
      "SV: [ 0.0452977   0.00456714 -0.22409272]\n",
      "Reward for action 20: 3.5069758616990563\n",
      "[0, 3, 8, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 18\n",
      "Steps done: 147\n",
      "SV: [-1.4376129 -1.2081318  9.19226  ]\n",
      "Reward for action 2: -415.00157206936217\n",
      "[0, 1, 2]\n",
      "Steps done: 148\n",
      "SV: [-1.4376129 -1.2081318  9.19226  ]\n",
      "Reward for action 12: -422.01127828852134\n",
      "[0, 1]\n",
      "Steps done: 149\n",
      "SV: [-1.4376129 -1.2081318  9.19226  ]\n",
      "Reward for action 20: -418.01127828852134\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 19\n",
      "Steps done: 150\n",
      "SV: [ 0.00610115 -0.05572874 -0.51457876]\n",
      "Reward for action 20: -15.681115994260999\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 20\n",
      "Steps done: 151\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 8: -60.12726804169983\n",
      "[0, 1, 8]\n",
      "Steps done: 152\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 4: -55.07771201482532\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 153\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 3: -64.02864586813476\n",
      "[0, 1, 8, 4, 3]\n",
      "Steps done: 154\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 6: -47.286221148207446\n",
      "[0, 1, 8, 4, 3, 6]\n",
      "Steps done: 155\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 7: -83.66386705068004\n",
      "[0, 1, 8, 4, 3, 6, 7]\n",
      "Steps done: 156\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 5: -89.68244243621669\n",
      "[0, 1, 8, 4, 3, 6, 7, 5]\n",
      "Steps done: 157\n",
      "SV: [ 0.233868   -0.02827935 -1.0396283 ]\n",
      "Reward for action 20: -85.68244243621669\n",
      "[0, 1, 8, 4, 3, 6, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 21\n",
      "Steps done: 158\n",
      "SV: [ 0.2938735  -0.48605523 -0.53570384]\n",
      "Reward for action 5: -110.33954228530888\n",
      "[0, 1, 5]\n",
      "Steps done: 159\n",
      "SV: [ 0.2938735  -0.48605523 -0.53570384]\n",
      "Reward for action 20: -106.33954228530888\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 22\n",
      "Steps done: 160\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 3: -53.512621687947856\n",
      "[0, 1, 3]\n",
      "Steps done: 161\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 4: -97.43971628094849\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 162\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 11: -159.49083435064435\n",
      "[0, 3, 4]\n",
      "Steps done: 163\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 13: -161.83407461253952\n",
      "[0, 4]\n",
      "Steps done: 164\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 2: -87.7173358828361\n",
      "[0, 4, 2]\n",
      "Steps done: 165\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 12: -161.83407461253952\n",
      "[0, 4]\n",
      "Steps done: 166\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 1: -55.88001978489797\n",
      "[0, 4, 1]\n",
      "Steps done: 167\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 2: -51.95290647827298\n",
      "[0, 4, 1, 2]\n",
      "Steps done: 168\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 12: -55.88001978489797\n",
      "[0, 4, 1]\n",
      "Steps done: 169\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 3: -97.43971628094849\n",
      "[0, 4, 1, 3]\n",
      "Steps done: 170\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 11: -159.4908343506443\n",
      "[0, 4, 3]\n",
      "Steps done: 171\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 10: -194.80960953559804\n",
      "[4, 3]\n",
      "Steps done: 172\n",
      "SV: [ 0.5310068  -0.41974193  0.17629075]\n",
      "Reward for action 20: -190.80960953559804\n",
      "[4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 23\n",
      "Steps done: 173\n",
      "SV: [-1.3051176  1.0992552  3.0617673]\n",
      "Reward for action 3: -8.124148734317028\n",
      "[0, 1, 3]\n",
      "Steps done: 174\n",
      "SV: [-1.3051176  1.0992552  3.0617673]\n",
      "Reward for action 13: -380.73079743456617\n",
      "[0, 1]\n",
      "Steps done: 175\n",
      "SV: [-1.3051176  1.0992552  3.0617673]\n",
      "Reward for action 20: -376.73079743456617\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 24\n",
      "Steps done: 176\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 2: -31.574604519921998\n",
      "[0, 1, 2]\n",
      "Steps done: 177\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 5: -30.33352367922902\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 178\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 6: -10.448231682920419\n",
      "[0, 1, 2, 5, 6]\n",
      "Steps done: 179\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 7: -15.537819543514676\n",
      "[0, 1, 2, 5, 6, 7]\n",
      "Steps done: 180\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 9: -16.36679633879196\n",
      "[0, 1, 2, 5, 6, 7, 9]\n",
      "Steps done: 181\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 11: -14.431555860090512\n",
      "[0, 2, 5, 6, 7, 9]\n",
      "Steps done: 182\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 3: -26.458252044883825\n",
      "[0, 2, 5, 6, 7, 9, 3]\n",
      "Steps done: 183\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 12: -24.252599203856498\n",
      "[0, 5, 6, 7, 9, 3]\n",
      "Steps done: 184\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 4: -23.135605423399\n",
      "[0, 5, 6, 7, 9, 3, 4]\n",
      "Steps done: 185\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 2: -25.040349933969345\n",
      "[0, 5, 6, 7, 9, 3, 4, 2]\n",
      "Steps done: 186\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 10: -22.81410294842442\n",
      "[5, 6, 7, 9, 3, 4, 2]\n",
      "Steps done: 187\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 19: -19.032990051068772\n",
      "[5, 6, 7, 3, 4, 2]\n",
      "Steps done: 188\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 17: -14.371919026402017\n",
      "[5, 6, 3, 4, 2]\n",
      "Steps done: 189\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 0: -22.83906086414097\n",
      "[5, 6, 3, 4, 2, 0]\n",
      "Steps done: 190\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 12: -23.13990178182226\n",
      "[5, 6, 3, 4, 0]\n",
      "Steps done: 191\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 7: -19.295482910107985\n",
      "[5, 6, 3, 4, 0, 7]\n",
      "Steps done: 192\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 8: -20.84747131756085\n",
      "[5, 6, 3, 4, 0, 7, 8]\n",
      "Steps done: 193\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 15: -29.721545845645107\n",
      "[6, 3, 4, 0, 7, 8]\n",
      "Steps done: 194\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 14: -39.89854507036882\n",
      "[6, 3, 0, 7, 8]\n",
      "Steps done: 195\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 2: -38.16744094316367\n",
      "[6, 3, 0, 7, 8, 2]\n",
      "Steps done: 196\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 10: -43.65100664626694\n",
      "[6, 3, 7, 8, 2]\n",
      "Steps done: 197\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 16: -95.18333647449558\n",
      "[3, 7, 8, 2]\n",
      "Steps done: 198\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 17: -69.7946526891441\n",
      "[3, 8, 2]\n",
      "Steps done: 199\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 18: -224.3041062671763\n",
      "[3, 2]\n",
      "Did target update\n",
      "Steps done: 200\n",
      "SV: [ 0.23018543 -0.08581384 -0.5948858 ]\n",
      "Reward for action 20: -220.3041062671763\n",
      "[3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 25\n",
      "Steps done: 201\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 8: -31.331273473934534\n",
      "[0, 1, 8]\n",
      "Steps done: 202\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 7: -10.915598086137864\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 203\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 5: -4.1467158721760935\n",
      "[0, 1, 8, 7, 5]\n",
      "Steps done: 204\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 18: -53.83212263393041\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 205\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 10: -101.71627461618137\n",
      "[1, 7, 5]\n",
      "Steps done: 206\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 17: -444.2499964757683\n",
      "[1, 5]\n",
      "Steps done: 207\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 7: -101.71627461618138\n",
      "[1, 5, 7]\n",
      "Steps done: 208\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 6: -15.020422987964249\n",
      "[1, 5, 7, 6]\n",
      "Steps done: 209\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 3: -11.104979603808765\n",
      "[1, 5, 7, 6, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 210\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 2: -4.2137237849762315\n",
      "[1, 5, 7, 6, 3, 2]\n",
      "Steps done: 211\n",
      "SV: [0.06548581 0.15702504 0.54994476]\n",
      "Reward for action 20: -0.21372378497623146\n",
      "[1, 5, 7, 6, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 26\n",
      "Steps done: 212\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 6: -53.063352978808915\n",
      "[0, 1, 6]\n",
      "Steps done: 213\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 3: -117.4317010650505\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 214\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 4: -139.61446805401158\n",
      "[0, 1, 6, 3, 4]\n",
      "Steps done: 215\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 11: -132.84081788505955\n",
      "[0, 6, 3, 4]\n",
      "Steps done: 216\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 1: -139.6144680540117\n",
      "[0, 6, 3, 4, 1]\n",
      "Steps done: 217\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 10: -111.81475178243245\n",
      "[6, 3, 4, 1]\n",
      "Steps done: 218\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 11: -73.4633331948049\n",
      "[6, 3, 4]\n",
      "Steps done: 219\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 5: -38.73799209466096\n",
      "[6, 3, 4, 5]\n",
      "Steps done: 220\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 16: -2.7027991951000487\n",
      "[3, 4, 5]\n",
      "Steps done: 221\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 6: -38.73799209466094\n",
      "[3, 4, 5, 6]\n",
      "Steps done: 222\n",
      "SV: [ 1.3549623 -0.2696617  3.575531 ]\n",
      "Reward for action 20: -34.73799209466094\n",
      "[3, 4, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 27\n",
      "Steps done: 223\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 2: -65.63230292615708\n",
      "[0, 1, 2]\n",
      "Steps done: 224\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 4: -51.14462445497217\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 225\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 3: -53.59785413600605\n",
      "[0, 1, 2, 4, 3]\n",
      "Steps done: 226\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 5: -52.333125588854415\n",
      "[0, 1, 2, 4, 3, 5]\n",
      "Steps done: 227\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 12: -51.72902792816578\n",
      "[0, 1, 4, 3, 5]\n",
      "Steps done: 228\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 13: -50.65401776376357\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 229\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 9: -50.05847555251575\n",
      "[0, 1, 4, 5, 9]\n",
      "Steps done: 230\n",
      "SV: [ 0.1518769   0.10109846 -0.7476591 ]\n",
      "Reward for action 20: -46.05847555251575\n",
      "[0, 1, 4, 5, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 28\n",
      "Steps done: 231\n",
      "SV: [-0.12286168  0.2884751  -0.88656837]\n",
      "Reward for action 3: -56.18419842370767\n",
      "[0, 1, 3]\n",
      "Steps done: 232\n",
      "SV: [-0.12286168  0.2884751  -0.88656837]\n",
      "Reward for action 4: -33.71125703405793\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 233\n",
      "SV: [-0.12286168  0.2884751  -0.88656837]\n",
      "Reward for action 20: -29.71125703405793\n",
      "[0, 1, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 29\n",
      "Steps done: 234\n",
      "SV: [ 0.12776671  0.07414518 -0.20856781]\n",
      "Reward for action 3: -5.022448914162648\n",
      "[0, 1, 3]\n",
      "Steps done: 235\n",
      "SV: [ 0.12776671  0.07414518 -0.20856781]\n",
      "Reward for action 2: -7.562872020208961\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 236\n",
      "SV: [ 0.12776671  0.07414518 -0.20856781]\n",
      "Reward for action 4: -6.384213649286388\n",
      "[0, 1, 3, 2, 4]\n",
      "Steps done: 237\n",
      "SV: [ 0.12776671  0.07414518 -0.20856781]\n",
      "Reward for action 14: -7.562872020208961\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 238\n",
      "SV: [ 0.12776671  0.07414518 -0.20856781]\n",
      "Reward for action 20: -3.562872020208961\n",
      "[0, 1, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 30\n",
      "Steps done: 239\n",
      "SV: [-1.738443   0.924008   1.2829458]\n",
      "Reward for action 20: -223.01748462714968\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 31\n",
      "Steps done: 240\n",
      "SV: [0.431607   0.27834162 1.3420405 ]\n",
      "Reward for action 20: -640.3712324317835\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 32\n",
      "Steps done: 241\n",
      "SV: [-1.3176852  0.446767  -4.92559  ]\n",
      "Reward for action 20: -566.7543450159009\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 33\n",
      "Steps done: 242\n",
      "SV: [ 0.00787942  0.01196349 -0.34007877]\n",
      "Reward for action 20: -5.394886716287715\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 34\n",
      "Steps done: 243\n",
      "SV: [ 0.11934531 -0.62448496  1.8913933 ]\n",
      "Reward for action 4: -19.22269087391057\n",
      "[0, 1, 4]\n",
      "Steps done: 244\n",
      "SV: [ 0.11934531 -0.62448496  1.8913933 ]\n",
      "Reward for action 6: -52.3310499048647\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 245\n",
      "SV: [ 0.11934531 -0.62448496  1.8913933 ]\n",
      "Reward for action 3: -84.84196722862974\n",
      "[0, 1, 4, 6, 3]\n",
      "Steps done: 246\n",
      "SV: [ 0.11934531 -0.62448496  1.8913933 ]\n",
      "Reward for action 7: -71.40586883647816\n",
      "[0, 1, 4, 6, 3, 7]\n",
      "Steps done: 247\n",
      "SV: [ 0.11934531 -0.62448496  1.8913933 ]\n",
      "Reward for action 20: -67.40586883647816\n",
      "[0, 1, 4, 6, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 35\n",
      "Steps done: 248\n",
      "SV: [ 0.21944983 -0.22105215  0.16865455]\n",
      "Reward for action 6: -7.772165700791548\n",
      "[0, 1, 6]\n",
      "Steps done: 249\n",
      "SV: [ 0.21944983 -0.22105215  0.16865455]\n",
      "Reward for action 5: -5.166995442006593\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 250\n",
      "SV: [ 0.21944983 -0.22105215  0.16865455]\n",
      "Reward for action 7: -1.7304566483314852\n",
      "[0, 1, 6, 5, 7]\n",
      "Steps done: 251\n",
      "SV: [ 0.21944983 -0.22105215  0.16865455]\n",
      "Reward for action 20: 2.269543351668515\n",
      "[0, 1, 6, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 36\n",
      "Steps done: 252\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 4: -34.372778526748654\n",
      "[0, 1, 4]\n",
      "Steps done: 253\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 11: -50.457598041577\n",
      "[0, 4]\n",
      "Steps done: 254\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 7: -29.23025913180996\n",
      "[0, 4, 7]\n",
      "Steps done: 255\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 14: -40.25983082884188\n",
      "[0, 7]\n",
      "Steps done: 256\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 1: -27.533791371581113\n",
      "[0, 7, 1]\n",
      "Steps done: 257\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 10: -37.906069583306845\n",
      "[7, 1]\n",
      "Steps done: 258\n",
      "SV: [-0.02749696  0.08830502 -0.4625488 ]\n",
      "Reward for action 20: -33.906069583306845\n",
      "[7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 37\n",
      "Steps done: 259\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 4: -271.5091221922246\n",
      "[0, 1, 4]\n",
      "Steps done: 260\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 8: -58.87482559038125\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 261\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 14: -23.75289854099224\n",
      "[0, 1, 8]\n",
      "Steps done: 262\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 4: -58.87482559038125\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 263\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 10: -13.913869058208782\n",
      "[1, 8, 4]\n",
      "Steps done: 264\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 7: -15.08482983106944\n",
      "[1, 8, 4, 7]\n",
      "Steps done: 265\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 17: -13.913869058208782\n",
      "[1, 8, 4]\n",
      "Steps done: 266\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 3: -23.347377551511936\n",
      "[1, 8, 4, 3]\n",
      "Steps done: 267\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 2: -96.6750807062322\n",
      "[1, 8, 4, 3, 2]\n",
      "Steps done: 268\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 0: -101.77271644345825\n",
      "[1, 8, 4, 3, 2, 0]\n",
      "Steps done: 269\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 9: -64.34835500782269\n",
      "[1, 8, 4, 3, 2, 0, 9]\n",
      "Steps done: 270\n",
      "SV: [ 1.3231487  -0.91094416  1.6427164 ]\n",
      "Reward for action 20: -60.34835500782269\n",
      "[1, 8, 4, 3, 2, 0, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 38\n",
      "Steps done: 271\n",
      "SV: [ 0.15247466 -0.03618934  0.50964236]\n",
      "Reward for action 6: -41.08007811169807\n",
      "[0, 1, 6]\n",
      "Steps done: 272\n",
      "SV: [ 0.15247466 -0.03618934  0.50964236]\n",
      "Reward for action 20: -37.08007811169807\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 39\n",
      "Steps done: 273\n",
      "SV: [-1.0091045   0.3392847   0.39310244]\n",
      "Reward for action 5: -50.82110906581006\n",
      "[0, 1, 5]\n",
      "Steps done: 274\n",
      "SV: [-1.0091045   0.3392847   0.39310244]\n",
      "Reward for action 4: -69.74433512628305\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 275\n",
      "SV: [-1.0091045   0.3392847   0.39310244]\n",
      "Reward for action 20: -65.74433512628305\n",
      "[0, 1, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 40\n",
      "Steps done: 276\n",
      "SV: [-0.05954222  0.04734319  0.1673675 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 5: -15.522282827734582\n",
      "[0, 1, 5]\n",
      "Steps done: 277\n",
      "SV: [-0.05954222  0.04734319  0.1673675 ]\n",
      "Reward for action 20: -11.522282827734582\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 41\n",
      "Steps done: 278\n",
      "SV: [0.31600496 2.4327202  0.77756596]\n",
      "Reward for action 2: -190.78896060549332\n",
      "[0, 1, 2]\n",
      "Steps done: 279\n",
      "SV: [0.31600496 2.4327202  0.77756596]\n",
      "Reward for action 20: -186.78896060549332\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 42\n",
      "Steps done: 280\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 2: -1.2918152326276906\n",
      "[0, 1, 2]\n",
      "Steps done: 281\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 7: -148.87527577511554\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 282\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 5: -126.73470008342073\n",
      "[0, 1, 2, 7, 5]\n",
      "Steps done: 283\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 4: -58.04227083409765\n",
      "[0, 1, 2, 7, 5, 4]\n",
      "Steps done: 284\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 11: -50.840721961305405\n",
      "[0, 2, 7, 5, 4]\n",
      "Steps done: 285\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 8: -51.498788372312816\n",
      "[0, 2, 7, 5, 4, 8]\n",
      "Steps done: 286\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 18: -50.840721961305405\n",
      "[0, 2, 7, 5, 4]\n",
      "Steps done: 287\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 3: -48.77916712479534\n",
      "[0, 2, 7, 5, 4, 3]\n",
      "Steps done: 288\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 13: -50.840721961305405\n",
      "[0, 2, 7, 5, 4]\n",
      "Steps done: 289\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 8: -51.498788372312816\n",
      "[0, 2, 7, 5, 4, 8]\n",
      "Steps done: 290\n",
      "SV: [-0.18031482 -0.11144966  0.24770133]\n",
      "Reward for action 20: -47.498788372312816\n",
      "[0, 2, 7, 5, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 43\n",
      "Steps done: 291\n",
      "SV: [-0.21827658  0.29130992 -0.90365076]\n",
      "Reward for action 4: -18.56894473265514\n",
      "[0, 1, 4]\n",
      "Steps done: 292\n",
      "SV: [-0.21827658  0.29130992 -0.90365076]\n",
      "Reward for action 14: -52.89548920708524\n",
      "[0, 1]\n",
      "Steps done: 293\n",
      "SV: [-0.21827658  0.29130992 -0.90365076]\n",
      "Reward for action 2: -51.280975324173724\n",
      "[0, 1, 2]\n",
      "Steps done: 294\n",
      "SV: [-0.21827658  0.29130992 -0.90365076]\n",
      "Reward for action 5: -92.87572025183198\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 295\n",
      "SV: [-0.21827658  0.29130992 -0.90365076]\n",
      "Reward for action 20: -88.87572025183198\n",
      "[0, 1, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 44\n",
      "Steps done: 296\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 2: -0.8145027500016115\n",
      "[0, 1, 2]\n",
      "Steps done: 297\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 4: -0.9468255201440163\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 298\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 11: -1.6354011255525667\n",
      "[0, 2, 4]\n",
      "Steps done: 299\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 12: -1.4368559220215724\n",
      "[0, 4]\n",
      "Did target update\n",
      "Steps done: 300\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 2: -1.6354011255525662\n",
      "[0, 4, 2]\n",
      "Steps done: 301\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 10: -1.5668615173805143\n",
      "[4, 2]\n",
      "Steps done: 302\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 0: -1.6354011255525667\n",
      "[4, 2, 0]\n",
      "Steps done: 303\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 10: -1.5668615173805143\n",
      "[4, 2]\n",
      "Steps done: 304\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 0: -1.6354011255525667\n",
      "[4, 2, 0]\n",
      "Steps done: 305\n",
      "SV: [-0.0889805   0.12229308  0.04589564]\n",
      "Reward for action 20: 2.3645988744474336\n",
      "[4, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 45\n",
      "Steps done: 306\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 3: -264.68970476325364\n",
      "[0, 1, 3]\n",
      "Steps done: 307\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 6: -121.58708687631372\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 308\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 10: -95.16841494874586\n",
      "[1, 3, 6]\n",
      "Steps done: 309\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 4: -181.74855831389243\n",
      "[1, 3, 6, 4]\n",
      "Steps done: 310\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 0: -68.97223390456988\n",
      "[1, 3, 6, 4, 0]\n",
      "Steps done: 311\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 13: -157.98632720306892\n",
      "[1, 6, 4, 0]\n",
      "Steps done: 312\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 3: -68.97223390456988\n",
      "[1, 6, 4, 0, 3]\n",
      "Steps done: 313\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 16: -75.59515523636914\n",
      "[1, 4, 0, 3]\n",
      "Steps done: 314\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 13: -238.02705996507063\n",
      "[1, 4, 0]\n",
      "Steps done: 315\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 2: -70.92071512591502\n",
      "[1, 4, 0, 2]\n",
      "Steps done: 316\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 5: -19.38359238797437\n",
      "[1, 4, 0, 2, 5]\n",
      "Steps done: 317\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 11: -48.225999797210164\n",
      "[4, 0, 2, 5]\n",
      "Steps done: 318\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 15: -236.85754170681255\n",
      "[4, 0, 2]\n",
      "Steps done: 319\n",
      "SV: [ 0.8520717 -1.7534565 -3.3473957]\n",
      "Reward for action 20: -232.85754170681255\n",
      "[4, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 46\n",
      "Steps done: 320\n",
      "SV: [ 0.26169977 -0.35983288  1.796873  ]\n",
      "Reward for action 7: -305.04624057535216\n",
      "[0, 1, 7]\n",
      "Steps done: 321\n",
      "SV: [ 0.26169977 -0.35983288  1.796873  ]\n",
      "Reward for action 10: -811.3285125979256\n",
      "[1, 7]\n",
      "Steps done: 322\n",
      "SV: [ 0.26169977 -0.35983288  1.796873  ]\n",
      "Reward for action 20: -807.3285125979256\n",
      "[1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 47\n",
      "Steps done: 323\n",
      "SV: [ 0.14627121 -0.05360571  0.44970414]\n",
      "Reward for action 2: -11.653576784296522\n",
      "[0, 1, 2]\n",
      "Steps done: 324\n",
      "SV: [ 0.14627121 -0.05360571  0.44970414]\n",
      "Reward for action 12: -8.856349869548016\n",
      "[0, 1]\n",
      "Steps done: 325\n",
      "SV: [ 0.14627121 -0.05360571  0.44970414]\n",
      "Reward for action 20: -4.856349869548016\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 48\n",
      "Steps done: 326\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 7: -1.4364642007384667\n",
      "[0, 1, 7]\n",
      "Steps done: 327\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 2: -1.148711728479079\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 328\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 11: -1.1606364218468683\n",
      "[0, 7, 2]\n",
      "Steps done: 329\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 1: -1.1487117284790795\n",
      "[0, 7, 2, 1]\n",
      "Steps done: 330\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 4: -4.520144563138738\n",
      "[0, 7, 2, 1, 4]\n",
      "Steps done: 331\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 8: -31.526765611244855\n",
      "[0, 7, 2, 1, 4, 8]\n",
      "Steps done: 332\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 3: -16.685352768033585\n",
      "[0, 7, 2, 1, 4, 8, 3]\n",
      "Steps done: 333\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 18: -1.2151763626708854\n",
      "[0, 7, 2, 1, 4, 3]\n",
      "Steps done: 334\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 10: -1.4506138401429411\n",
      "[7, 2, 1, 4, 3]\n",
      "Steps done: 335\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 0: -1.2151763626708858\n",
      "[7, 2, 1, 4, 3, 0]\n",
      "Steps done: 336\n",
      "SV: [-0.11598881  0.1440614   0.04773778]\n",
      "Reward for action 20: 2.784823637329114\n",
      "[7, 2, 1, 4, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 49\n",
      "Steps done: 337\n",
      "SV: [0.03071264 0.02288481 0.5620131 ]\n",
      "Reward for action 2: -23.2218900609608\n",
      "[0, 1, 2]\n",
      "Steps done: 338\n",
      "SV: [0.03071264 0.02288481 0.5620131 ]\n",
      "Reward for action 3: -22.627738329849016\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 339\n",
      "SV: [0.03071264 0.02288481 0.5620131 ]\n",
      "Reward for action 4: -21.525851740602853\n",
      "[0, 1, 2, 3, 4]\n",
      "Steps done: 340\n",
      "SV: [0.03071264 0.02288481 0.5620131 ]\n",
      "Reward for action 8: -20.294959398631214\n",
      "[0, 1, 2, 3, 4, 8]\n",
      "Steps done: 341\n",
      "SV: [0.03071264 0.02288481 0.5620131 ]\n",
      "Reward for action 20: -16.294959398631214\n",
      "[0, 1, 2, 3, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 50\n",
      "Steps done: 342\n",
      "SV: [ 0.59366983 -0.13921951  0.0017035 ]\n",
      "Reward for action 3: -2.128653263502297\n",
      "[0, 1, 3]\n",
      "Steps done: 343\n",
      "SV: [ 0.59366983 -0.13921951  0.0017035 ]\n",
      "Reward for action 20: 1.8713467364977028\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 51\n",
      "Steps done: 344\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 9: -124.90899692366908\n",
      "[0, 1, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 345\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 19: -34.06563024233869\n",
      "[0, 1]\n",
      "Steps done: 346\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 5: -24.01539661881186\n",
      "[0, 1, 5]\n",
      "Steps done: 347\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 15: -34.06563024233869\n",
      "[0, 1]\n",
      "Steps done: 348\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 5: -24.01539661881186\n",
      "[0, 1, 5]\n",
      "Steps done: 349\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 2: -27.78243718822601\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 350\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 3: -62.20450471568963\n",
      "[0, 1, 5, 2, 3]\n",
      "Steps done: 351\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 7: -43.84382057561984\n",
      "[0, 1, 5, 2, 3, 7]\n",
      "Steps done: 352\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 8: -32.66638920700455\n",
      "[0, 1, 5, 2, 3, 7, 8]\n",
      "Steps done: 353\n",
      "SV: [-0.13411148 -0.29865724 -0.46986014]\n",
      "Reward for action 20: -28.66638920700455\n",
      "[0, 1, 5, 2, 3, 7, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 52\n",
      "Steps done: 354\n",
      "SV: [ 0.4051675   0.32062885 -0.21898346]\n",
      "Reward for action 8: -8.825099024508633\n",
      "[0, 1, 8]\n",
      "Steps done: 355\n",
      "SV: [ 0.4051675   0.32062885 -0.21898346]\n",
      "Reward for action 20: -4.825099024508633\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 53\n",
      "Steps done: 356\n",
      "SV: [0.10240521 0.08413237 0.7719366 ]\n",
      "Reward for action 4: -10.471480172291361\n",
      "[0, 1, 4]\n",
      "Steps done: 357\n",
      "SV: [0.10240521 0.08413237 0.7719366 ]\n",
      "Reward for action 5: -11.750463503795727\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 358\n",
      "SV: [0.10240521 0.08413237 0.7719366 ]\n",
      "Reward for action 6: -11.200598252596645\n",
      "[0, 1, 4, 5, 6]\n",
      "Steps done: 359\n",
      "SV: [0.10240521 0.08413237 0.7719366 ]\n",
      "Reward for action 20: -7.200598252596645\n",
      "[0, 1, 4, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 54\n",
      "Steps done: 360\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 2: -6.866319238348451\n",
      "[0, 1, 2]\n",
      "Steps done: 361\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 12: -59.953606610932695\n",
      "[0, 1]\n",
      "Steps done: 362\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 3: -3.841706525748933\n",
      "[0, 1, 3]\n",
      "Steps done: 363\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 13: -59.953606610932695\n",
      "[0, 1]\n",
      "Steps done: 364\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 4: -1.6400391258418998\n",
      "[0, 1, 4]\n",
      "Steps done: 365\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 5: -14.940840753046952\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 366\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 10: -28.70948676469095\n",
      "[1, 4, 5]\n",
      "Steps done: 367\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 3: -21.10017055378223\n",
      "[1, 4, 5, 3]\n",
      "Steps done: 368\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 14: -17.418703734539992\n",
      "[1, 5, 3]\n",
      "Steps done: 369\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 15: -14.497474619389696\n",
      "[1, 3]\n",
      "Steps done: 370\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 5: -17.418703734539985\n",
      "[1, 3, 5]\n",
      "Steps done: 371\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 15: -14.497474619389696\n",
      "[1, 3]\n",
      "Steps done: 372\n",
      "SV: [ 0.10299791 -0.49075523 -0.36234173]\n",
      "Reward for action 20: -10.497474619389696\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 55\n",
      "Steps done: 373\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 4: -0.7023180640215321\n",
      "[0, 1, 4]\n",
      "Steps done: 374\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 11: -214.08704044178745\n",
      "[0, 4]\n",
      "Steps done: 375\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 1: -0.7023180640215334\n",
      "[0, 4, 1]\n",
      "Steps done: 376\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 5: -17.89229721381938\n",
      "[0, 4, 1, 5]\n",
      "Steps done: 377\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 10: -0.7602677074508695\n",
      "[4, 1, 5]\n",
      "Steps done: 378\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 7: -29.46537174582406\n",
      "[4, 1, 5, 7]\n",
      "Steps done: 379\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 11: -6.674757436073035\n",
      "[4, 5, 7]\n",
      "Steps done: 380\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 2: -9.641542274869023\n",
      "[4, 5, 7, 2]\n",
      "Steps done: 381\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 0: -0.2222251114989824\n",
      "[4, 5, 7, 2, 0]\n",
      "Steps done: 382\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 14: -3.4219723884106146\n",
      "[5, 7, 2, 0]\n",
      "Steps done: 383\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 6: -22.50120918285566\n",
      "[5, 7, 2, 0, 6]\n",
      "Steps done: 384\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 15: -25.1171640079032\n",
      "[7, 2, 0, 6]\n",
      "Steps done: 385\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 16: -14.021128695008723\n",
      "[7, 2, 0]\n",
      "Steps done: 386\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 10: -73.42969947459609\n",
      "[7, 2]\n",
      "Steps done: 387\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 5: -14.424955102473437\n",
      "[7, 2, 5]\n",
      "Steps done: 388\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 15: -73.42969947459609\n",
      "[7, 2]\n",
      "Steps done: 389\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 3: -4.600830507240141\n",
      "[7, 2, 3]\n",
      "Steps done: 390\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 4: -0.8853699714442179\n",
      "[7, 2, 3, 4]\n",
      "Steps done: 391\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 14: -4.600830507240141\n",
      "[7, 2, 3]\n",
      "Steps done: 392\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 6: -4.0517591168516445\n",
      "[7, 2, 3, 6]\n",
      "Steps done: 393\n",
      "SV: [-0.344902  -0.9449943  0.9760311]\n",
      "Reward for action 20: -0.05175911685164447\n",
      "[7, 2, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 56\n",
      "Steps done: 394\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 2: -26.723002212784955\n",
      "[0, 1, 2]\n",
      "Steps done: 395\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 5: -11.37214759152777\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 396\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 3: -24.491470867415824\n",
      "[0, 1, 2, 5, 3]\n",
      "Steps done: 397\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 6: -20.260622531284625\n",
      "[0, 1, 2, 5, 3, 6]\n",
      "Steps done: 398\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 12: -26.843107386698392\n",
      "[0, 1, 5, 3, 6]\n",
      "Steps done: 399\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 11: -68.21828443886072\n",
      "[0, 5, 3, 6]\n",
      "Did target update\n",
      "Steps done: 400\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 4: -50.91414785275029\n",
      "[0, 5, 3, 6, 4]\n",
      "Steps done: 401\n",
      "SV: [-0.05687603 -0.53929704 -0.33380356]\n",
      "Reward for action 20: -46.91414785275029\n",
      "[0, 5, 3, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 57\n",
      "Steps done: 402\n",
      "SV: [-0.40699863  0.05816276  0.6553584 ]\n",
      "Reward for action 2: -88.36542519087362\n",
      "[0, 1, 2]\n",
      "Steps done: 403\n",
      "SV: [-0.40699863  0.05816276  0.6553584 ]\n",
      "Reward for action 10: -73.61947037800142\n",
      "[1, 2]\n",
      "Steps done: 404\n",
      "SV: [-0.40699863  0.05816276  0.6553584 ]\n",
      "Reward for action 3: -63.86530081947853\n",
      "[1, 2, 3]\n",
      "Steps done: 405\n",
      "SV: [-0.40699863  0.05816276  0.6553584 ]\n",
      "Reward for action 20: -59.86530081947853\n",
      "[1, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 58\n",
      "Steps done: 406\n",
      "SV: [ 1.6063474 -1.7745012  3.985305 ]\n",
      "Reward for action 2: -23.046321178270986\n",
      "[0, 1, 2]\n",
      "Steps done: 407\n",
      "SV: [ 1.6063474 -1.7745012  3.985305 ]\n",
      "Reward for action 20: -19.046321178270986\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 59\n",
      "Steps done: 408\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 3: -14.741891664735148\n",
      "[0, 1, 3]\n",
      "Steps done: 409\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 2: -16.035841108245055\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 410\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 11: -19.39162787124006\n",
      "[0, 3, 2]\n",
      "Steps done: 411\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 13: -4.355726870939909\n",
      "[0, 2]\n",
      "Steps done: 412\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 1: -126.26374883974886\n",
      "[0, 2, 1]\n",
      "Steps done: 413\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 10: -838.4762054677986\n",
      "[2, 1]\n",
      "Steps done: 414\n",
      "SV: [-0.04880191  0.08350582 -0.7098776 ]\n",
      "Reward for action 20: -834.4762054677986\n",
      "[2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 60\n",
      "Steps done: 415\n",
      "SV: [-0.12016743 -0.16848604  0.00447651]\n",
      "Reward for action 2: -3.423656762791259\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 416\n",
      "SV: [-0.12016743 -0.16848604  0.00447651]\n",
      "Reward for action 20: 0.5763432372087411\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 61\n",
      "Steps done: 417\n",
      "SV: [ 0.45306155 -0.160175    0.06680135]\n",
      "Reward for action 2: -53.66213047880617\n",
      "[0, 1, 2]\n",
      "Steps done: 418\n",
      "SV: [ 0.45306155 -0.160175    0.06680135]\n",
      "Reward for action 5: -32.29562454654136\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 419\n",
      "SV: [ 0.45306155 -0.160175    0.06680135]\n",
      "Reward for action 12: -9.980808283910404\n",
      "[0, 1, 5]\n",
      "Steps done: 420\n",
      "SV: [ 0.45306155 -0.160175    0.06680135]\n",
      "Reward for action 20: -5.980808283910404\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 62\n",
      "Steps done: 421\n",
      "SV: [ 0.07466381 -0.0194338  -1.2529944 ]\n",
      "Reward for action 3: -131.6882859878298\n",
      "[0, 1, 3]\n",
      "Steps done: 422\n",
      "SV: [ 0.07466381 -0.0194338  -1.2529944 ]\n",
      "Reward for action 6: -147.59043258573104\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 423\n",
      "SV: [ 0.07466381 -0.0194338  -1.2529944 ]\n",
      "Reward for action 5: -128.96815474913527\n",
      "[0, 1, 3, 6, 5]\n",
      "Steps done: 424\n",
      "SV: [ 0.07466381 -0.0194338  -1.2529944 ]\n",
      "Reward for action 7: -123.4302559983703\n",
      "[0, 1, 3, 6, 5, 7]\n",
      "Steps done: 425\n",
      "SV: [ 0.07466381 -0.0194338  -1.2529944 ]\n",
      "Reward for action 20: -119.4302559983703\n",
      "[0, 1, 3, 6, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 63\n",
      "Steps done: 426\n",
      "SV: [ 2.347253  -1.5689574  6.5617876]\n",
      "Reward for action 20: -222.13975837681758\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 64\n",
      "Steps done: 427\n",
      "SV: [-0.08832058 -0.25898573  0.7572622 ]\n",
      "Reward for action 2: -57.33133590379852\n",
      "[0, 1, 2]\n",
      "Steps done: 428\n",
      "SV: [-0.08832058 -0.25898573  0.7572622 ]\n",
      "Reward for action 11: -87.27160080183637\n",
      "[0, 2]\n",
      "Steps done: 429\n",
      "SV: [-0.08832058 -0.25898573  0.7572622 ]\n",
      "Reward for action 20: -83.27160080183637\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 65\n",
      "Steps done: 430\n",
      "SV: [ 0.17568068 -0.07723945 -0.40749788]\n",
      "Reward for action 3: -12.15467734973937\n",
      "[0, 1, 3]\n",
      "Steps done: 431\n",
      "SV: [ 0.17568068 -0.07723945 -0.40749788]\n",
      "Reward for action 5: -1.703758404807168\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 432\n",
      "SV: [ 0.17568068 -0.07723945 -0.40749788]\n",
      "Reward for action 7: -0.3160659788958629\n",
      "[0, 1, 3, 5, 7]\n",
      "Steps done: 433\n",
      "SV: [ 0.17568068 -0.07723945 -0.40749788]\n",
      "Reward for action 20: 3.683934021104137\n",
      "[0, 1, 3, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 66\n",
      "Steps done: 434\n",
      "SV: [-1.0713891  0.8926541  1.5494062]\n",
      "Reward for action 3: -208.95048077185567\n",
      "[0, 1, 3]\n",
      "Steps done: 435\n",
      "SV: [-1.0713891  0.8926541  1.5494062]\n",
      "Reward for action 4: -131.398080488552\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 436\n",
      "SV: [-1.0713891  0.8926541  1.5494062]\n",
      "Reward for action 14: -208.95048077185567\n",
      "[0, 1, 3]\n",
      "Steps done: 437\n",
      "SV: [-1.0713891  0.8926541  1.5494062]\n",
      "Reward for action 10: -207.97365151177956\n",
      "[1, 3]\n",
      "Steps done: 438\n",
      "SV: [-1.0713891  0.8926541  1.5494062]\n",
      "Reward for action 20: -203.97365151177956\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 67\n",
      "Steps done: 439\n",
      "SV: [-0.10333069  0.08801516 -0.52852196]\n",
      "Reward for action 3: -12.701515292227633\n",
      "[0, 1, 3]\n",
      "Steps done: 440\n",
      "SV: [-0.10333069  0.08801516 -0.52852196]\n",
      "Reward for action 10: -9.175140353463114\n",
      "[1, 3]\n",
      "Steps done: 441\n",
      "SV: [-0.10333069  0.08801516 -0.52852196]\n",
      "Reward for action 2: -16.398288322957466\n",
      "[1, 3, 2]\n",
      "Steps done: 442\n",
      "SV: [-0.10333069  0.08801516 -0.52852196]\n",
      "Reward for action 20: -12.398288322957466\n",
      "[1, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 68\n",
      "Steps done: 443\n",
      "SV: [-0.25031602  0.13780896  0.59271187]\n",
      "Reward for action 20: -69.48000521933399\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 69\n",
      "Steps done: 444\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 4: -8.622842357033054\n",
      "[0, 1, 4]\n",
      "Steps done: 445\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 9: -3.4900957334269282\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 446\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 6: -1.6708179230233209\n",
      "[0, 1, 4, 9, 6]\n",
      "Steps done: 447\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 14: -1.2243380160120743\n",
      "[0, 1, 9, 6]\n",
      "Steps done: 448\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 5: -5.760221068130688\n",
      "[0, 1, 9, 6, 5]\n",
      "Steps done: 449\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 11: -14.117015183261053\n",
      "[0, 9, 6, 5]\n",
      "Steps done: 450\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 16: -13.157689003187617\n",
      "[0, 9, 5]\n",
      "Steps done: 451\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 15: -2.1447179740487714\n",
      "[0, 9]\n",
      "Steps done: 452\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 2: -26.79857516139469\n",
      "[0, 9, 2]\n",
      "Steps done: 453\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 19: -533.0879335428734\n",
      "[0, 2]\n",
      "Steps done: 454\n",
      "SV: [ 0.14432493  0.02684273 -0.8542301 ]\n",
      "Reward for action 20: -529.0879335428734\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 70\n",
      "Steps done: 455\n",
      "SV: [ 0.26666588  0.59187543 -1.0771044 ]\n",
      "Reward for action 2: -32.1594116112478\n",
      "[0, 1, 2]\n",
      "Steps done: 456\n",
      "SV: [ 0.26666588  0.59187543 -1.0771044 ]\n",
      "Reward for action 10: -9.231586465638717\n",
      "[1, 2]\n",
      "Steps done: 457\n",
      "SV: [ 0.26666588  0.59187543 -1.0771044 ]\n",
      "Reward for action 20: -5.231586465638717\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 71\n",
      "Steps done: 458\n",
      "SV: [-0.01758278  0.07942364  0.27185827]\n",
      "Reward for action 20: -5.545658124001951\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 72\n",
      "Steps done: 459\n",
      "SV: [ 0.16150555  0.00668573 -0.7732224 ]\n",
      "Reward for action 4: -58.741763149967795\n",
      "[0, 1, 4]\n",
      "Steps done: 460\n",
      "SV: [ 0.16150555  0.00668573 -0.7732224 ]\n",
      "Reward for action 2: -113.2432923366263\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 461\n",
      "SV: [ 0.16150555  0.00668573 -0.7732224 ]\n",
      "Reward for action 5: -69.68868001735261\n",
      "[0, 1, 4, 2, 5]\n",
      "Steps done: 462\n",
      "SV: [ 0.16150555  0.00668573 -0.7732224 ]\n",
      "Reward for action 20: -65.68868001735261\n",
      "[0, 1, 4, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 73\n",
      "Steps done: 463\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 6: -91.9405792058779\n",
      "[0, 1, 6]\n",
      "Steps done: 464\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 3: -86.22957031031953\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 465\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 16: -44.347597991810545\n",
      "[0, 1, 3]\n",
      "Steps done: 466\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 5: -78.05478103120379\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 467\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 10: -144.3719649584979\n",
      "[1, 3, 5]\n",
      "Steps done: 468\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 2: -125.67172730570643\n",
      "[1, 3, 5, 2]\n",
      "Steps done: 469\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 0: -94.63696269887656\n",
      "[1, 3, 5, 2, 0]\n",
      "Steps done: 470\n",
      "SV: [-0.81427443  0.36373958 -0.9649748 ]\n",
      "Reward for action 20: -90.63696269887656\n",
      "[1, 3, 5, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 74\n",
      "Steps done: 471\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 2: -11.805985456087958\n",
      "[0, 1, 2]\n",
      "Steps done: 472\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 6: -12.131658890118956\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 473\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 7: -10.823372094113635\n",
      "[0, 1, 2, 6, 7]\n",
      "Steps done: 474\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 17: -12.131658890118956\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 475\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 5: -12.74474866916948\n",
      "[0, 1, 2, 6, 5]\n",
      "Steps done: 476\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 10: -9.6965131653835\n",
      "[1, 2, 6, 5]\n",
      "Steps done: 477\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 7: -14.890472105750238\n",
      "[1, 2, 6, 5, 7]\n",
      "Steps done: 478\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 4: -15.262709672291408\n",
      "[1, 2, 6, 5, 7, 4]\n",
      "Steps done: 479\n",
      "SV: [ 0.0875085  -0.01314996  0.18958707]\n",
      "Reward for action 20: -11.262709672291408\n",
      "[1, 2, 6, 5, 7, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 75\n",
      "Steps done: 480\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 3: -2.822705542974579\n",
      "[0, 1, 3]\n",
      "Steps done: 481\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 4: -67.38067552634561\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 482\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 10: -132.54813897818016\n",
      "[1, 3, 4]\n",
      "Steps done: 483\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 11: -295.80862780938304\n",
      "[3, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 484\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 1: -132.54813897818013\n",
      "[3, 4, 1]\n",
      "Steps done: 485\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 14: -5.629739756928034\n",
      "[3, 1]\n",
      "Steps done: 486\n",
      "SV: [-0.00427144  0.16435844  0.14448258]\n",
      "Reward for action 20: -1.6297397569280339\n",
      "[3, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 76\n",
      "Steps done: 487\n",
      "SV: [ 0.996339   -0.8748565   0.40717745]\n",
      "Reward for action 20: -117.30089789659137\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 77\n",
      "Steps done: 488\n",
      "SV: [ 0.05056142 -0.06708403 -0.582641  ]\n",
      "Reward for action 3: -27.86907973965023\n",
      "[0, 1, 3]\n",
      "Steps done: 489\n",
      "SV: [ 0.05056142 -0.06708403 -0.582641  ]\n",
      "Reward for action 4: -27.765947242389405\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 490\n",
      "SV: [ 0.05056142 -0.06708403 -0.582641  ]\n",
      "Reward for action 13: -27.55149880781527\n",
      "[0, 1, 4]\n",
      "Steps done: 491\n",
      "SV: [ 0.05056142 -0.06708403 -0.582641  ]\n",
      "Reward for action 20: -23.55149880781527\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 78\n",
      "Steps done: 492\n",
      "SV: [ 0.2275712   0.41158015 -0.2013812 ]\n",
      "Reward for action 20: -17.39509247218418\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 79\n",
      "Steps done: 493\n",
      "SV: [-0.10200526 -0.0166576  -1.0663301 ]\n",
      "Reward for action 2: -119.3961386589536\n",
      "[0, 1, 2]\n",
      "Steps done: 494\n",
      "SV: [-0.10200526 -0.0166576  -1.0663301 ]\n",
      "Reward for action 20: -115.3961386589536\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 80\n",
      "Steps done: 495\n",
      "SV: [1.303943  1.1221017 3.615778 ]\n",
      "Reward for action 3: -144.0296285151759\n",
      "[0, 1, 3]\n",
      "Steps done: 496\n",
      "SV: [1.303943  1.1221017 3.615778 ]\n",
      "Reward for action 2: -167.03540949224063\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 497\n",
      "SV: [1.303943  1.1221017 3.615778 ]\n",
      "Reward for action 13: -513.4091524866503\n",
      "[0, 1, 2]\n",
      "Steps done: 498\n",
      "SV: [1.303943  1.1221017 3.615778 ]\n",
      "Reward for action 10: -508.5600625518083\n",
      "[1, 2]\n",
      "Steps done: 499\n",
      "SV: [1.303943  1.1221017 3.615778 ]\n",
      "Reward for action 4: -16.292777587672404\n",
      "[1, 2, 4]\n",
      "Did target update\n",
      "Steps done: 500\n",
      "SV: [1.303943  1.1221017 3.615778 ]\n",
      "Reward for action 20: -12.292777587672404\n",
      "[1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 81\n",
      "Steps done: 501\n",
      "SV: [0.28691357 0.0526144  0.1147719 ]\n",
      "Reward for action 20: 2.772251086004136\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 82\n",
      "Steps done: 502\n",
      "SV: [-0.22229679  0.14597778 -0.03439748]\n",
      "Reward for action 9: -3.7725668653830113\n",
      "[0, 1, 9]\n",
      "Steps done: 503\n",
      "SV: [-0.22229679  0.14597778 -0.03439748]\n",
      "Reward for action 4: -4.046711869504694\n",
      "[0, 1, 9, 4]\n",
      "Steps done: 504\n",
      "SV: [-0.22229679  0.14597778 -0.03439748]\n",
      "Reward for action 5: -3.532958078101605\n",
      "[0, 1, 9, 4, 5]\n",
      "Steps done: 505\n",
      "SV: [-0.22229679  0.14597778 -0.03439748]\n",
      "Reward for action 15: -4.046711869504694\n",
      "[0, 1, 9, 4]\n",
      "Steps done: 506\n",
      "SV: [-0.22229679  0.14597778 -0.03439748]\n",
      "Reward for action 5: -3.532958078101605\n",
      "[0, 1, 9, 4, 5]\n",
      "Steps done: 507\n",
      "SV: [-0.22229679  0.14597778 -0.03439748]\n",
      "Reward for action 20: 0.467041921898395\n",
      "[0, 1, 9, 4, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 84\n",
      "Steps done: 508\n",
      "SV: [-0.11497797  0.0129694  -0.06107404]\n",
      "Reward for action 3: -3.3369530683706206\n",
      "[0, 1, 3]\n",
      "Steps done: 509\n",
      "SV: [-0.11497797  0.0129694  -0.06107404]\n",
      "Reward for action 10: -4.92802865384321\n",
      "[1, 3]\n",
      "Steps done: 510\n",
      "SV: [-0.11497797  0.0129694  -0.06107404]\n",
      "Reward for action 20: -0.9280286538432101\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 85\n",
      "Steps done: 511\n",
      "SV: [-0.83685315 -1.1660005  -0.1467688 ]\n",
      "Reward for action 2: -49.303771183974234\n",
      "[0, 1, 2]\n",
      "Steps done: 512\n",
      "SV: [-0.83685315 -1.1660005  -0.1467688 ]\n",
      "Reward for action 20: -45.303771183974234\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 86\n",
      "Steps done: 513\n",
      "SV: [ 0.682097 -1.204914  4.511764]\n",
      "Reward for action 3: -280.83441072598663\n",
      "[0, 1, 3]\n",
      "Steps done: 514\n",
      "SV: [ 0.682097 -1.204914  4.511764]\n",
      "Reward for action 6: -45.1985456768621\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 515\n",
      "SV: [ 0.682097 -1.204914  4.511764]\n",
      "Reward for action 10: -3.925261278428969\n",
      "[1, 3, 6]\n",
      "Steps done: 516\n",
      "SV: [ 0.682097 -1.204914  4.511764]\n",
      "Reward for action 4: -37.52883688976916\n",
      "[1, 3, 6, 4]\n",
      "Steps done: 517\n",
      "SV: [ 0.682097 -1.204914  4.511764]\n",
      "Reward for action 20: -33.52883688976916\n",
      "[1, 3, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 87\n",
      "Steps done: 518\n",
      "SV: [ 0.07410955 -0.05272305 -0.4767607 ]\n",
      "Reward for action 6: -67.25406639627951\n",
      "[0, 1, 6]\n",
      "Steps done: 519\n",
      "SV: [ 0.07410955 -0.05272305 -0.4767607 ]\n",
      "Reward for action 9: -23.01355012836901\n",
      "[0, 1, 6, 9]\n",
      "Steps done: 520\n",
      "SV: [ 0.07410955 -0.05272305 -0.4767607 ]\n",
      "Reward for action 7: -20.612473862886056\n",
      "[0, 1, 6, 9, 7]\n",
      "Steps done: 521\n",
      "SV: [ 0.07410955 -0.05272305 -0.4767607 ]\n",
      "Reward for action 20: -16.612473862886056\n",
      "[0, 1, 6, 9, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 88\n",
      "Steps done: 522\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 2: -26.09262114445592\n",
      "[0, 1, 2]\n",
      "Steps done: 523\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 11: -43.01960800243205\n",
      "[0, 2]\n",
      "Steps done: 524\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 4: -30.592498111306725\n",
      "[0, 2, 4]\n",
      "Steps done: 525\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 6: -31.084188970080277\n",
      "[0, 2, 4, 6]\n",
      "Steps done: 526\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 1: -26.463571665441467\n",
      "[0, 2, 4, 6, 1]\n",
      "Steps done: 527\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 5: -28.115514169807\n",
      "[0, 2, 4, 6, 1, 5]\n",
      "Steps done: 528\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 12: -27.139594420952655\n",
      "[0, 4, 6, 1, 5]\n",
      "Steps done: 529\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 2: -28.115514169806993\n",
      "[0, 4, 6, 1, 5, 2]\n",
      "Steps done: 530\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 10: -28.453208571137306\n",
      "[4, 6, 1, 5, 2]\n",
      "Steps done: 531\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 11: -29.423481458361003\n",
      "[4, 6, 5, 2]\n",
      "Steps done: 532\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 14: -30.44928396583628\n",
      "[6, 5, 2]\n",
      "Steps done: 533\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 1: -29.342355551620372\n",
      "[6, 5, 2, 1]\n",
      "Steps done: 534\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 16: -37.35971260155215\n",
      "[5, 2, 1]\n",
      "Steps done: 535\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 15: -26.71933567175951\n",
      "[2, 1]\n",
      "Steps done: 536\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 0: -26.092621144455897\n",
      "[2, 1, 0]\n",
      "Steps done: 537\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 6: -26.332772875560437\n",
      "[2, 1, 0, 6]\n",
      "Steps done: 538\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 12: -24.790170894412103\n",
      "[1, 0, 6]\n",
      "Steps done: 539\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 2: -26.332772875560444\n",
      "[1, 0, 6, 2]\n",
      "Steps done: 540\n",
      "SV: [-0.44769463 -0.04343913  0.4915165 ]\n",
      "Reward for action 20: -22.332772875560444\n",
      "[1, 0, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 89\n",
      "Steps done: 541\n",
      "SV: [ 0.4622971 -1.8109028  1.1612562]\n",
      "Reward for action 2: -0.12456032815909315\n",
      "[0, 1, 2]\n",
      "Steps done: 542\n",
      "SV: [ 0.4622971 -1.8109028  1.1612562]\n",
      "Reward for action 7: -11.787584719468704\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 543\n",
      "SV: [ 0.4622971 -1.8109028  1.1612562]\n",
      "Reward for action 20: -7.787584719468704\n",
      "[0, 1, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 91\n",
      "Steps done: 544\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 8: -6.19451252987862\n",
      "[0, 1, 8]\n",
      "Steps done: 545\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 6: -6.836086689440664\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 546\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 16: -6.19451252987862\n",
      "[0, 1, 8]\n",
      "Steps done: 547\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 10: -9.03523450117094\n",
      "[1, 8]\n",
      "Steps done: 548\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 5: -2.986688249358751\n",
      "[1, 8, 5]\n",
      "Steps done: 549\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 0: -2.8525714922546936\n",
      "[1, 8, 5, 0]\n",
      "Steps done: 550\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 10: -2.986688249358751\n",
      "[1, 8, 5]\n",
      "Steps done: 551\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 0: -2.8525714922546936\n",
      "[1, 8, 5, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 552\n",
      "SV: [ 0.04720749  0.10166313 -0.30000344]\n",
      "Reward for action 20: 1.1474285077453064\n",
      "[1, 8, 5, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 92\n",
      "Steps done: 553\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 5: -6.362605591559899\n",
      "[0, 1, 5]\n",
      "Steps done: 554\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 2: -4.83618667206656\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 555\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 11: -6.23083725727734\n",
      "[0, 5, 2]\n",
      "Steps done: 556\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 10: -3.420785676338779\n",
      "[5, 2]\n",
      "Steps done: 557\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 6: -2.415086928376833\n",
      "[5, 2, 6]\n",
      "Steps done: 558\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 12: -1.2952280959388907\n",
      "[5, 6]\n",
      "Steps done: 559\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 4: -2.024165883487742\n",
      "[5, 6, 4]\n",
      "Steps done: 560\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 15: -3.298150180580573\n",
      "[6, 4]\n",
      "Steps done: 561\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 1: -0.2232032236641997\n",
      "[6, 4, 1]\n",
      "Steps done: 562\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 8: -0.5824846423875513\n",
      "[6, 4, 1, 8]\n",
      "Steps done: 563\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 5: -1.133152837015854\n",
      "[6, 4, 1, 8, 5]\n",
      "Steps done: 564\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 11: -2.0155048447561135\n",
      "[6, 4, 8, 5]\n",
      "Steps done: 565\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 15: -1.7994005525009287\n",
      "[6, 4, 8]\n",
      "Steps done: 566\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 1: -0.5824846423875513\n",
      "[6, 4, 8, 1]\n",
      "Steps done: 567\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 5: -1.1331528370158535\n",
      "[6, 4, 8, 1, 5]\n",
      "Steps done: 568\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 9: -1.4367616596856805\n",
      "[6, 4, 8, 1, 5, 9]\n",
      "Steps done: 569\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 0: -1.7120386515854973\n",
      "[6, 4, 8, 1, 5, 9, 0]\n",
      "Steps done: 570\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 14: -2.7388766919430343\n",
      "[6, 8, 1, 5, 9, 0]\n",
      "Steps done: 571\n",
      "SV: [-0.14364375  0.04694343 -0.14647768]\n",
      "Reward for action 20: 1.2611233080569657\n",
      "[6, 8, 1, 5, 9, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 93\n",
      "Steps done: 572\n",
      "SV: [-0.4298923  -0.58620703 -0.13147841]\n",
      "Reward for action 2: -38.35229006031415\n",
      "[0, 1, 2]\n",
      "Steps done: 573\n",
      "SV: [-0.4298923  -0.58620703 -0.13147841]\n",
      "Reward for action 12: -35.846680172240326\n",
      "[0, 1]\n",
      "Steps done: 574\n",
      "SV: [-0.4298923  -0.58620703 -0.13147841]\n",
      "Reward for action 20: -31.846680172240326\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 94\n",
      "Steps done: 575\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 6: -269.9656432705845\n",
      "[0, 1, 6]\n",
      "Steps done: 576\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 5: -149.4712118071636\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 577\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 4: -108.12349393769472\n",
      "[0, 1, 6, 5, 4]\n",
      "Steps done: 578\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 9: -87.55954724166882\n",
      "[0, 1, 6, 5, 4, 9]\n",
      "Steps done: 579\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 3: -74.88401284147386\n",
      "[0, 1, 6, 5, 4, 9, 3]\n",
      "Steps done: 580\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 2: -72.87821389820475\n",
      "[0, 1, 6, 5, 4, 9, 3, 2]\n",
      "Steps done: 581\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 8: -63.08145664382686\n",
      "[0, 1, 6, 5, 4, 9, 3, 2, 8]\n",
      "Steps done: 582\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 18: -72.87821389820475\n",
      "[0, 1, 6, 5, 4, 9, 3, 2]\n",
      "Steps done: 583\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 8: -63.08145664382686\n",
      "[0, 1, 6, 5, 4, 9, 3, 2, 8]\n",
      "Steps done: 584\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 12: -64.72842276338537\n",
      "[0, 1, 6, 5, 4, 9, 3, 8]\n",
      "Steps done: 585\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 19: -72.95957237929935\n",
      "[0, 1, 6, 5, 4, 3, 8]\n",
      "Steps done: 586\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 13: -87.0118632370554\n",
      "[0, 1, 6, 5, 4, 8]\n",
      "Steps done: 587\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 15: -118.19957830643777\n",
      "[0, 1, 6, 4, 8]\n",
      "Steps done: 588\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 18: -161.83771044667446\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 589\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 8: -118.19957830643777\n",
      "[0, 1, 6, 4, 8]\n",
      "Steps done: 590\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 7: -94.1047944781724\n",
      "[0, 1, 6, 4, 8, 7]\n",
      "Steps done: 591\n",
      "SV: [-4.2560880e-04 -1.5594214e-02 -4.4829136e-01]\n",
      "Reward for action 20: -90.1047944781724\n",
      "[0, 1, 6, 4, 8, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 95\n",
      "Steps done: 592\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 4: -20.235419966636275\n",
      "[0, 1, 4]\n",
      "Steps done: 593\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 2: -14.159262008540848\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 594\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 7: -0.6355856534886144\n",
      "[0, 1, 4, 2, 7]\n",
      "Steps done: 595\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 10: -10.747374132382395\n",
      "[1, 4, 2, 7]\n",
      "Steps done: 596\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 5: -6.838692411105254\n",
      "[1, 4, 2, 7, 5]\n",
      "Steps done: 597\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 15: -10.747374132382395\n",
      "[1, 4, 2, 7]\n",
      "Steps done: 598\n",
      "SV: [ 0.43137595  1.9543412  -1.3276614 ]\n",
      "Reward for action 20: -6.7473741323823955\n",
      "[1, 4, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 96\n",
      "Steps done: 599\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 3: -606.8037300050419\n",
      "[0, 1, 3]\n",
      "Did target update\n",
      "Steps done: 600\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 11: -625.4195784608474\n",
      "[0, 3]\n",
      "Steps done: 601\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 6: -109.80092107150138\n",
      "[0, 3, 6]\n",
      "Steps done: 602\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 7: -318.6654727306304\n",
      "[0, 3, 6, 7]\n",
      "Steps done: 603\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 17: -109.80092107150138\n",
      "[0, 3, 6]\n",
      "Steps done: 604\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 4: -326.79941302990903\n",
      "[0, 3, 6, 4]\n",
      "Steps done: 605\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 1: -425.40745354999274\n",
      "[0, 3, 6, 4, 1]\n",
      "Steps done: 606\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 13: -636.659189376709\n",
      "[0, 6, 4, 1]\n",
      "Steps done: 607\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 14: -621.2214612285096\n",
      "[0, 6, 1]\n",
      "Steps done: 608\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 16: -597.5603854647804\n",
      "[0, 1]\n",
      "Steps done: 609\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 8: -68.32709643640388\n",
      "[0, 1, 8]\n",
      "Steps done: 610\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 7: -277.3960840259414\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 611\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 17: -68.32709643640388\n",
      "[0, 1, 8]\n",
      "Steps done: 612\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 4: -287.34464684189777\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 613\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 7: -409.3103081160505\n",
      "[0, 1, 8, 4, 7]\n",
      "Steps done: 614\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 17: -287.34464684189777\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 615\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 7: -409.3103081160505\n",
      "[0, 1, 8, 4, 7]\n",
      "Steps done: 616\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 5: -468.524599554356\n",
      "[0, 1, 8, 4, 7, 5]\n",
      "Steps done: 617\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 18: -614.1965484837988\n",
      "[0, 1, 4, 7, 5]\n",
      "Steps done: 618\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 17: -609.1706952401503\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 619\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 15: -620.089059390145\n",
      "[0, 1, 4]\n",
      "Steps done: 620\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 6: -636.659189376709\n",
      "[0, 1, 4, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 621\n",
      "SV: [ 0.57960165  0.1173381  -3.6108768 ]\n",
      "Reward for action 20: -632.659189376709\n",
      "[0, 1, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 97\n",
      "Steps done: 622\n",
      "SV: [ 0.03939867  0.21617189 -0.9468221 ]\n",
      "Reward for action 20: -136.58678033460473\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 98\n",
      "Steps done: 623\n",
      "SV: [-0.7941267   0.67478174 -0.16195317]\n",
      "Reward for action 3: -1.7034996181027064\n",
      "[0, 1, 3]\n",
      "Steps done: 624\n",
      "SV: [-0.7941267   0.67478174 -0.16195317]\n",
      "Reward for action 20: 2.2965003818972933\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 99\n",
      "Steps done: 625\n",
      "SV: [ 0.26431623 -3.7746499   0.9201708 ]\n",
      "Reward for action 20: -321.829605613621\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 100\n",
      "Steps done: 626\n",
      "SV: [ 0.5369703  -0.02785907 -2.5553014 ]\n",
      "Reward for action 5: -626.0737050984136\n",
      "[0, 1, 5]\n",
      "Steps done: 627\n",
      "SV: [ 0.5369703  -0.02785907 -2.5553014 ]\n",
      "Reward for action 3: -271.4529641555883\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 628\n",
      "SV: [ 0.5369703  -0.02785907 -2.5553014 ]\n",
      "Reward for action 2: -513.0214761289252\n",
      "[0, 1, 5, 3, 2]\n",
      "Steps done: 629\n",
      "SV: [ 0.5369703  -0.02785907 -2.5553014 ]\n",
      "Reward for action 20: -509.0214761289252\n",
      "[0, 1, 5, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 101\n",
      "Steps done: 630\n",
      "SV: [-0.12925468  0.04652886  0.05077615]\n",
      "Reward for action 20: -266.8459498365175\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 102\n",
      "Steps done: 631\n",
      "SV: [ 0.08126477  0.03141028 -0.7078101 ]\n",
      "Reward for action 5: -47.5796718013152\n",
      "[0, 1, 5]\n",
      "Steps done: 632\n",
      "SV: [ 0.08126477  0.03141028 -0.7078101 ]\n",
      "Reward for action 4: -42.25608125400381\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 633\n",
      "SV: [ 0.08126477  0.03141028 -0.7078101 ]\n",
      "Reward for action 14: -47.5796718013152\n",
      "[0, 1, 5]\n",
      "Steps done: 634\n",
      "SV: [ 0.08126477  0.03141028 -0.7078101 ]\n",
      "Reward for action 3: -42.326920538465906\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 635\n",
      "SV: [ 0.08126477  0.03141028 -0.7078101 ]\n",
      "Reward for action 20: -38.326920538465906\n",
      "[0, 1, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 103\n",
      "Steps done: 636\n",
      "SV: [ 0.00949995  0.05843732 -0.4276197 ]\n",
      "Reward for action 5: -39.64175119005369\n",
      "[0, 1, 5]\n",
      "Steps done: 637\n",
      "SV: [ 0.00949995  0.05843732 -0.4276197 ]\n",
      "Reward for action 20: -35.64175119005369\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 104\n",
      "Steps done: 638\n",
      "SV: [-0.52408165 -2.3061702   1.9073437 ]\n",
      "Reward for action 2: -1.8556259727354976\n",
      "[0, 1, 2]\n",
      "Steps done: 639\n",
      "SV: [-0.52408165 -2.3061702   1.9073437 ]\n",
      "Reward for action 7: -0.9549039731291618\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 640\n",
      "SV: [-0.52408165 -2.3061702   1.9073437 ]\n",
      "Reward for action 20: 3.045096026870838\n",
      "[0, 1, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 105\n",
      "Steps done: 641\n",
      "SV: [0.00424519 0.02212792 0.61916715]\n",
      "Reward for action 20: -26.570537796668393\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 106\n",
      "Steps done: 642\n",
      "SV: [-0.00606866 -0.08527146 -0.94029564]\n",
      "Reward for action 5: -31.989645410463908\n",
      "[0, 1, 5]\n",
      "Steps done: 643\n",
      "SV: [-0.00606866 -0.08527146 -0.94029564]\n",
      "Reward for action 10: -11.392590482965476\n",
      "[1, 5]\n",
      "Steps done: 644\n",
      "SV: [-0.00606866 -0.08527146 -0.94029564]\n",
      "Reward for action 4: -13.417355845494045\n",
      "[1, 5, 4]\n",
      "Steps done: 645\n",
      "SV: [-0.00606866 -0.08527146 -0.94029564]\n",
      "Reward for action 15: -37.73900104571878\n",
      "[1, 4]\n",
      "Steps done: 646\n",
      "SV: [-0.00606866 -0.08527146 -0.94029564]\n",
      "Reward for action 20: -33.73900104571878\n",
      "[1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 108\n",
      "Steps done: 647\n",
      "SV: [ 0.18907762 -0.22972314  0.4760406 ]\n",
      "Reward for action 5: -27.7920321642939\n",
      "[0, 1, 5]\n",
      "Steps done: 648\n",
      "SV: [ 0.18907762 -0.22972314  0.4760406 ]\n",
      "Reward for action 15: -193.641251976977\n",
      "[0, 1]\n",
      "Steps done: 649\n",
      "SV: [ 0.18907762 -0.22972314  0.4760406 ]\n",
      "Reward for action 7: -81.87916812879577\n",
      "[0, 1, 7]\n",
      "Steps done: 650\n",
      "SV: [ 0.18907762 -0.22972314  0.4760406 ]\n",
      "Reward for action 5: -22.54350174208274\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 651\n",
      "SV: [ 0.18907762 -0.22972314  0.4760406 ]\n",
      "Reward for action 20: -18.54350174208274\n",
      "[0, 1, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 109\n",
      "Steps done: 652\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 5: -0.7069597808551705\n",
      "[0, 1, 5]\n",
      "Steps done: 653\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 10: -6.6900363767288376\n",
      "[1, 5]\n",
      "Steps done: 654\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 7: -239.6069740730814\n",
      "[1, 5, 7]\n",
      "Steps done: 655\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 3: -142.42906737266685\n",
      "[1, 5, 7, 3]\n",
      "Steps done: 656\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 2: -100.17129926125492\n",
      "[1, 5, 7, 3, 2]\n",
      "Steps done: 657\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 11: -141.85835395926017\n",
      "[5, 7, 3, 2]\n",
      "Steps done: 658\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 0: -98.53774250506332\n",
      "[5, 7, 3, 2, 0]\n",
      "Steps done: 659\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 6: -72.07876070387998\n",
      "[5, 7, 3, 2, 0, 6]\n",
      "Steps done: 660\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 17: -3.0946737726678304\n",
      "[5, 3, 2, 0, 6]\n",
      "Steps done: 661\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 1: -2.655895793759913\n",
      "[5, 3, 2, 0, 6, 1]\n",
      "Steps done: 662\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 9: -10.222364439016529\n",
      "[5, 3, 2, 0, 6, 1, 9]\n",
      "Steps done: 663\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 19: -2.655895793759913\n",
      "[5, 3, 2, 0, 6, 1]\n",
      "Steps done: 664\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 4: -1.6362013515723053\n",
      "[5, 3, 2, 0, 6, 1, 4]\n",
      "Steps done: 665\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 14: -2.655895793759913\n",
      "[5, 3, 2, 0, 6, 1]\n",
      "Steps done: 666\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 15: -2.4763525985325154\n",
      "[3, 2, 0, 6, 1]\n",
      "Steps done: 667\n",
      "SV: [-0.04396141  0.11052733 -0.24623103]\n",
      "Reward for action 20: 1.5236474014674846\n",
      "[3, 2, 0, 6, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 110\n",
      "Steps done: 668\n",
      "SV: [-0.36277777  0.29426378 -0.8226587 ]\n",
      "Reward for action 6: -71.67517405147177\n",
      "[0, 1, 6]\n",
      "Steps done: 669\n",
      "SV: [-0.36277777  0.29426378 -0.8226587 ]\n",
      "Reward for action 5: -85.29126746825986\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 670\n",
      "SV: [-0.36277777  0.29426378 -0.8226587 ]\n",
      "Reward for action 16: -41.513290241197545\n",
      "[0, 1, 5]\n",
      "Steps done: 671\n",
      "SV: [-0.36277777  0.29426378 -0.8226587 ]\n",
      "Reward for action 10: -53.60857075906037\n",
      "[1, 5]\n",
      "Steps done: 672\n",
      "SV: [-0.36277777  0.29426378 -0.8226587 ]\n",
      "Reward for action 3: -68.00218315150876\n",
      "[1, 5, 3]\n",
      "Steps done: 673\n",
      "SV: [-0.36277777  0.29426378 -0.8226587 ]\n",
      "Reward for action 20: -64.00218315150876\n",
      "[1, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 111\n",
      "Steps done: 674\n",
      "SV: [ 0.4677064  -0.21284533 -0.23447934]\n",
      "Reward for action 3: -958.0056198174586\n",
      "[0, 1, 3]\n",
      "Steps done: 675\n",
      "SV: [ 0.4677064  -0.21284533 -0.23447934]\n",
      "Reward for action 13: -1140.7062893118514\n",
      "[0, 1]\n",
      "Steps done: 676\n",
      "SV: [ 0.4677064  -0.21284533 -0.23447934]\n",
      "Reward for action 20: -1136.7062893118514\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 112\n",
      "Steps done: 677\n",
      "SV: [-0.8913552  1.0516819  3.9334362]\n",
      "Reward for action 3: -622.0546964076138\n",
      "[0, 1, 3]\n",
      "Steps done: 678\n",
      "SV: [-0.8913552  1.0516819  3.9334362]\n",
      "Reward for action 13: -530.8952107336736\n",
      "[0, 1]\n",
      "Steps done: 679\n",
      "SV: [-0.8913552  1.0516819  3.9334362]\n",
      "Reward for action 20: -526.8952107336736\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 113\n",
      "Steps done: 680\n",
      "SV: [ 0.41011307 -0.03306357 -0.26997066]\n",
      "Reward for action 9: -16.337101360610177\n",
      "[0, 1, 9]\n",
      "Steps done: 681\n",
      "SV: [ 0.41011307 -0.03306357 -0.26997066]\n",
      "Reward for action 7: -26.11071698739056\n",
      "[0, 1, 9, 7]\n",
      "Steps done: 682\n",
      "SV: [ 0.41011307 -0.03306357 -0.26997066]\n",
      "Reward for action 11: -22.403002226422334\n",
      "[0, 9, 7]\n",
      "Steps done: 683\n",
      "SV: [ 0.41011307 -0.03306357 -0.26997066]\n",
      "Reward for action 17: -18.641227795584502\n",
      "[0, 9]\n",
      "Steps done: 684\n",
      "SV: [ 0.41011307 -0.03306357 -0.26997066]\n",
      "Reward for action 20: -14.641227795584502\n",
      "[0, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 114\n",
      "Steps done: 685\n",
      "SV: [-1.7883418  1.7687317 -4.685663 ]\n",
      "Reward for action 20: 1.845483240436951\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 115\n",
      "Steps done: 686\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 3: -134.22192039003014\n",
      "[0, 1, 3]\n",
      "Steps done: 687\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 4: -48.64799589528276\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 688\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 2: -53.33782374077107\n",
      "[0, 1, 3, 4, 2]\n",
      "Steps done: 689\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 12: -48.64799589528276\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 690\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 8: -51.676059222553924\n",
      "[0, 1, 3, 4, 8]\n",
      "Steps done: 691\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 14: -90.45050553629584\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 692\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 2: -80.44363598612843\n",
      "[0, 1, 3, 8, 2]\n",
      "Steps done: 693\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 10: -93.53983429121587\n",
      "[1, 3, 8, 2]\n",
      "Steps done: 694\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 0: -80.44363598612848\n",
      "[1, 3, 8, 2, 0]\n",
      "Steps done: 695\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 4: -54.84628780593211\n",
      "[1, 3, 8, 2, 0, 4]\n",
      "Steps done: 696\n",
      "SV: [ 0.10816506 -0.19704416  0.6891857 ]\n",
      "Reward for action 20: -50.84628780593211\n",
      "[1, 3, 8, 2, 0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 116\n",
      "Steps done: 697\n",
      "SV: [-0.43272135  0.6475221  -2.0522597 ]\n",
      "Reward for action 5: -570.1576253998569\n",
      "[0, 1, 5]\n",
      "Steps done: 698\n",
      "SV: [-0.43272135  0.6475221  -2.0522597 ]\n",
      "Reward for action 4: -178.29385569854102\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 699\n",
      "SV: [-0.43272135  0.6475221  -2.0522597 ]\n",
      "Reward for action 2: -29.348694250706252\n",
      "[0, 1, 5, 4, 2]\n",
      "Did target update\n",
      "Steps done: 700\n",
      "SV: [-0.43272135  0.6475221  -2.0522597 ]\n",
      "Reward for action 14: -108.72573497243754\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 701\n",
      "SV: [-0.43272135  0.6475221  -2.0522597 ]\n",
      "Reward for action 3: -85.14957083320772\n",
      "[0, 1, 5, 2, 3]\n",
      "Steps done: 702\n",
      "SV: [-0.43272135  0.6475221  -2.0522597 ]\n",
      "Reward for action 20: -81.14957083320772\n",
      "[0, 1, 5, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 117\n",
      "Steps done: 703\n",
      "SV: [ 0.9962783  -0.43887493 -1.5946317 ]\n",
      "Reward for action 3: -92.53018371616257\n",
      "[0, 1, 3]\n",
      "Steps done: 704\n",
      "SV: [ 0.9962783  -0.43887493 -1.5946317 ]\n",
      "Reward for action 13: -159.3364223935629\n",
      "[0, 1]\n",
      "Steps done: 705\n",
      "SV: [ 0.9962783  -0.43887493 -1.5946317 ]\n",
      "Reward for action 20: -155.3364223935629\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 118\n",
      "Steps done: 706\n",
      "SV: [ 0.18879174 -0.2111696   0.09310968]\n",
      "Reward for action 5: -4.843758878089967\n",
      "[0, 1, 5]\n",
      "Steps done: 707\n",
      "SV: [ 0.18879174 -0.2111696   0.09310968]\n",
      "Reward for action 20: -0.8437588780899672\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 119\n",
      "Steps done: 708\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 2: -61.46858703120848\n",
      "[0, 1, 2]\n",
      "Steps done: 709\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 3: -49.4012030438699\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 710\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 10: -60.00497675985934\n",
      "[1, 2, 3]\n",
      "Steps done: 711\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 13: -111.61768160574937\n",
      "[1, 2]\n",
      "Steps done: 712\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 3: -60.00497675985934\n",
      "[1, 2, 3]\n",
      "Steps done: 713\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 0: -49.4012030438699\n",
      "[1, 2, 3, 0]\n",
      "Steps done: 714\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 11: -66.77547594303192\n",
      "[2, 3, 0]\n",
      "Steps done: 715\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 1: -49.40120304386991\n",
      "[2, 3, 0, 1]\n",
      "Steps done: 716\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 12: -54.891090096879005\n",
      "[3, 0, 1]\n",
      "Steps done: 717\n",
      "SV: [-0.30293858 -0.37090227  0.38604638]\n",
      "Reward for action 20: -50.891090096879005\n",
      "[3, 0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 120\n",
      "Steps done: 718\n",
      "SV: [-0.16568504  0.10073997 -1.2261236 ]\n",
      "Reward for action 5: -121.60785249451538\n",
      "[0, 1, 5]\n",
      "Steps done: 719\n",
      "SV: [-0.16568504  0.10073997 -1.2261236 ]\n",
      "Reward for action 3: -122.82534108462755\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 720\n",
      "SV: [-0.16568504  0.10073997 -1.2261236 ]\n",
      "Reward for action 20: -118.82534108462755\n",
      "[0, 1, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 121\n",
      "Steps done: 721\n",
      "SV: [ 0.2697628  -0.08448033 -0.69058746]\n",
      "Reward for action 4: -73.0838045408152\n",
      "[0, 1, 4]\n",
      "Steps done: 722\n",
      "SV: [ 0.2697628  -0.08448033 -0.69058746]\n",
      "Reward for action 8: -51.328699043440665\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 723\n",
      "SV: [ 0.2697628  -0.08448033 -0.69058746]\n",
      "Reward for action 9: -52.64276627066279\n",
      "[0, 1, 4, 8, 9]\n",
      "Steps done: 724\n",
      "SV: [ 0.2697628  -0.08448033 -0.69058746]\n",
      "Reward for action 19: -51.328699043440665\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 725\n",
      "SV: [ 0.2697628  -0.08448033 -0.69058746]\n",
      "Reward for action 6: -48.57029766797825\n",
      "[0, 1, 4, 8, 6]\n",
      "Steps done: 726\n",
      "SV: [ 0.2697628  -0.08448033 -0.69058746]\n",
      "Reward for action 20: -44.57029766797825\n",
      "[0, 1, 4, 8, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 122\n",
      "Steps done: 727\n",
      "SV: [-1.5926784 -0.9169326  0.9889637]\n",
      "Reward for action 5: -30.0060292097493\n",
      "[0, 1, 5]\n",
      "Steps done: 728\n",
      "SV: [-1.5926784 -0.9169326  0.9889637]\n",
      "Reward for action 2: -50.42227257888604\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 729\n",
      "SV: [-1.5926784 -0.9169326  0.9889637]\n",
      "Reward for action 4: -50.67486038286107\n",
      "[0, 1, 5, 2, 4]\n",
      "Steps done: 730\n",
      "SV: [-1.5926784 -0.9169326  0.9889637]\n",
      "Reward for action 14: -50.42227257888604\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 731\n",
      "SV: [-1.5926784 -0.9169326  0.9889637]\n",
      "Reward for action 10: -26.699161175455487\n",
      "[1, 5, 2]\n",
      "Steps done: 732\n",
      "SV: [-1.5926784 -0.9169326  0.9889637]\n",
      "Reward for action 20: -22.699161175455487\n",
      "[1, 5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 123\n",
      "Steps done: 733\n",
      "SV: [-0.002008    0.21455629 -0.38420853]\n",
      "Reward for action 5: -44.32813233571435\n",
      "[0, 1, 5]\n",
      "Steps done: 734\n",
      "SV: [-0.002008    0.21455629 -0.38420853]\n",
      "Reward for action 10: -23.34767587999942\n",
      "[1, 5]\n",
      "Steps done: 735\n",
      "SV: [-0.002008    0.21455629 -0.38420853]\n",
      "Reward for action 20: -19.34767587999942\n",
      "[1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 124\n",
      "Steps done: 736\n",
      "SV: [0.09659791 0.17286162 0.32656136]\n",
      "Reward for action 4: -85.92226949259413\n",
      "[0, 1, 4]\n",
      "Steps done: 737\n",
      "SV: [0.09659791 0.17286162 0.32656136]\n",
      "Reward for action 10: -46.134188780567555\n",
      "[1, 4]\n",
      "Steps done: 738\n",
      "SV: [0.09659791 0.17286162 0.32656136]\n",
      "Reward for action 0: -85.92226949259418\n",
      "[1, 4, 0]\n",
      "Steps done: 739\n",
      "SV: [0.09659791 0.17286162 0.32656136]\n",
      "Reward for action 11: -298.69690651267103\n",
      "[4, 0]\n",
      "Steps done: 740\n",
      "SV: [0.09659791 0.17286162 0.32656136]\n",
      "Reward for action 3: -9.090930786370027\n",
      "[4, 0, 3]\n",
      "Steps done: 741\n",
      "SV: [0.09659791 0.17286162 0.32656136]\n",
      "Reward for action 20: -5.090930786370027\n",
      "[4, 0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 125\n",
      "Steps done: 742\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 3: -171.60747969118034\n",
      "[0, 1, 3]\n",
      "Steps done: 743\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 5: -159.1068252803726\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 744\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 15: -171.60747969118034\n",
      "[0, 1, 3]\n",
      "Steps done: 745\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 4: -111.93690322409236\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 746\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 10: -234.97622278297104\n",
      "[1, 3, 4]\n",
      "Steps done: 747\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 11: -272.81656522197926\n",
      "[3, 4]\n",
      "Steps done: 748\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 2: -59.83811123293371\n",
      "[3, 4, 2]\n",
      "Steps done: 749\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 12: -272.81656522197926\n",
      "[3, 4]\n",
      "Steps done: 750\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 0: -84.74658960137933\n",
      "[3, 4, 0]\n",
      "Steps done: 751\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 5: -105.2867225930614\n",
      "[3, 4, 0, 5]\n",
      "Steps done: 752\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 2: -86.59709478037226\n",
      "[3, 4, 0, 5, 2]\n",
      "Steps done: 753\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 12: -105.2867225930614\n",
      "[3, 4, 0, 5]\n",
      "Steps done: 754\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 1: -111.62387940241871\n",
      "[3, 4, 0, 5, 1]\n",
      "Steps done: 755\n",
      "SV: [0.27082866 0.36695716 0.53196365]\n",
      "Reward for action 20: -107.62387940241871\n",
      "[3, 4, 0, 5, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 126\n",
      "Steps done: 756\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 5: -40.43703042761243\n",
      "[0, 1, 5]\n",
      "Steps done: 757\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 15: -32.02202612891351\n",
      "[0, 1]\n",
      "Steps done: 758\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 7: -29.032152335151775\n",
      "[0, 1, 7]\n",
      "Steps done: 759\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 10: -6.386542513822719\n",
      "[1, 7]\n",
      "Steps done: 760\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 8: -6.897637150476646\n",
      "[1, 7, 8]\n",
      "Steps done: 761\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 5: -9.661929422708297\n",
      "[1, 7, 8, 5]\n",
      "Steps done: 762\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 0: -29.45413811626287\n",
      "[1, 7, 8, 5, 0]\n",
      "Steps done: 763\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 9: -154.08983968872155\n",
      "[1, 7, 8, 5, 0, 9]\n",
      "Steps done: 764\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 2: -8.867051940059381\n",
      "[1, 7, 8, 5, 0, 9, 2]\n",
      "Steps done: 765\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 10: -3.719229599252799\n",
      "[1, 7, 8, 5, 9, 2]\n",
      "Steps done: 766\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 6: -5.163906043764468\n",
      "[1, 7, 8, 5, 9, 2, 6]\n",
      "Steps done: 767\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 17: -3.2055669607304904\n",
      "[1, 8, 5, 9, 2, 6]\n",
      "Steps done: 768\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 7: -5.163906043764462\n",
      "[1, 8, 5, 9, 2, 6, 7]\n",
      "Steps done: 769\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 11: -10.072489474060285\n",
      "[8, 5, 9, 2, 6, 7]\n",
      "Steps done: 770\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 12: -121.13041739133071\n",
      "[8, 5, 9, 6, 7]\n",
      "Steps done: 771\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 19: -6.424064786963721\n",
      "[8, 5, 6, 7]\n",
      "Steps done: 772\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 1: -5.324879009201162\n",
      "[8, 5, 6, 7, 1]\n",
      "Steps done: 773\n",
      "SV: [ 0.05220518 -0.01499177 -0.41688642]\n",
      "Reward for action 20: -1.3248790092011618\n",
      "[8, 5, 6, 7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 127\n",
      "Steps done: 774\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 7: -0.19390031038736144\n",
      "[0, 1, 7]\n",
      "Steps done: 775\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 6: -3.2310814602311986\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 776\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 3: -1.7833588175646269\n",
      "[0, 1, 7, 6, 3]\n",
      "Steps done: 777\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 10: -5.417301348414189\n",
      "[1, 7, 6, 3]\n",
      "Steps done: 778\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 13: -12.502858604933667\n",
      "[1, 7, 6]\n",
      "Steps done: 779\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 3: -5.417301348414189\n",
      "[1, 7, 6, 3]\n",
      "Steps done: 780\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 11: -5.080576786754192\n",
      "[7, 6, 3]\n",
      "Steps done: 781\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 17: -9.086702403848847\n",
      "[6, 3]\n",
      "Steps done: 782\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 7: -5.080576786754192\n",
      "[6, 3, 7]\n",
      "Steps done: 783\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 4: -7.414021565591056\n",
      "[6, 3, 7, 4]\n",
      "Steps done: 784\n",
      "SV: [-0.11482527 -0.12137802 -0.16567378]\n",
      "Reward for action 20: -3.4140215655910557\n",
      "[6, 3, 7, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 128\n",
      "Steps done: 785\n",
      "SV: [ 0.0134666   0.00638019 -0.5755902 ]\n",
      "Reward for action 20: -17.602530151110592\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 129\n",
      "Steps done: 786\n",
      "SV: [-0.2747563  -0.11679018  0.55487186]\n",
      "Reward for action 2: -16.84715991413196\n",
      "[0, 1, 2]\n",
      "Steps done: 787\n",
      "SV: [-0.2747563  -0.11679018  0.55487186]\n",
      "Reward for action 5: -15.38589117340807\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 788\n",
      "SV: [-0.2747563  -0.11679018  0.55487186]\n",
      "Reward for action 10: -61.46343637412687\n",
      "[1, 2, 5]\n",
      "Steps done: 789\n",
      "SV: [-0.2747563  -0.11679018  0.55487186]\n",
      "Reward for action 6: -80.26425679031084\n",
      "[1, 2, 5, 6]\n",
      "Steps done: 790\n",
      "SV: [-0.2747563  -0.11679018  0.55487186]\n",
      "Reward for action 16: -61.46343637412687\n",
      "[1, 2, 5]\n",
      "Steps done: 791\n",
      "SV: [-0.2747563  -0.11679018  0.55487186]\n",
      "Reward for action 20: -57.46343637412687\n",
      "[1, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 130\n",
      "Steps done: 792\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 6: -57.9177235043431\n",
      "[0, 1, 6]\n",
      "Steps done: 793\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 8: -62.2882016091109\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 794\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 3: -49.35688852567294\n",
      "[0, 1, 6, 8, 3]\n",
      "Steps done: 795\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 4: -39.54315701782728\n",
      "[0, 1, 6, 8, 3, 4]\n",
      "Steps done: 796\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 7: -30.30848971209879\n",
      "[0, 1, 6, 8, 3, 4, 7]\n",
      "Steps done: 797\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 9: -5.497140814673789\n",
      "[0, 1, 6, 8, 3, 4, 7, 9]\n",
      "Steps done: 798\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 18: -4.048722203142891\n",
      "[0, 1, 6, 3, 4, 7, 9]\n",
      "Steps done: 799\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 2: -6.379601212396759\n",
      "[0, 1, 6, 3, 4, 7, 9, 2]\n",
      "Did target update\n",
      "Steps done: 800\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 16: -6.57161662398484\n",
      "[0, 1, 3, 4, 7, 9, 2]\n",
      "Steps done: 801\n",
      "SV: [ 0.57388294 -0.09079944  0.19170241]\n",
      "Reward for action 20: -2.5716166239848404\n",
      "[0, 1, 3, 4, 7, 9, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 131\n",
      "Steps done: 802\n",
      "SV: [ 1.6664635 -1.6690267 -4.819262 ]\n",
      "Reward for action 20: -245.28287597766843\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 132\n",
      "Steps done: 803\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 4: -447.5467701178812\n",
      "[0, 1, 4]\n",
      "Steps done: 804\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 10: -632.1220765723414\n",
      "[1, 4]\n",
      "Steps done: 805\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 8: -636.637832972591\n",
      "[1, 4, 8]\n",
      "Steps done: 806\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 6: -391.9128606243629\n",
      "[1, 4, 8, 6]\n",
      "Steps done: 807\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 18: -207.81719923774094\n",
      "[1, 4, 6]\n",
      "Steps done: 808\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 7: -6.6608692348752525\n",
      "[1, 4, 6, 7]\n",
      "Steps done: 809\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 8: -123.01575075725604\n",
      "[1, 4, 6, 7, 8]\n",
      "Steps done: 810\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 17: -391.9128606243629\n",
      "[1, 4, 6, 8]\n",
      "Steps done: 811\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 3: -0.3217058467879139\n",
      "[1, 4, 6, 8, 3]\n",
      "Steps done: 812\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 7: -9.472880451134845\n",
      "[1, 4, 6, 8, 3, 7]\n",
      "Steps done: 813\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 11: -51.84361843222736\n",
      "[4, 6, 8, 3, 7]\n",
      "Steps done: 814\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 1: -9.472880451134847\n",
      "[4, 6, 8, 3, 7, 1]\n",
      "Steps done: 815\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 17: -0.3217058467879104\n",
      "[4, 6, 8, 3, 1]\n",
      "Steps done: 816\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 13: -391.9128606243631\n",
      "[4, 6, 8, 1]\n",
      "Steps done: 817\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 0: -330.45212654760104\n",
      "[4, 6, 8, 1, 0]\n",
      "Steps done: 818\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 5: -141.34787135891915\n",
      "[4, 6, 8, 1, 0, 5]\n",
      "Steps done: 819\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 14: -87.13778376164316\n",
      "[6, 8, 1, 0, 5]\n",
      "Steps done: 820\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 4: -141.34787135891926\n",
      "[6, 8, 1, 0, 5, 4]\n",
      "Steps done: 821\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 16: -248.28450762677957\n",
      "[8, 1, 0, 5, 4]\n",
      "Steps done: 822\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 7: -96.09412566213439\n",
      "[8, 1, 0, 5, 4, 7]\n",
      "Steps done: 823\n",
      "SV: [ 1.1220499   0.48133156 -3.6182353 ]\n",
      "Reward for action 20: -92.09412566213439\n",
      "[8, 1, 0, 5, 4, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 133\n",
      "Steps done: 824\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 4: -20.58026915232208\n",
      "[0, 1, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 825\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 2: -35.30325144743469\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 826\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 3: -20.84409125755204\n",
      "[0, 1, 4, 2, 3]\n",
      "Steps done: 827\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 6: -26.577106774325916\n",
      "[0, 1, 4, 2, 3, 6]\n",
      "Steps done: 828\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 13: -53.602144581417534\n",
      "[0, 1, 4, 2, 6]\n",
      "Steps done: 829\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 3: -26.577106774325934\n",
      "[0, 1, 4, 2, 6, 3]\n",
      "Steps done: 830\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 5: -42.00437755573821\n",
      "[0, 1, 4, 2, 6, 3, 5]\n",
      "Steps done: 831\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 14: -44.493179281772925\n",
      "[0, 1, 2, 6, 3, 5]\n",
      "Steps done: 832\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 12: -24.619588137818244\n",
      "[0, 1, 6, 3, 5]\n",
      "Steps done: 833\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 7: -73.35065951035172\n",
      "[0, 1, 6, 3, 5, 7]\n",
      "Steps done: 834\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 13: -131.40035587265794\n",
      "[0, 1, 6, 5, 7]\n",
      "Steps done: 835\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 11: -264.7601053865342\n",
      "[0, 6, 5, 7]\n",
      "Steps done: 836\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 1: -131.40035587265797\n",
      "[0, 6, 5, 7, 1]\n",
      "Steps done: 837\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 8: -101.36854362887411\n",
      "[0, 6, 5, 7, 1, 8]\n",
      "Steps done: 838\n",
      "SV: [-0.23101519  0.25729334  0.38002282]\n",
      "Reward for action 20: -97.36854362887411\n",
      "[0, 6, 5, 7, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 134\n",
      "Steps done: 839\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 2: -53.11845066363332\n",
      "[0, 1, 2]\n",
      "Steps done: 840\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 6: -40.13610328364103\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 841\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 10: -40.061544302488855\n",
      "[1, 2, 6]\n",
      "Steps done: 842\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 8: -22.396126936868644\n",
      "[1, 2, 6, 8]\n",
      "Steps done: 843\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 12: -25.57113354801899\n",
      "[1, 6, 8]\n",
      "Steps done: 844\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 3: -18.915150274563512\n",
      "[1, 6, 8, 3]\n",
      "Steps done: 845\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 2: -14.289799190145146\n",
      "[1, 6, 8, 3, 2]\n",
      "Steps done: 846\n",
      "SV: [0.53728926 0.06086067 0.61404395]\n",
      "Reward for action 20: -10.289799190145146\n",
      "[1, 6, 8, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 135\n",
      "Steps done: 847\n",
      "SV: [-0.00549658  0.03262528 -0.5966743 ]\n",
      "Reward for action 2: -76.34027691588778\n",
      "[0, 1, 2]\n",
      "Steps done: 848\n",
      "SV: [-0.00549658  0.03262528 -0.5966743 ]\n",
      "Reward for action 10: -27.87964739405978\n",
      "[1, 2]\n",
      "Steps done: 849\n",
      "SV: [-0.00549658  0.03262528 -0.5966743 ]\n",
      "Reward for action 5: -26.431867693390913\n",
      "[1, 2, 5]\n",
      "Steps done: 850\n",
      "SV: [-0.00549658  0.03262528 -0.5966743 ]\n",
      "Reward for action 11: -29.25880283220915\n",
      "[2, 5]\n",
      "Steps done: 851\n",
      "SV: [-0.00549658  0.03262528 -0.5966743 ]\n",
      "Reward for action 20: -25.25880283220915\n",
      "[2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 136\n",
      "Steps done: 852\n",
      "SV: [ 0.15409972 -0.1858246   0.33360022]\n",
      "Reward for action 20: -188.82781450329298\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 137\n",
      "Steps done: 853\n",
      "SV: [-2.515466   2.0659223  3.163862 ]\n",
      "Reward for action 20: -69.1153894713398\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 138\n",
      "Steps done: 854\n",
      "SV: [0.02186181 0.96708405 2.4119585 ]\n",
      "Reward for action 2: -319.9876040734189\n",
      "[0, 1, 2]\n",
      "Steps done: 855\n",
      "SV: [0.02186181 0.96708405 2.4119585 ]\n",
      "Reward for action 10: -748.9028654004067\n",
      "[1, 2]\n",
      "Steps done: 856\n",
      "SV: [0.02186181 0.96708405 2.4119585 ]\n",
      "Reward for action 20: -744.9028654004067\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 139\n",
      "Steps done: 857\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 6: -7.820903345402262\n",
      "[0, 1, 6]\n",
      "Steps done: 858\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 16: -1.0673240865700953\n",
      "[0, 1]\n",
      "Steps done: 859\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 5: -3.680711454626287\n",
      "[0, 1, 5]\n",
      "Steps done: 860\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 2: -2.4117347264433904\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 861\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 6: -3.8116382834569444\n",
      "[0, 1, 5, 2, 6]\n",
      "Steps done: 862\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 10: -5.123523681441529\n",
      "[1, 5, 2, 6]\n",
      "Steps done: 863\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 0: -3.811638283456948\n",
      "[1, 5, 2, 6, 0]\n",
      "Steps done: 864\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 4: -3.6132146007980506\n",
      "[1, 5, 2, 6, 0, 4]\n",
      "Steps done: 865\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 14: -3.811638283456948\n",
      "[1, 5, 2, 6, 0]\n",
      "Steps done: 866\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 3: -4.604903637267414\n",
      "[1, 5, 2, 6, 0, 3]\n",
      "Steps done: 867\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 16: -3.8429350846995662\n",
      "[1, 5, 2, 0, 3]\n",
      "Steps done: 868\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 15: -2.8954973590807285\n",
      "[1, 2, 0, 3]\n",
      "Steps done: 869\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 4: -2.4418688707311613\n",
      "[1, 2, 0, 3, 4]\n",
      "Steps done: 870\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 11: -2.79213380485082\n",
      "[2, 0, 3, 4]\n",
      "Steps done: 871\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 14: -4.075388048249239\n",
      "[2, 0, 3]\n",
      "Steps done: 872\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 4: -2.79213380485082\n",
      "[2, 0, 3, 4]\n",
      "Steps done: 873\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 14: -4.075388048249239\n",
      "[2, 0, 3]\n",
      "Steps done: 874\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 5: -4.579787882930928\n",
      "[2, 0, 3, 5]\n",
      "Steps done: 875\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 1: -3.8429350846995662\n",
      "[2, 0, 3, 5, 1]\n",
      "Steps done: 876\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 11: -4.579787882930928\n",
      "[2, 0, 3, 5]\n",
      "Steps done: 877\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 1: -3.8429350846995662\n",
      "[2, 0, 3, 5, 1]\n",
      "Steps done: 878\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 12: -4.684856619827269\n",
      "[0, 3, 5, 1]\n",
      "Steps done: 879\n",
      "SV: [ 0.05424418  0.03373835 -0.22344244]\n",
      "Reward for action 20: -0.6848566198272694\n",
      "[0, 3, 5, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 140\n",
      "Steps done: 880\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 9: -31.821986001406355\n",
      "[0, 1, 9]\n",
      "Steps done: 881\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 10: -87.54615849237172\n",
      "[1, 9]\n",
      "Steps done: 882\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 0: -31.821986001406362\n",
      "[1, 9, 0]\n",
      "Steps done: 883\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 5: -23.52598921446414\n",
      "[1, 9, 0, 5]\n",
      "Steps done: 884\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 10: -28.9656730084301\n",
      "[1, 9, 5]\n",
      "Steps done: 885\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 7: -20.581858416173514\n",
      "[1, 9, 5, 7]\n",
      "Steps done: 886\n",
      "SV: [ 0.09979036  0.00450292 -0.41251037]\n",
      "Reward for action 20: -16.581858416173514\n",
      "[1, 9, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 141\n",
      "Steps done: 887\n",
      "SV: [ 0.05529029  0.04405772 -0.47176695]\n",
      "Reward for action 20: -15.373332562877607\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 142\n",
      "Steps done: 888\n",
      "SV: [ 0.10051709  0.11397503 -0.01920798]\n",
      "Reward for action 20: 2.0137421250635\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 143\n",
      "Steps done: 889\n",
      "SV: [ 0.637613  -0.5622906 -1.5015583]\n",
      "Reward for action 2: -213.53616814804224\n",
      "[0, 1, 2]\n",
      "Steps done: 890\n",
      "SV: [ 0.637613  -0.5622906 -1.5015583]\n",
      "Reward for action 10: -413.75523425154154\n",
      "[1, 2]\n",
      "Steps done: 891\n",
      "SV: [ 0.637613  -0.5622906 -1.5015583]\n",
      "Reward for action 20: -409.75523425154154\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 144\n",
      "Steps done: 892\n",
      "SV: [ 0.43852112 -0.9270075   1.1389041 ]\n",
      "Reward for action 2: -280.0706060104007\n",
      "[0, 1, 2]\n",
      "Steps done: 893\n",
      "SV: [ 0.43852112 -0.9270075   1.1389041 ]\n",
      "Reward for action 12: -1037.8524956722476\n",
      "[0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 894\n",
      "SV: [ 0.43852112 -0.9270075   1.1389041 ]\n",
      "Reward for action 2: -280.0706060104007\n",
      "[0, 1, 2]\n",
      "Steps done: 895\n",
      "SV: [ 0.43852112 -0.9270075   1.1389041 ]\n",
      "Reward for action 3: -13.494491193772134\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 896\n",
      "SV: [ 0.43852112 -0.9270075   1.1389041 ]\n",
      "Reward for action 13: -280.0706060104007\n",
      "[0, 1, 2]\n",
      "Steps done: 897\n",
      "SV: [ 0.43852112 -0.9270075   1.1389041 ]\n",
      "Reward for action 20: -276.0706060104007\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 145\n",
      "Steps done: 898\n",
      "SV: [-0.2625912  -0.7177446   0.89805603]\n",
      "Reward for action 3: -76.49180900286692\n",
      "[0, 1, 3]\n",
      "Steps done: 899\n",
      "SV: [-0.2625912  -0.7177446   0.89805603]\n",
      "Reward for action 10: -459.1559280359251\n",
      "[1, 3]\n",
      "Did target update\n",
      "Steps done: 900\n",
      "SV: [-0.2625912  -0.7177446   0.89805603]\n",
      "Reward for action 0: -76.49180900286692\n",
      "[1, 3, 0]\n",
      "Steps done: 901\n",
      "SV: [-0.2625912  -0.7177446   0.89805603]\n",
      "Reward for action 20: -72.49180900286692\n",
      "[1, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 146\n",
      "Steps done: 902\n",
      "SV: [ 0.1713281   0.02415572 -0.41963425]\n",
      "Reward for action 20: -12.714069720808936\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 147\n",
      "Steps done: 903\n",
      "SV: [ 0.03607439 -0.04056088 -0.7283444 ]\n",
      "Reward for action 7: -45.00424569731893\n",
      "[0, 1, 7]\n",
      "Steps done: 904\n",
      "SV: [ 0.03607439 -0.04056088 -0.7283444 ]\n",
      "Reward for action 20: -41.00424569731893\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 148\n",
      "Steps done: 905\n",
      "SV: [ 0.17645217 -1.7716969   0.7751924 ]\n",
      "Reward for action 5: -158.98189468293626\n",
      "[0, 1, 5]\n",
      "Steps done: 906\n",
      "SV: [ 0.17645217 -1.7716969   0.7751924 ]\n",
      "Reward for action 20: -154.98189468293626\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 149\n",
      "Steps done: 907\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 6: -18.775218497239067\n",
      "[0, 1, 6]\n",
      "Steps done: 908\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 3: -1.627467743003674\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 909\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 11: -0.9314113579688501\n",
      "[0, 6, 3]\n",
      "Steps done: 910\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 2: -0.5433636112402251\n",
      "[0, 6, 3, 2]\n",
      "Steps done: 911\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 13: -3.5812006906114235\n",
      "[0, 6, 2]\n",
      "Steps done: 912\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 4: -0.5789558028753703\n",
      "[0, 6, 2, 4]\n",
      "Steps done: 913\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 1: -0.5306506672484074\n",
      "[0, 6, 2, 4, 1]\n",
      "Steps done: 914\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 12: -1.5195328697051402\n",
      "[0, 6, 4, 1]\n",
      "Steps done: 915\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 11: -0.8013110416596398\n",
      "[0, 6, 4]\n",
      "Steps done: 916\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 14: -37.092220554546614\n",
      "[0, 6]\n",
      "Steps done: 917\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 3: -0.9314113579688501\n",
      "[0, 6, 3]\n",
      "Steps done: 918\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 4: -1.1987945879596817\n",
      "[0, 6, 3, 4]\n",
      "Steps done: 919\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 2: -1.7017214332692994\n",
      "[0, 6, 3, 4, 2]\n",
      "Steps done: 920\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 13: -0.5789558028753702\n",
      "[0, 6, 4, 2]\n",
      "Steps done: 921\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 16: -10.115483841607185\n",
      "[0, 4, 2]\n",
      "Steps done: 922\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 3: -8.753310696579408\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 923\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 1: -5.1565416134748245\n",
      "[0, 4, 2, 3, 1]\n",
      "Steps done: 924\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 14: -3.9291285728135543\n",
      "[0, 2, 3, 1]\n",
      "Steps done: 925\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 5: -8.133421810940096\n",
      "[0, 2, 3, 1, 5]\n",
      "Steps done: 926\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 10: -11.324899787389784\n",
      "[2, 3, 1, 5]\n",
      "Steps done: 927\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 11: -14.343720306877595\n",
      "[2, 3, 5]\n",
      "Steps done: 928\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 12: -19.213348792709365\n",
      "[3, 5]\n",
      "Steps done: 929\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 0: -14.521118165267236\n",
      "[3, 5, 0]\n",
      "Steps done: 930\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 4: -8.055728015599556\n",
      "[3, 5, 0, 4]\n",
      "Steps done: 931\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 15: -8.201764329160218\n",
      "[3, 0, 4]\n",
      "Steps done: 932\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Reward for action 1: -3.7075568012531144\n",
      "[3, 0, 4, 1]\n",
      "Steps done: 933\n",
      "SV: [-0.2731315   0.08071637  0.07724806]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: 0.2924431987468856\n",
      "[3, 0, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 150\n",
      "Steps done: 934\n",
      "SV: [-0.2581706   0.12537971  0.2899705 ]\n",
      "Reward for action 6: -462.0612555009262\n",
      "[0, 1, 6]\n",
      "Steps done: 935\n",
      "SV: [-0.2581706   0.12537971  0.2899705 ]\n",
      "Reward for action 7: -146.27036302938816\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 936\n",
      "SV: [-0.2581706   0.12537971  0.2899705 ]\n",
      "Reward for action 9: -43.17534318441277\n",
      "[0, 1, 6, 7, 9]\n",
      "Steps done: 937\n",
      "SV: [-0.2581706   0.12537971  0.2899705 ]\n",
      "Reward for action 3: -46.702152475411964\n",
      "[0, 1, 6, 7, 9, 3]\n",
      "Steps done: 938\n",
      "SV: [-0.2581706   0.12537971  0.2899705 ]\n",
      "Reward for action 20: -42.702152475411964\n",
      "[0, 1, 6, 7, 9, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 151\n",
      "Steps done: 939\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 8: -65.91480771039454\n",
      "[0, 1, 8]\n",
      "Steps done: 940\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 10: -65.3909210542877\n",
      "[1, 8]\n",
      "Steps done: 941\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 7: -67.99560923578275\n",
      "[1, 8, 7]\n",
      "Steps done: 942\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 18: -91.33181295315234\n",
      "[1, 7]\n",
      "Steps done: 943\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 2: -69.78596526191207\n",
      "[1, 7, 2]\n",
      "Steps done: 944\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 12: -91.33181295315234\n",
      "[1, 7]\n",
      "Steps done: 945\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 4: -68.72905784394916\n",
      "[1, 7, 4]\n",
      "Steps done: 946\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 0: -68.35112121922354\n",
      "[1, 7, 4, 0]\n",
      "Steps done: 947\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 9: -66.68668679973985\n",
      "[1, 7, 4, 0, 9]\n",
      "Steps done: 948\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 17: -65.7300415032621\n",
      "[1, 4, 0, 9]\n",
      "Steps done: 949\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 10: -64.87494609698267\n",
      "[1, 4, 9]\n",
      "Steps done: 950\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 6: -64.7844101211108\n",
      "[1, 4, 9, 6]\n",
      "Steps done: 951\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 0: -65.24528453373246\n",
      "[1, 4, 9, 6, 0]\n",
      "Steps done: 952\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 2: -65.1340783289007\n",
      "[1, 4, 9, 6, 0, 2]\n",
      "Steps done: 953\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 8: -64.92191915311935\n",
      "[1, 4, 9, 6, 0, 2, 8]\n",
      "Steps done: 954\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 16: -65.04437655025515\n",
      "[1, 4, 9, 0, 2, 8]\n",
      "Steps done: 955\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 5: -65.03623817288407\n",
      "[1, 4, 9, 0, 2, 8, 5]\n",
      "Steps done: 956\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 12: -65.14185316482308\n",
      "[1, 4, 9, 0, 8, 5]\n",
      "Steps done: 957\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 3: -65.04963640857527\n",
      "[1, 4, 9, 0, 8, 5, 3]\n",
      "Steps done: 958\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 18: -65.26936072908133\n",
      "[1, 4, 9, 0, 5, 3]\n",
      "Steps done: 959\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 15: -65.28283479593445\n",
      "[1, 4, 9, 0, 3]\n",
      "Steps done: 960\n",
      "SV: [-0.03609568  0.09978994 -0.8866933 ]\n",
      "Reward for action 20: -61.28283479593445\n",
      "[1, 4, 9, 0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 152\n",
      "Steps done: 961\n",
      "SV: [-0.5537707   0.30303442  1.3374866 ]\n",
      "Reward for action 6: -208.06365754579116\n",
      "[0, 1, 6]\n",
      "Steps done: 962\n",
      "SV: [-0.5537707   0.30303442  1.3374866 ]\n",
      "Reward for action 10: -740.5622526845401\n",
      "[1, 6]\n",
      "Steps done: 963\n",
      "SV: [-0.5537707   0.30303442  1.3374866 ]\n",
      "Reward for action 20: -736.5622526845401\n",
      "[1, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 153\n",
      "Steps done: 964\n",
      "SV: [-0.1502277  -0.1793679  -0.18586443]\n",
      "Reward for action 2: -10.598246133667557\n",
      "[0, 1, 2]\n",
      "Steps done: 965\n",
      "SV: [-0.1502277  -0.1793679  -0.18586443]\n",
      "Reward for action 20: -6.598246133667557\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 154\n",
      "Steps done: 966\n",
      "SV: [-0.27642497  0.8807506  -0.8351696 ]\n",
      "Reward for action 3: -32.21634863196741\n",
      "[0, 1, 3]\n",
      "Steps done: 967\n",
      "SV: [-0.27642497  0.8807506  -0.8351696 ]\n",
      "Reward for action 7: -15.460974077728975\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 968\n",
      "SV: [-0.27642497  0.8807506  -0.8351696 ]\n",
      "Reward for action 8: -24.249201895171648\n",
      "[0, 1, 3, 7, 8]\n",
      "Steps done: 969\n",
      "SV: [-0.27642497  0.8807506  -0.8351696 ]\n",
      "Reward for action 10: -32.93698183833162\n",
      "[1, 3, 7, 8]\n",
      "Steps done: 970\n",
      "SV: [-0.27642497  0.8807506  -0.8351696 ]\n",
      "Reward for action 18: -20.163788625667614\n",
      "[1, 3, 7]\n",
      "Steps done: 971\n",
      "SV: [-0.27642497  0.8807506  -0.8351696 ]\n",
      "Reward for action 20: -16.163788625667614\n",
      "[1, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 155\n",
      "Steps done: 972\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 4: -554.9920781640676\n",
      "[0, 1, 4]\n",
      "Steps done: 973\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 14: -501.1912805781251\n",
      "[0, 1]\n",
      "Steps done: 974\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 5: -8.66434151659329\n",
      "[0, 1, 5]\n",
      "Steps done: 975\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 15: -501.1912805781251\n",
      "[0, 1]\n",
      "Steps done: 976\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 2: -556.1002664803981\n",
      "[0, 1, 2]\n",
      "Steps done: 977\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 4: -584.9842040219698\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 978\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 12: -554.9920781640676\n",
      "[0, 1, 4]\n",
      "Steps done: 979\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 5: -89.91675254070972\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 980\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 11: -11.075239320393276\n",
      "[0, 4, 5]\n",
      "Steps done: 981\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 14: -990.8192120810712\n",
      "[0, 5]\n",
      "Steps done: 982\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 1: -8.66434151659329\n",
      "[0, 5, 1]\n",
      "Steps done: 983\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 10: -292.101519864557\n",
      "[5, 1]\n",
      "Steps done: 984\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 0: -8.66434151659329\n",
      "[5, 1, 0]\n",
      "Steps done: 985\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 3: -14.88771609205187\n",
      "[5, 1, 0, 3]\n",
      "Steps done: 986\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 15: -131.19905167192954\n",
      "[1, 0, 3]\n",
      "Steps done: 987\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 2: -208.06408436979999\n",
      "[1, 0, 3, 2]\n",
      "Steps done: 988\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 4: -225.50670425806655\n",
      "[1, 0, 3, 2, 4]\n",
      "Steps done: 989\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 12: -143.69248849634334\n",
      "[1, 0, 3, 4]\n",
      "Steps done: 990\n",
      "SV: [-0.49155575 -1.0001347   3.2678413 ]\n",
      "Reward for action 20: -139.69248849634334\n",
      "[1, 0, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 156\n",
      "Steps done: 991\n",
      "SV: [-0.271121    0.35760957 -1.7085234 ]\n",
      "Reward for action 3: -187.97117748241143\n",
      "[0, 1, 3]\n",
      "Steps done: 992\n",
      "SV: [-0.271121    0.35760957 -1.7085234 ]\n",
      "Reward for action 10: -256.01331597342846\n",
      "[1, 3]\n",
      "Steps done: 993\n",
      "SV: [-0.271121    0.35760957 -1.7085234 ]\n",
      "Reward for action 4: -248.5190668583705\n",
      "[1, 3, 4]\n",
      "Steps done: 994\n",
      "SV: [-0.271121    0.35760957 -1.7085234 ]\n",
      "Reward for action 20: -244.5190668583705\n",
      "[1, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 157\n",
      "Steps done: 995\n",
      "SV: [ 0.3114855 -0.9512164 -1.7015469]\n",
      "Reward for action 20: -187.33049955322178\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 158\n",
      "Steps done: 996\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 2: -194.82486152520215\n",
      "[0, 1, 2]\n",
      "Steps done: 997\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 11: -195.79932511063646\n",
      "[0, 2]\n",
      "Steps done: 998\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 1: -194.82486152520215\n",
      "[0, 2, 1]\n",
      "Steps done: 999\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 12: -215.57869871099078\n",
      "[0, 1]\n",
      "Did target update\n",
      "Steps done: 1000\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 2: -194.82486152520215\n",
      "[0, 1, 2]\n",
      "Steps done: 1001\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 10: -180.52575299740164\n",
      "[1, 2]\n",
      "Steps done: 1002\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 0: -194.82486152520215\n",
      "[1, 2, 0]\n",
      "Steps done: 1003\n",
      "SV: [ 0.01466289 -0.8605563  -1.6961663 ]\n",
      "Reward for action 20: -190.82486152520215\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 159\n",
      "Steps done: 1004\n",
      "SV: [ 0.4525536   0.47198692 -0.7989646 ]\n",
      "Reward for action 3: -17.147220920248632\n",
      "[0, 1, 3]\n",
      "Steps done: 1005\n",
      "SV: [ 0.4525536   0.47198692 -0.7989646 ]\n",
      "Reward for action 4: -59.90558566571352\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 1006\n",
      "SV: [ 0.4525536   0.47198692 -0.7989646 ]\n",
      "Reward for action 5: -97.9700331000901\n",
      "[0, 1, 3, 4, 5]\n",
      "Steps done: 1007\n",
      "SV: [ 0.4525536   0.47198692 -0.7989646 ]\n",
      "Reward for action 11: -275.2189096389196\n",
      "[0, 3, 4, 5]\n",
      "Steps done: 1008\n",
      "SV: [ 0.4525536   0.47198692 -0.7989646 ]\n",
      "Reward for action 20: -271.2189096389196\n",
      "[0, 3, 4, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 160\n",
      "Steps done: 1009\n",
      "SV: [0.70671535 0.00167501 0.5795776 ]\n",
      "Reward for action 2: -62.92829935060566\n",
      "[0, 1, 2]\n",
      "Steps done: 1010\n",
      "SV: [0.70671535 0.00167501 0.5795776 ]\n",
      "Reward for action 12: -70.5828421672253\n",
      "[0, 1]\n",
      "Steps done: 1011\n",
      "SV: [0.70671535 0.00167501 0.5795776 ]\n",
      "Reward for action 20: -66.5828421672253\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 161\n",
      "Steps done: 1012\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 3: -0.10625376166362702\n",
      "[0, 1, 3]\n",
      "Steps done: 1013\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 9: -0.10204697200269056\n",
      "[0, 1, 3, 9]\n",
      "Steps done: 1014\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 13: -0.10249000687774415\n",
      "[0, 1, 9]\n",
      "Steps done: 1015\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 8: -3.9722374687934243\n",
      "[0, 1, 9, 8]\n",
      "Steps done: 1016\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 10: -13.562073274731238\n",
      "[1, 9, 8]\n",
      "Steps done: 1017\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 2: -5.3859652469944\n",
      "[1, 9, 8, 2]\n",
      "Steps done: 1018\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 18: -0.10055347619387674\n",
      "[1, 9, 2]\n",
      "Steps done: 1019\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 4: -0.09897050489228654\n",
      "[1, 9, 2, 4]\n",
      "Steps done: 1020\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 19: -0.11326524273126942\n",
      "[1, 2, 4]\n",
      "Steps done: 1021\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 14: -0.1106094077898379\n",
      "[1, 2]\n",
      "Steps done: 1022\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 4: -0.11326524273126942\n",
      "[1, 2, 4]\n",
      "Steps done: 1023\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 12: -0.11308196869049333\n",
      "[1, 4]\n",
      "Steps done: 1024\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 6: -0.11280439121263694\n",
      "[1, 4, 6]\n",
      "Steps done: 1025\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 3: -0.11817563366843617\n",
      "[1, 4, 6, 3]\n",
      "Steps done: 1026\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 13: -0.11280439121263694\n",
      "[1, 4, 6]\n",
      "Steps done: 1027\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 9: -0.10076263781788505\n",
      "[1, 4, 6, 9]\n",
      "Steps done: 1028\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 5: -0.12031449313734384\n",
      "[1, 4, 6, 9, 5]\n",
      "Steps done: 1029\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 16: -0.12200763583635262\n",
      "[1, 4, 9, 5]\n",
      "Steps done: 1030\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 8: -2.5912251323817266\n",
      "[1, 4, 9, 5, 8]\n",
      "Steps done: 1031\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 0: -1.371461765419543\n",
      "[1, 4, 9, 5, 8, 0]\n",
      "Steps done: 1032\n",
      "SV: [0.01003978 0.02019061 0.02970176]\n",
      "Reward for action 20: 2.628538234580457\n",
      "[1, 4, 9, 5, 8, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 162\n",
      "Steps done: 1033\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 3: -56.233869777790986\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1034\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 13: -33.90399586764756\n",
      "[0, 1]\n",
      "Steps done: 1035\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 2: -9.58309050382606\n",
      "[0, 1, 2]\n",
      "Steps done: 1036\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 12: -33.90399586764756\n",
      "[0, 1]\n",
      "Steps done: 1037\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 2: -9.58309050382606\n",
      "[0, 1, 2]\n",
      "Steps done: 1038\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 10: -2.487537912817308\n",
      "[1, 2]\n",
      "Steps done: 1039\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 5: -184.03342305066\n",
      "[1, 2, 5]\n",
      "Steps done: 1040\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 11: -655.3687802366153\n",
      "[2, 5]\n",
      "Steps done: 1041\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 6: -386.9715043218886\n",
      "[2, 5, 6]\n",
      "Steps done: 1042\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 15: -16.44190165411361\n",
      "[2, 6]\n",
      "Steps done: 1043\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 5: -386.97150432188835\n",
      "[2, 6, 5]\n",
      "Steps done: 1044\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 3: -57.96345759644831\n",
      "[2, 6, 5, 3]\n",
      "Steps done: 1045\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 4: -13.909088077737799\n",
      "[2, 6, 5, 3, 4]\n",
      "Steps done: 1046\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 0: -3.3314735080636435\n",
      "[2, 6, 5, 3, 4, 0]\n",
      "Steps done: 1047\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 12: -32.83127617200975\n",
      "[6, 5, 3, 4, 0]\n",
      "Steps done: 1048\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 2: -3.3314735080636444\n",
      "[6, 5, 3, 4, 0, 2]\n",
      "Steps done: 1049\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 12: -32.83127617200975\n",
      "[6, 5, 3, 4, 0]\n",
      "Steps done: 1050\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 14: -62.24386976281783\n",
      "[6, 5, 3, 0]\n",
      "Steps done: 1051\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 15: -64.35997254568444\n",
      "[6, 3, 0]\n",
      "Steps done: 1052\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 1: -5.729472052454861\n",
      "[6, 3, 0, 1]\n",
      "Steps done: 1053\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 13: -20.17142668164832\n",
      "[6, 0, 1]\n",
      "Steps done: 1054\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 11: -15.566267135146703\n",
      "[6, 0]\n",
      "Steps done: 1055\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 3: -64.35997254568444\n",
      "[6, 0, 3]\n",
      "Steps done: 1056\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 10: -14.485642177480536\n",
      "[6, 3]\n",
      "Steps done: 1057\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 4: -13.866671712090048\n",
      "[6, 3, 4]\n",
      "Steps done: 1058\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Reward for action 0: -6.812786963603997\n",
      "[6, 3, 4, 0]\n",
      "Steps done: 1059\n",
      "SV: [0.01265822 0.01916253 0.43585452]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -2.8127869636039966\n",
      "[6, 3, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 163\n",
      "Steps done: 1060\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 8: -10.417855297369458\n",
      "[0, 1, 8]\n",
      "Steps done: 1061\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 11: -23.80256760956775\n",
      "[0, 8]\n",
      "Steps done: 1062\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 7: -55.745040186319955\n",
      "[0, 8, 7]\n",
      "Steps done: 1063\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 4: -61.18904052283977\n",
      "[0, 8, 7, 4]\n",
      "Steps done: 1064\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 1: -28.04587415514014\n",
      "[0, 8, 7, 4, 1]\n",
      "Steps done: 1065\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 10: -32.39903774226558\n",
      "[8, 7, 4, 1]\n",
      "Steps done: 1066\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 14: -9.106433657526818\n",
      "[8, 7, 1]\n",
      "Steps done: 1067\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 11: -73.75708427353764\n",
      "[8, 7]\n",
      "Steps done: 1068\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 0: -55.74504018631997\n",
      "[8, 7, 0]\n",
      "Steps done: 1069\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 2: -79.44135261729014\n",
      "[8, 7, 0, 2]\n",
      "Steps done: 1070\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 1: -47.292439250092094\n",
      "[8, 7, 0, 2, 1]\n",
      "Steps done: 1071\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 10: -45.96701214935463\n",
      "[8, 7, 2, 1]\n",
      "Steps done: 1072\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 17: -4.138955738476303\n",
      "[8, 2, 1]\n",
      "Steps done: 1073\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 12: -292.3642853901905\n",
      "[8, 1]\n",
      "Steps done: 1074\n",
      "SV: [-0.2979084 -0.35509    1.0085341]\n",
      "Reward for action 20: -288.3642853901905\n",
      "[8, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 164\n",
      "Steps done: 1075\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 8: -256.59218320238483\n",
      "[0, 1, 8]\n",
      "Steps done: 1076\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 2: -60.46155668349056\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 1077\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 4: -50.83342272018888\n",
      "[0, 1, 8, 2, 4]\n",
      "Steps done: 1078\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 14: -60.46155668349056\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 1079\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 7: -91.11172611676406\n",
      "[0, 1, 8, 2, 7]\n",
      "Steps done: 1080\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 12: -209.011341256103\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 1081\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 10: -221.58086050834373\n",
      "[1, 8, 7]\n",
      "Steps done: 1082\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 2: -57.75416048209699\n",
      "[1, 8, 7, 2]\n",
      "Steps done: 1083\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 5: -36.471155836990164\n",
      "[1, 8, 7, 2, 5]\n",
      "Steps done: 1084\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 3: -38.15358332388995\n",
      "[1, 8, 7, 2, 5, 3]\n",
      "Steps done: 1085\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 17: -36.70027550209682\n",
      "[1, 8, 2, 5, 3]\n",
      "Steps done: 1086\n",
      "SV: [-0.52418673  0.42949128  0.74495757]\n",
      "Reward for action 20: -32.70027550209682\n",
      "[1, 8, 2, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 165\n",
      "Steps done: 1087\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 5: -28.415865791190456\n",
      "[0, 1, 5]\n",
      "Steps done: 1088\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 4: -16.34093905635479\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 1089\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 9: -15.001170278730157\n",
      "[0, 1, 5, 4, 9]\n",
      "Steps done: 1090\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 7: -12.298548690419736\n",
      "[0, 1, 5, 4, 9, 7]\n",
      "Steps done: 1091\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 14: -16.856479228336426\n",
      "[0, 1, 5, 9, 7]\n",
      "Steps done: 1092\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 2: -10.50268624353518\n",
      "[0, 1, 5, 9, 7, 2]\n",
      "Steps done: 1093\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 11: -8.804968666493622\n",
      "[0, 5, 9, 7, 2]\n",
      "Steps done: 1094\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 8: -3.05505974280327\n",
      "[0, 5, 9, 7, 2, 8]\n",
      "Steps done: 1095\n",
      "SV: [0.02478988 0.2308238  0.2615499 ]\n",
      "Reward for action 20: 0.94494025719673\n",
      "[0, 5, 9, 7, 2, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 166\n",
      "Steps done: 1096\n",
      "SV: [-0.82447606  0.42485148  1.7240441 ]\n",
      "Reward for action 9: -129.38802702963375\n",
      "[0, 1, 9]\n",
      "Steps done: 1097\n",
      "SV: [-0.82447606  0.42485148  1.7240441 ]\n",
      "Reward for action 10: -332.70340209792363\n",
      "[1, 9]\n",
      "Steps done: 1098\n",
      "SV: [-0.82447606  0.42485148  1.7240441 ]\n",
      "Reward for action 20: -328.70340209792363\n",
      "[1, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 167\n",
      "Steps done: 1099\n",
      "SV: [ 0.19029005 -0.597068   -0.6123552 ]\n",
      "Reward for action 20: -4.7575144752452285\n",
      "[0, 1]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 168\n",
      "Steps done: 1100\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 4: -30.383769114218953\n",
      "[0, 1, 4]\n",
      "Steps done: 1101\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 7: -12.009347002589806\n",
      "[0, 1, 4, 7]\n",
      "Steps done: 1102\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 5: -8.559624724106484\n",
      "[0, 1, 4, 7, 5]\n",
      "Steps done: 1103\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 17: -78.45277824933612\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 1104\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 6: -61.18630482705199\n",
      "[0, 1, 4, 5, 6]\n",
      "Steps done: 1105\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 11: -37.54033388661469\n",
      "[0, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1106\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 7: -2.273800158521593\n",
      "[0, 4, 5, 6, 7]\n",
      "Steps done: 1107\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 17: -37.54033388661469\n",
      "[0, 4, 5, 6]\n",
      "Steps done: 1108\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 10: -120.42191817392364\n",
      "[4, 5, 6]\n",
      "Steps done: 1109\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 2: -44.36210743106464\n",
      "[4, 5, 6, 2]\n",
      "Steps done: 1110\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 0: -10.411820843348242\n",
      "[4, 5, 6, 2, 0]\n",
      "Steps done: 1111\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 14: -23.66152488081458\n",
      "[5, 6, 2, 0]\n",
      "Steps done: 1112\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 7: -1.9423217438491736\n",
      "[5, 6, 2, 0, 7]\n",
      "Steps done: 1113\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 12: -8.034632926644186\n",
      "[5, 6, 0, 7]\n",
      "Steps done: 1114\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 17: -67.88099865182046\n",
      "[5, 6, 0]\n",
      "Steps done: 1115\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 1: -70.08942794568328\n",
      "[5, 6, 0, 1]\n",
      "Steps done: 1116\n",
      "SV: [0.65258485 0.7196151  0.837679  ]\n",
      "Reward for action 20: -66.08942794568328\n",
      "[5, 6, 0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 169\n",
      "Steps done: 1117\n",
      "SV: [ 0.00079213 -0.01443787  0.19765483]\n",
      "Reward for action 9: -9.693990145775231\n",
      "[0, 1, 9]\n",
      "Steps done: 1118\n",
      "SV: [ 0.00079213 -0.01443787  0.19765483]\n",
      "Reward for action 2: -0.9513107937224348\n",
      "[0, 1, 9, 2]\n",
      "Steps done: 1119\n",
      "SV: [ 0.00079213 -0.01443787  0.19765483]\n",
      "Reward for action 6: -2.3195043099813475\n",
      "[0, 1, 9, 2, 6]\n",
      "Steps done: 1120\n",
      "SV: [ 0.00079213 -0.01443787  0.19765483]\n",
      "Reward for action 20: 1.6804956900186525\n",
      "[0, 1, 9, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 170\n",
      "Steps done: 1121\n",
      "SV: [ 0.41656494  0.00393095 -0.69653106]\n",
      "Reward for action 4: -13.134583341667934\n",
      "[0, 1, 4]\n",
      "Steps done: 1122\n",
      "SV: [ 0.41656494  0.00393095 -0.69653106]\n",
      "Reward for action 14: -32.258819721717074\n",
      "[0, 1]\n",
      "Steps done: 1123\n",
      "SV: [ 0.41656494  0.00393095 -0.69653106]\n",
      "Reward for action 20: -28.258819721717074\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 171\n",
      "Steps done: 1124\n",
      "SV: [-0.6711375   0.51645344 -3.20859   ]\n",
      "Reward for action 20: 0.05472713410683472\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 172\n",
      "Steps done: 1125\n",
      "SV: [-0.44100484  0.08667218 -2.142288  ]\n",
      "Reward for action 8: -62.852133512701975\n",
      "[0, 1, 8]\n",
      "Steps done: 1126\n",
      "SV: [-0.44100484  0.08667218 -2.142288  ]\n",
      "Reward for action 18: -418.6875562095011\n",
      "[0, 1]\n",
      "Steps done: 1127\n",
      "SV: [-0.44100484  0.08667218 -2.142288  ]\n",
      "Reward for action 20: -414.6875562095011\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 173\n",
      "Steps done: 1128\n",
      "SV: [ 0.66871506 -0.16598111  0.5600783 ]\n",
      "Reward for action 3: -84.4203589004997\n",
      "[0, 1, 3]\n",
      "Steps done: 1129\n",
      "SV: [ 0.66871506 -0.16598111  0.5600783 ]\n",
      "Reward for action 10: -17.65239277997974\n",
      "[1, 3]\n",
      "Steps done: 1130\n",
      "SV: [ 0.66871506 -0.16598111  0.5600783 ]\n",
      "Reward for action 20: -13.652392779979738\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 174\n",
      "Steps done: 1131\n",
      "SV: [0.364813   0.20683663 0.86807853]\n",
      "Reward for action 3: -154.94920518364825\n",
      "[0, 1, 3]\n",
      "Steps done: 1132\n",
      "SV: [0.364813   0.20683663 0.86807853]\n",
      "Reward for action 11: -562.4001114616257\n",
      "[0, 3]\n",
      "Steps done: 1133\n",
      "SV: [0.364813   0.20683663 0.86807853]\n",
      "Reward for action 20: -558.4001114616257\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 175\n",
      "Steps done: 1134\n",
      "SV: [ 0.80963326  0.04887025 -0.92861336]\n",
      "Reward for action 20: -312.7643255664697\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 176\n",
      "Steps done: 1135\n",
      "SV: [-0.01979844 -0.01888907 -0.470638  ]\n",
      "Reward for action 20: -62.15435884212103\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 177\n",
      "Steps done: 1136\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 2: -83.17553915345334\n",
      "[0, 1, 2]\n",
      "Steps done: 1137\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 4: -82.51096757226992\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 1138\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 14: -83.17553915345334\n",
      "[0, 1, 2]\n",
      "Steps done: 1139\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 12: -85.13935588085853\n",
      "[0, 1]\n",
      "Steps done: 1140\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 3: -236.9877479076961\n",
      "[0, 1, 3]\n",
      "Steps done: 1141\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 10: -285.9110443632459\n",
      "[1, 3]\n",
      "Steps done: 1142\n",
      "SV: [-0.12833512  0.0067809   1.0353339 ]\n",
      "Reward for action 20: -281.9110443632459\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 178\n",
      "Steps done: 1143\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 4: -217.71966214748028\n",
      "[0, 1, 4]\n",
      "Steps done: 1144\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 11: -204.836798804245\n",
      "[0, 4]\n",
      "Steps done: 1145\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 3: -10.729159867357328\n",
      "[0, 4, 3]\n",
      "Steps done: 1146\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 6: -3.1082711744492526\n",
      "[0, 4, 3, 6]\n",
      "Steps done: 1147\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 13: -68.45166772588418\n",
      "[0, 4, 6]\n",
      "Steps done: 1148\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 14: -76.60215825210122\n",
      "[0, 6]\n",
      "Steps done: 1149\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 2: -15.549488336386283\n",
      "[0, 6, 2]\n",
      "Steps done: 1150\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 16: -108.13069497460242\n",
      "[0, 2]\n",
      "Steps done: 1151\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 5: -146.03599718380758\n",
      "[0, 2, 5]\n",
      "Steps done: 1152\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 10: -133.0357350155254\n",
      "[2, 5]\n",
      "Steps done: 1153\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 6: -89.10193418556098\n",
      "[2, 5, 6]\n",
      "Steps done: 1154\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 15: -73.11051131813224\n",
      "[2, 6]\n",
      "Steps done: 1155\n",
      "SV: [ 0.0332774   0.89083576 -0.7456221 ]\n",
      "Reward for action 20: -69.11051131813224\n",
      "[2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 179\n",
      "Steps done: 1156\n",
      "SV: [-0.02463142 -0.03026912 -0.07431407]\n",
      "Reward for action 5: -71.50465862345821\n",
      "[0, 1, 5]\n",
      "Steps done: 1157\n",
      "SV: [-0.02463142 -0.03026912 -0.07431407]\n",
      "Reward for action 11: -150.6732717038065\n",
      "[0, 5]\n",
      "Steps done: 1158\n",
      "SV: [-0.02463142 -0.03026912 -0.07431407]\n",
      "Reward for action 3: -97.09761647360514\n",
      "[0, 5, 3]\n",
      "Steps done: 1159\n",
      "SV: [-0.02463142 -0.03026912 -0.07431407]\n",
      "Reward for action 20: -93.09761647360514\n",
      "[0, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 180\n",
      "Steps done: 1160\n",
      "SV: [-0.01528218  0.05955357  1.004683  ]\n",
      "Reward for action 20: -224.27750930157202\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 181\n",
      "Steps done: 1161\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 6: -40.8362297408767\n",
      "[0, 1, 6]\n",
      "Steps done: 1162\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 3: -61.337694137693354\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 1163\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 4: -62.199873085481535\n",
      "[0, 1, 6, 3, 4]\n",
      "Steps done: 1164\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 7: -71.52823600491624\n",
      "[0, 1, 6, 3, 4, 7]\n",
      "Steps done: 1165\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 2: -87.00217002318209\n",
      "[0, 1, 6, 3, 4, 7, 2]\n",
      "Steps done: 1166\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 12: -71.52823600491624\n",
      "[0, 1, 6, 3, 4, 7]\n",
      "Steps done: 1167\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 14: -86.144166625091\n",
      "[0, 1, 6, 3, 7]\n",
      "Steps done: 1168\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 17: -61.337694137693354\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 1169\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 13: -40.8362297408767\n",
      "[0, 1, 6]\n",
      "Steps done: 1170\n",
      "SV: [0.3535781 0.5096902 0.7990599]\n",
      "Reward for action 20: -36.8362297408767\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 182\n",
      "Steps done: 1171\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 2: -31.285018300801674\n",
      "[0, 1, 2]\n",
      "Steps done: 1172\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 10: -31.10416488905247\n",
      "[1, 2]\n",
      "Steps done: 1173\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 4: -32.27695246540016\n",
      "[1, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1174\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 3: -31.5667919976826\n",
      "[1, 2, 4, 3]\n",
      "Steps done: 1175\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 0: -31.65064814796309\n",
      "[1, 2, 4, 3, 0]\n",
      "Steps done: 1176\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 14: -31.213789281457693\n",
      "[1, 2, 3, 0]\n",
      "Steps done: 1177\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 13: -31.28501830080167\n",
      "[1, 2, 0]\n",
      "Steps done: 1178\n",
      "SV: [-0.04116203  0.13218945 -0.602665  ]\n",
      "Reward for action 20: -27.28501830080167\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 183\n",
      "Steps done: 1179\n",
      "SV: [ 0.16544467 -0.35765326  0.35933742]\n",
      "Reward for action 20: -2.0691991137521235\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 184\n",
      "Steps done: 1180\n",
      "SV: [-0.09282478 -0.26391825 -0.0931679 ]\n",
      "Reward for action 4: -6.2691509886067225\n",
      "[0, 1, 4]\n",
      "Steps done: 1181\n",
      "SV: [-0.09282478 -0.26391825 -0.0931679 ]\n",
      "Reward for action 20: -2.2691509886067225\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 185\n",
      "Steps done: 1182\n",
      "SV: [ 0.04231034 -0.19611774  0.7483327 ]\n",
      "Reward for action 20: -53.341570886813784\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 186\n",
      "Steps done: 1183\n",
      "SV: [ 0.7607505  -0.23922816  3.1977038 ]\n",
      "Reward for action 3: -26.813281336537933\n",
      "[0, 1, 3]\n",
      "Steps done: 1184\n",
      "SV: [ 0.7607505  -0.23922816  3.1977038 ]\n",
      "Reward for action 20: -22.813281336537933\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 187\n",
      "Steps done: 1185\n",
      "SV: [ 0.4600905  -0.33745235  0.66200536]\n",
      "Reward for action 4: -5.9115768780735545\n",
      "[0, 1, 4]\n",
      "Steps done: 1186\n",
      "SV: [ 0.4600905  -0.33745235  0.66200536]\n",
      "Reward for action 3: -23.406632041905652\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 1187\n",
      "SV: [ 0.4600905  -0.33745235  0.66200536]\n",
      "Reward for action 11: -40.843395698590285\n",
      "[0, 4, 3]\n",
      "Steps done: 1188\n",
      "SV: [ 0.4600905  -0.33745235  0.66200536]\n",
      "Reward for action 13: -8.184120077218282\n",
      "[0, 4]\n",
      "Steps done: 1189\n",
      "SV: [ 0.4600905  -0.33745235  0.66200536]\n",
      "Reward for action 1: -5.9115768780735545\n",
      "[0, 4, 1]\n",
      "Steps done: 1190\n",
      "SV: [ 0.4600905  -0.33745235  0.66200536]\n",
      "Reward for action 20: -1.9115768780735545\n",
      "[0, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 188\n",
      "Steps done: 1191\n",
      "SV: [ 0.28929648 -1.0906647  -0.20352907]\n",
      "Reward for action 20: -21.880440793577286\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 189\n",
      "Steps done: 1192\n",
      "SV: [ 0.01880543  0.06055268 -0.6162085 ]\n",
      "Reward for action 20: -24.72005741398876\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 190\n",
      "Steps done: 1193\n",
      "SV: [ 2.4091275  -0.08473382  1.1261998 ]\n",
      "Reward for action 20: -119.53140697150042\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 191\n",
      "Steps done: 1194\n",
      "SV: [ 0.20016353  1.071825   -2.591824  ]\n",
      "Reward for action 20: -670.061484462827\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 192\n",
      "Steps done: 1195\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 2: -16.964817739504564\n",
      "[0, 1, 2]\n",
      "Steps done: 1196\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 6: -36.97829924545752\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 1197\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 16: -16.964817739504564\n",
      "[0, 1, 2]\n",
      "Steps done: 1198\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 6: -36.97829924545752\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 1199\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 10: -30.352261246613537\n",
      "[1, 2, 6]\n",
      "Did target update\n",
      "Steps done: 1200\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 16: -21.239765672813526\n",
      "[1, 2]\n",
      "Steps done: 1201\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 7: -16.879583903688044\n",
      "[1, 2, 7]\n",
      "Steps done: 1202\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 17: -21.239765672813526\n",
      "[1, 2]\n",
      "Steps done: 1203\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 4: -17.935162545334464\n",
      "[1, 2, 4]\n",
      "Steps done: 1204\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 6: -31.261811208518413\n",
      "[1, 2, 4, 6]\n",
      "Steps done: 1205\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 5: -15.407448027574048\n",
      "[1, 2, 4, 6, 5]\n",
      "Steps done: 1206\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 15: -31.261811208518413\n",
      "[1, 2, 4, 6]\n",
      "Steps done: 1207\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 11: -50.59785077747104\n",
      "[2, 4, 6]\n",
      "Steps done: 1208\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 12: -54.790110611605115\n",
      "[4, 6]\n",
      "Steps done: 1209\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 7: -19.31570691719894\n",
      "[4, 6, 7]\n",
      "Steps done: 1210\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 0: -31.109678214850874\n",
      "[4, 6, 7, 0]\n",
      "Steps done: 1211\n",
      "SV: [-0.24024339 -0.2460755  -0.3778497 ]\n",
      "Reward for action 20: -27.109678214850874\n",
      "[4, 6, 7, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 193\n",
      "Steps done: 1212\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 4: -176.96411427714045\n",
      "[0, 1, 4]\n",
      "Steps done: 1213\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 14: -656.8436461203\n",
      "[0, 1]\n",
      "Steps done: 1214\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 3: -225.81737139029408\n",
      "[0, 1, 3]\n",
      "Steps done: 1215\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 11: -126.4607135885734\n",
      "[0, 3]\n",
      "Steps done: 1216\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 4: -64.00052675419164\n",
      "[0, 3, 4]\n",
      "Steps done: 1217\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 2: -75.05288966728416\n",
      "[0, 3, 4, 2]\n",
      "Steps done: 1218\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 8: -25.12047687000999\n",
      "[0, 3, 4, 2, 8]\n",
      "Steps done: 1219\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 13: -35.86753803036116\n",
      "[0, 4, 2, 8]\n",
      "Steps done: 1220\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 14: -54.21719117851282\n",
      "[0, 2, 8]\n",
      "Steps done: 1221\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 10: -9.762210972582867\n",
      "[2, 8]\n",
      "Steps done: 1222\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 3: -17.959121507144093\n",
      "[2, 8, 3]\n",
      "Steps done: 1223\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 12: -232.58074769971256\n",
      "[8, 3]\n",
      "Steps done: 1224\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 4: -35.600278048536524\n",
      "[8, 3, 4]\n",
      "Steps done: 1225\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 6: -18.773926704398384\n",
      "[8, 3, 4, 6]\n",
      "Steps done: 1226\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 0: -17.16628368612095\n",
      "[8, 3, 4, 6, 0]\n",
      "Steps done: 1227\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 14: -15.29425799684886\n",
      "[8, 3, 6, 0]\n",
      "Steps done: 1228\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 16: -11.78531815974166\n",
      "[8, 3, 0]\n",
      "Steps done: 1229\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 2: -26.75849536513505\n",
      "[8, 3, 0, 2]\n",
      "Steps done: 1230\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 12: -11.78531815974166\n",
      "[8, 3, 0]\n",
      "Steps done: 1231\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 5: -52.928682993993036\n",
      "[8, 3, 0, 5]\n",
      "Steps done: 1232\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 18: -100.40558739726244\n",
      "[3, 0, 5]\n",
      "Steps done: 1233\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 10: -70.36834903673915\n",
      "[3, 5]\n",
      "Steps done: 1234\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 4: -37.79585553992543\n",
      "[3, 5, 4]\n",
      "Steps done: 1235\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 9: -50.31968837839354\n",
      "[3, 5, 4, 9]\n",
      "Steps done: 1236\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 14: -71.61170725451375\n",
      "[3, 5, 9]\n",
      "Steps done: 1237\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Reward for action 8: -41.2861072267125\n",
      "[3, 5, 9, 8]\n",
      "Steps done: 1238\n",
      "SV: [-0.13612394 -0.01160661 -0.91917527]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -37.2861072267125\n",
      "[3, 5, 9, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 194\n",
      "Steps done: 1239\n",
      "SV: [-0.01618732  0.01100992  0.16025952]\n",
      "Reward for action 2: -36.120560454467686\n",
      "[0, 1, 2]\n",
      "Steps done: 1240\n",
      "SV: [-0.01618732  0.01100992  0.16025952]\n",
      "Reward for action 20: -32.120560454467686\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 195\n",
      "Steps done: 1241\n",
      "SV: [-0.10435545  0.1116161  -0.37252808]\n",
      "Reward for action 8: -10.278143145716648\n",
      "[0, 1, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1242\n",
      "SV: [-0.10435545  0.1116161  -0.37252808]\n",
      "Reward for action 20: -6.278143145716648\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 196\n",
      "Steps done: 1243\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 3: -139.76131172765147\n",
      "[0, 1, 3]\n",
      "Steps done: 1244\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 5: -237.72092595190588\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 1245\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 7: -108.3973748465929\n",
      "[0, 1, 3, 5, 7]\n",
      "Steps done: 1246\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 4: -120.23539836273686\n",
      "[0, 1, 3, 5, 7, 4]\n",
      "Steps done: 1247\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 15: -59.20555423637852\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 1248\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 8: -22.812981246922025\n",
      "[0, 1, 3, 7, 4, 8]\n",
      "Steps done: 1249\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 18: -59.20555423637852\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 1250\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 11: -40.19504933476033\n",
      "[0, 3, 7, 4]\n",
      "Steps done: 1251\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 6: -159.36505337789694\n",
      "[0, 3, 7, 4, 6]\n",
      "Steps done: 1252\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 8: -69.52658302429366\n",
      "[0, 3, 7, 4, 6, 8]\n",
      "Steps done: 1253\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 16: -9.407339697486535\n",
      "[0, 3, 7, 4, 8]\n",
      "Steps done: 1254\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 2: -10.10006936064506\n",
      "[0, 3, 7, 4, 8, 2]\n",
      "Steps done: 1255\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 6: -62.79275444783468\n",
      "[0, 3, 7, 4, 8, 2, 6]\n",
      "Steps done: 1256\n",
      "SV: [ 0.70553076  0.05035746 -2.0517852 ]\n",
      "Reward for action 20: -58.79275444783468\n",
      "[0, 3, 7, 4, 8, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 197\n",
      "Steps done: 1257\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 4: -8.023386141419033\n",
      "[0, 1, 4]\n",
      "Steps done: 1258\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 10: -6.87891978773604\n",
      "[1, 4]\n",
      "Steps done: 1259\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 9: -165.4530794913951\n",
      "[1, 4, 9]\n",
      "Steps done: 1260\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 7: -119.21794057165928\n",
      "[1, 4, 9, 7]\n",
      "Steps done: 1261\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 17: -165.4530794913951\n",
      "[1, 4, 9]\n",
      "Steps done: 1262\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 19: -6.87891978773604\n",
      "[1, 4]\n",
      "Steps done: 1263\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 6: -6.23785281056412\n",
      "[1, 4, 6]\n",
      "Steps done: 1264\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 2: -8.015061662436503\n",
      "[1, 4, 6, 2]\n",
      "Steps done: 1265\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 11: -9.572344347426123\n",
      "[4, 6, 2]\n",
      "Steps done: 1266\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 0: -9.654110524420656\n",
      "[4, 6, 2, 0]\n",
      "Steps done: 1267\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 5: -6.860439814348883\n",
      "[4, 6, 2, 0, 5]\n",
      "Steps done: 1268\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 3: -7.848873462180174\n",
      "[4, 6, 2, 0, 5, 3]\n",
      "Steps done: 1269\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 13: -6.860439814348883\n",
      "[4, 6, 2, 0, 5]\n",
      "Steps done: 1270\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 8: -8.991991266144716\n",
      "[4, 6, 2, 0, 5, 8]\n",
      "Steps done: 1271\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 7: -8.402225237267555\n",
      "[4, 6, 2, 0, 5, 8, 7]\n",
      "Steps done: 1272\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 9: -43.875393958708344\n",
      "[4, 6, 2, 0, 5, 8, 7, 9]\n",
      "Steps done: 1273\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 19: -8.402225237267555\n",
      "[4, 6, 2, 0, 5, 8, 7]\n",
      "Steps done: 1274\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 15: -10.35900480132882\n",
      "[4, 6, 2, 0, 8, 7]\n",
      "Steps done: 1275\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 18: -8.5775479736564\n",
      "[4, 6, 2, 0, 7]\n",
      "Steps done: 1276\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 14: -7.969086749207098\n",
      "[6, 2, 0, 7]\n",
      "Steps done: 1277\n",
      "SV: [-0.00845378  0.07928395 -0.29721776]\n",
      "Reward for action 20: -3.969086749207098\n",
      "[6, 2, 0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 198\n",
      "Steps done: 1278\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 7: -0.17038827497671605\n",
      "[0, 1, 7]\n",
      "Steps done: 1279\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 4: -0.05876656229469973\n",
      "[0, 1, 7, 4]\n",
      "Steps done: 1280\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 14: -0.17038827497671605\n",
      "[0, 1, 7]\n",
      "Steps done: 1281\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 6: -7.616213823572007\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 1282\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 17: -15.901059824945527\n",
      "[0, 1, 6]\n",
      "Steps done: 1283\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 4: -7.909547081673228\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 1284\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 7: -4.793789462276971\n",
      "[0, 1, 6, 4, 7]\n",
      "Steps done: 1285\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 2: -1.8973528833500537\n",
      "[0, 1, 6, 4, 7, 2]\n",
      "Steps done: 1286\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 12: -4.793789462276971\n",
      "[0, 1, 6, 4, 7]\n",
      "Steps done: 1287\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 2: -1.8973528833500537\n",
      "[0, 1, 6, 4, 7, 2]\n",
      "Steps done: 1288\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 10: -2.6920788796796775\n",
      "[1, 6, 4, 7, 2]\n",
      "Steps done: 1289\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 11: -3.3007930198724544\n",
      "[6, 4, 7, 2]\n",
      "Steps done: 1290\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 14: -3.435486906052556\n",
      "[6, 7, 2]\n",
      "Steps done: 1291\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 17: -5.472001579420414\n",
      "[6, 2]\n",
      "Steps done: 1292\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 1: -7.939326144620965\n",
      "[6, 2, 1]\n",
      "Steps done: 1293\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 12: -49.65372873288938\n",
      "[6, 1]\n",
      "Steps done: 1294\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 5: -64.60581509343439\n",
      "[6, 1, 5]\n",
      "Steps done: 1295\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 4: -22.197939523254917\n",
      "[6, 1, 5, 4]\n",
      "Steps done: 1296\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 2: -7.792969759019199\n",
      "[6, 1, 5, 4, 2]\n",
      "Steps done: 1297\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 16: -0.8040205257185168\n",
      "[1, 5, 4, 2]\n",
      "Steps done: 1298\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 12: -0.7681193796769302\n",
      "[1, 5, 4]\n",
      "Steps done: 1299\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 11: -0.7569775902839443\n",
      "[5, 4]\n",
      "Did target update\n",
      "Steps done: 1300\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 3: -0.6667307882792615\n",
      "[5, 4, 3]\n",
      "Steps done: 1301\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 13: -0.7569775902839443\n",
      "[5, 4]\n",
      "Steps done: 1302\n",
      "SV: [-0.02463009  0.06315447 -0.06780972]\n",
      "Reward for action 20: 3.2430224097160556\n",
      "[5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 199\n",
      "Steps done: 1303\n",
      "SV: [-0.46095425  0.1502571  -0.48322645]\n",
      "Reward for action 6: -8.89398636597891\n",
      "[0, 1, 6]\n",
      "Steps done: 1304\n",
      "SV: [-0.46095425  0.1502571  -0.48322645]\n",
      "Reward for action 8: -1.7113279933697831\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 1305\n",
      "SV: [-0.46095425  0.1502571  -0.48322645]\n",
      "Reward for action 20: 2.288672006630217\n",
      "[0, 1, 6, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 200\n",
      "Steps done: 1306\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 8: -15.86465675163769\n",
      "[0, 1, 8]\n",
      "Steps done: 1307\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 2: -4.301081329460941\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 1308\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 11: -3.8913437002726075\n",
      "[0, 8, 2]\n",
      "Steps done: 1309\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 10: -3.508477553027258\n",
      "[8, 2]\n",
      "Steps done: 1310\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 4: -10.208520356526405\n",
      "[8, 2, 4]\n",
      "Steps done: 1311\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 7: -8.223958187607362\n",
      "[8, 2, 4, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1312\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 12: -5.493813212001314\n",
      "[8, 4, 7]\n",
      "Steps done: 1313\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 1: -2.853909124286371\n",
      "[8, 4, 7, 1]\n",
      "Steps done: 1314\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 5: -3.146489571366075\n",
      "[8, 4, 7, 1, 5]\n",
      "Steps done: 1315\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 9: -3.0625113013425347\n",
      "[8, 4, 7, 1, 5, 9]\n",
      "Steps done: 1316\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 18: -5.230697389747568\n",
      "[4, 7, 1, 5, 9]\n",
      "Steps done: 1317\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 0: -6.000060722963868\n",
      "[4, 7, 1, 5, 9, 0]\n",
      "Steps done: 1318\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 8: -3.59294561752184\n",
      "[4, 7, 1, 5, 9, 0, 8]\n",
      "Steps done: 1319\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 10: -3.0625113013425347\n",
      "[4, 7, 1, 5, 9, 8]\n",
      "Steps done: 1320\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 6: -3.13825318963023\n",
      "[4, 7, 1, 5, 9, 8, 6]\n",
      "Steps done: 1321\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 17: -2.8998670175442376\n",
      "[4, 1, 5, 9, 8, 6]\n",
      "Steps done: 1322\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 19: -2.5363681359410375\n",
      "[4, 1, 5, 8, 6]\n",
      "Steps done: 1323\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 0: -3.184087985157519\n",
      "[4, 1, 5, 8, 6, 0]\n",
      "Steps done: 1324\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 7: -4.070626921318691\n",
      "[4, 1, 5, 8, 6, 0, 7]\n",
      "Steps done: 1325\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 11: -6.091300156847883\n",
      "[4, 5, 8, 6, 0, 7]\n",
      "Steps done: 1326\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 17: -6.620944338901231\n",
      "[4, 5, 8, 6, 0]\n",
      "Steps done: 1327\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 7: -6.091300156847883\n",
      "[4, 5, 8, 6, 0, 7]\n",
      "Steps done: 1328\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 14: -5.043468581477036\n",
      "[5, 8, 6, 0, 7]\n",
      "Steps done: 1329\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 4: -6.091300156847883\n",
      "[5, 8, 6, 0, 7, 4]\n",
      "Steps done: 1330\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 16: -8.317119310121992\n",
      "[5, 8, 0, 7, 4]\n",
      "Steps done: 1331\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Reward for action 10: -6.743930729155725\n",
      "[5, 8, 7, 4]\n",
      "Steps done: 1332\n",
      "SV: [-0.0353946  -0.10675206 -0.16942023]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -2.743930729155725\n",
      "[5, 8, 7, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 201\n",
      "Steps done: 1333\n",
      "SV: [2.4133573 1.2806023 1.9862324]\n",
      "Reward for action 20: -119.62248791541826\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 202\n",
      "Steps done: 1334\n",
      "SV: [-1.4001446 -0.3126033  2.8795412]\n",
      "Reward for action 20: -458.11791867436517\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 203\n",
      "Steps done: 1335\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 6: -16.429242225195797\n",
      "[0, 1, 6]\n",
      "Steps done: 1336\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 4: -29.064724965914174\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 1337\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 5: -16.624299035874387\n",
      "[0, 1, 6, 4, 5]\n",
      "Steps done: 1338\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 15: -29.064724965914174\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 1339\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 14: -16.429242225195797\n",
      "[0, 1, 6]\n",
      "Steps done: 1340\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 10: -22.577436496672227\n",
      "[1, 6]\n",
      "Steps done: 1341\n",
      "SV: [-0.24463645 -0.05515898  0.37459737]\n",
      "Reward for action 20: -18.577436496672227\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 204\n",
      "Steps done: 1342\n",
      "SV: [-0.08582821  0.11114668  0.71671337]\n",
      "Reward for action 6: -30.49654418618833\n",
      "[0, 1, 6]\n",
      "Steps done: 1343\n",
      "SV: [-0.08582821  0.11114668  0.71671337]\n",
      "Reward for action 9: -21.04523761247623\n",
      "[0, 1, 6, 9]\n",
      "Steps done: 1344\n",
      "SV: [-0.08582821  0.11114668  0.71671337]\n",
      "Reward for action 5: -20.9048526796867\n",
      "[0, 1, 6, 9, 5]\n",
      "Steps done: 1345\n",
      "SV: [-0.08582821  0.11114668  0.71671337]\n",
      "Reward for action 20: -16.9048526796867\n",
      "[0, 1, 6, 9, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 205\n",
      "Steps done: 1346\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 2: -486.46282685881596\n",
      "[0, 1, 2]\n",
      "Steps done: 1347\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 4: -264.71849926054153\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 1348\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 10: -250.69002626038915\n",
      "[1, 2, 4]\n",
      "Steps done: 1349\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 3: -224.25485776698747\n",
      "[1, 2, 4, 3]\n",
      "Steps done: 1350\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 12: -161.40283461503068\n",
      "[1, 4, 3]\n",
      "Steps done: 1351\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 2: -224.2548577669874\n",
      "[1, 4, 3, 2]\n",
      "Steps done: 1352\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 14: -388.71632061116725\n",
      "[1, 3, 2]\n",
      "Steps done: 1353\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 0: -247.60979041892475\n",
      "[1, 3, 2, 0]\n",
      "Steps done: 1354\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 11: -111.67519669622058\n",
      "[3, 2, 0]\n",
      "Steps done: 1355\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 13: -411.25715902451157\n",
      "[2, 0]\n",
      "Steps done: 1356\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 5: -92.72931851328998\n",
      "[2, 0, 5]\n",
      "Steps done: 1357\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 12: -169.0884666121852\n",
      "[0, 5]\n",
      "Steps done: 1358\n",
      "SV: [ 1.0189751 -1.9509051 -2.006234 ]\n",
      "Reward for action 20: -165.0884666121852\n",
      "[0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 206\n",
      "Steps done: 1359\n",
      "SV: [-1.1074294  0.9103305 -3.6330278]\n",
      "Reward for action 20: -727.9665797101377\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 207\n",
      "Steps done: 1360\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 4: -29.961157981210647\n",
      "[0, 1, 4]\n",
      "Steps done: 1361\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 10: -23.16487516795558\n",
      "[1, 4]\n",
      "Steps done: 1362\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 2: -18.515278633152246\n",
      "[1, 4, 2]\n",
      "Steps done: 1363\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 11: -12.447644415223385\n",
      "[4, 2]\n",
      "Steps done: 1364\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 5: -12.093689790244078\n",
      "[4, 2, 5]\n",
      "Steps done: 1365\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 7: -19.040901909197004\n",
      "[4, 2, 5, 7]\n",
      "Steps done: 1366\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 6: -18.970855709814707\n",
      "[4, 2, 5, 7, 6]\n",
      "Steps done: 1367\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 15: -17.088155565743413\n",
      "[4, 2, 7, 6]\n",
      "Steps done: 1368\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 0: -16.28533422760524\n",
      "[4, 2, 7, 6, 0]\n",
      "Steps done: 1369\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 16: -19.950223203096325\n",
      "[4, 2, 7, 0]\n",
      "Steps done: 1370\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 14: -20.022573900708167\n",
      "[2, 7, 0]\n",
      "Steps done: 1371\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 3: -9.629355672640711\n",
      "[2, 7, 0, 3]\n",
      "Steps done: 1372\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 10: -26.60089228054383\n",
      "[2, 7, 3]\n",
      "Steps done: 1373\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 6: -9.047362921644343\n",
      "[2, 7, 3, 6]\n",
      "Steps done: 1374\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 0: -17.227423914869718\n",
      "[2, 7, 3, 6, 0]\n",
      "Steps done: 1375\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 10: -9.047362921644343\n",
      "[2, 7, 3, 6]\n",
      "Steps done: 1376\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 12: -38.35295663497839\n",
      "[7, 3, 6]\n",
      "Steps done: 1377\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 8: -22.2941253934195\n",
      "[7, 3, 6, 8]\n",
      "Steps done: 1378\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 16: -6.021793478616943\n",
      "[7, 3, 8]\n",
      "Steps done: 1379\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 1: -9.550105768166533\n",
      "[7, 3, 8, 1]\n",
      "Steps done: 1380\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 18: -5.255777803179623\n",
      "[7, 3, 1]\n",
      "Steps done: 1381\n",
      "SV: [ 0.07755464  0.08239173 -0.5557453 ]\n",
      "Reward for action 20: -1.2557778031796234\n",
      "[7, 3, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 208\n",
      "Steps done: 1382\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 4: -221.59038538601035\n",
      "[0, 1, 4]\n",
      "Steps done: 1383\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 2: -156.8530008197904\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 1384\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 3: -119.62801255805701\n",
      "[0, 1, 4, 2, 3]\n",
      "Steps done: 1385\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 14: -18.332089773395904\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 1386\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 4: -119.62801255805708\n",
      "[0, 1, 2, 3, 4]\n",
      "Steps done: 1387\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 10: -173.25900801162908\n",
      "[1, 2, 3, 4]\n",
      "Steps done: 1388\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 0: -119.62801255805708\n",
      "[1, 2, 3, 4, 0]\n",
      "Steps done: 1389\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 11: -155.79907981825303\n",
      "[2, 3, 4, 0]\n",
      "Steps done: 1390\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 12: -224.74192973753873\n",
      "[3, 4, 0]\n",
      "Steps done: 1391\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 2: -155.79907981825303\n",
      "[3, 4, 0, 2]\n",
      "Steps done: 1392\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 14: -17.560181503529165\n",
      "[3, 0, 2]\n",
      "Steps done: 1393\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 12: -16.841660555175196\n",
      "[3, 0]\n",
      "Steps done: 1394\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 2: -17.560181503529165\n",
      "[3, 0, 2]\n",
      "Steps done: 1395\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 10: -19.01007942449693\n",
      "[3, 2]\n",
      "Steps done: 1396\n",
      "SV: [-0.00896652  0.04699838  0.44966754]\n",
      "Reward for action 20: -15.01007942449693\n",
      "[3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 209\n",
      "Steps done: 1397\n",
      "SV: [ 1.508939   -0.34835973  0.17273055]\n",
      "Reward for action 20: -150.3630005886286\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 210\n",
      "Steps done: 1398\n",
      "SV: [ 0.05488166 -0.01959294  0.11169212]\n",
      "Reward for action 7: -2.816935015672109\n",
      "[0, 1, 7]\n",
      "Steps done: 1399\n",
      "SV: [ 0.05488166 -0.01959294  0.11169212]\n",
      "Reward for action 20: 1.183064984327891\n",
      "[0, 1, 7]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 211\n",
      "Steps done: 1400\n",
      "SV: [-0.9329834 -1.9785193 -1.4887418]\n",
      "Reward for action 2: -14.462070882105213\n",
      "[0, 1, 2]\n",
      "Steps done: 1401\n",
      "SV: [-0.9329834 -1.9785193 -1.4887418]\n",
      "Reward for action 11: -128.83811569055393\n",
      "[0, 2]\n",
      "Steps done: 1402\n",
      "SV: [-0.9329834 -1.9785193 -1.4887418]\n",
      "Reward for action 20: -124.83811569055393\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 212\n",
      "Steps done: 1403\n",
      "SV: [ 0.10992787 -0.17731045 -0.5214649 ]\n",
      "Reward for action 20: -3.773424591925477\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 213\n",
      "Steps done: 1404\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 7: -24.991043073985814\n",
      "[0, 1, 7]\n",
      "Steps done: 1405\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 11: -24.768574399630186\n",
      "[0, 7]\n",
      "Steps done: 1406\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 2: -24.964809966149133\n",
      "[0, 7, 2]\n",
      "Steps done: 1407\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 1: -24.979254800750795\n",
      "[0, 7, 2, 1]\n",
      "Steps done: 1408\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 9: -24.871489727898144\n",
      "[0, 7, 2, 1, 9]\n",
      "Steps done: 1409\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 17: -24.925904538944646\n",
      "[0, 2, 1, 9]\n",
      "Steps done: 1410\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 10: -24.91943078568965\n",
      "[2, 1, 9]\n",
      "Steps done: 1411\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 5: -25.060054642835787\n",
      "[2, 1, 9, 5]\n",
      "Steps done: 1412\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 8: -25.309577592957453\n",
      "[2, 1, 9, 5, 8]\n",
      "Steps done: 1413\n",
      "SV: [ 0.00969792  0.01693438 -0.5536847 ]\n",
      "Reward for action 20: -21.309577592957453\n",
      "[2, 1, 9, 5, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 214\n",
      "Steps done: 1414\n",
      "SV: [-0.64885336  0.5605764   0.5618409 ]\n",
      "Reward for action 6: -106.85216923403772\n",
      "[0, 1, 6]\n",
      "Steps done: 1415\n",
      "SV: [-0.64885336  0.5605764   0.5618409 ]\n",
      "Reward for action 10: -66.76481766178051\n",
      "[1, 6]\n",
      "Steps done: 1416\n",
      "SV: [-0.64885336  0.5605764   0.5618409 ]\n",
      "Reward for action 20: -62.76481766178051\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 215\n",
      "Steps done: 1417\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 4: -106.51114466619029\n",
      "[0, 1, 4]\n",
      "Steps done: 1418\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 2: -83.61086521268092\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 1419\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 3: -36.72789495323333\n",
      "[0, 1, 4, 2, 3]\n",
      "Steps done: 1420\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 10: -12.82321898190408\n",
      "[1, 4, 2, 3]\n",
      "Steps done: 1421\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 13: -65.87589547492288\n",
      "[1, 4, 2]\n",
      "Steps done: 1422\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 11: -8.609045583698189\n",
      "[4, 2]\n",
      "Steps done: 1423\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 1: -65.8758954749229\n",
      "[4, 2, 1]\n",
      "Steps done: 1424\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 12: -122.51397617813606\n",
      "[4, 1]\n",
      "Steps done: 1425\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 3: -40.581484778481\n",
      "[4, 1, 3]\n",
      "Steps done: 1426\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 11: -3.133208628861115\n",
      "[4, 3]\n",
      "Steps done: 1427\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 0: -37.682442415407046\n",
      "[4, 3, 0]\n",
      "Steps done: 1428\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 10: -3.133208628861115\n",
      "[4, 3]\n",
      "Steps done: 1429\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 0: -37.682442415407046\n",
      "[4, 3, 0]\n",
      "Steps done: 1430\n",
      "SV: [0.18713619 0.61569834 0.9702941 ]\n",
      "Reward for action 20: -33.682442415407046\n",
      "[4, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 216\n",
      "Steps done: 1431\n",
      "SV: [-0.88665247  0.3154946  -1.8692797 ]\n",
      "Reward for action 20: -462.19602423016204\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 217\n",
      "Steps done: 1432\n",
      "SV: [-0.60766625  0.43537125 -0.5321601 ]\n",
      "Reward for action 3: -96.13133859074765\n",
      "[0, 1, 3]\n",
      "Steps done: 1433\n",
      "SV: [-0.60766625  0.43537125 -0.5321601 ]\n",
      "Reward for action 2: -14.404930574973989\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 1434\n",
      "SV: [-0.60766625  0.43537125 -0.5321601 ]\n",
      "Reward for action 10: -24.022929618841708\n",
      "[1, 3, 2]\n",
      "Steps done: 1435\n",
      "SV: [-0.60766625  0.43537125 -0.5321601 ]\n",
      "Reward for action 13: -109.36763316770953\n",
      "[1, 2]\n",
      "Steps done: 1436\n",
      "SV: [-0.60766625  0.43537125 -0.5321601 ]\n",
      "Reward for action 0: -9.543147579043817\n",
      "[1, 2, 0]\n",
      "Steps done: 1437\n",
      "SV: [-0.60766625  0.43537125 -0.5321601 ]\n",
      "Reward for action 20: -5.543147579043817\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 218\n",
      "Steps done: 1438\n",
      "SV: [ 0.0184012 -0.0708547 -0.8811011]\n",
      "Reward for action 20: -125.30849582132626\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 219\n",
      "Steps done: 1439\n",
      "SV: [-2.5363686  -0.31644937  3.5270813 ]\n",
      "Reward for action 20: -327.63654929141234\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 220\n",
      "Steps done: 1440\n",
      "SV: [-0.00168173  0.16457564 -0.28458223]\n",
      "Reward for action 8: -5.884982989738631\n",
      "[0, 1, 8]\n",
      "Steps done: 1441\n",
      "SV: [-0.00168173  0.16457564 -0.28458223]\n",
      "Reward for action 20: -1.8849829897386314\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 221\n",
      "Steps done: 1442\n",
      "SV: [-0.86341375 -0.15738206  0.5853896 ]\n",
      "Reward for action 7: -28.509693560320336\n",
      "[0, 1, 7]\n",
      "Steps done: 1443\n",
      "SV: [-0.86341375 -0.15738206  0.5853896 ]\n",
      "Reward for action 11: -109.14476807318331\n",
      "[0, 7]\n",
      "Steps done: 1444\n",
      "SV: [-0.86341375 -0.15738206  0.5853896 ]\n",
      "Reward for action 20: -105.14476807318331\n",
      "[0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 222\n",
      "Steps done: 1445\n",
      "SV: [-0.03811249  0.06948293  0.12564407]\n",
      "Reward for action 4: -4.787200547859015\n",
      "[0, 1, 4]\n",
      "Steps done: 1446\n",
      "SV: [-0.03811249  0.06948293  0.12564407]\n",
      "Reward for action 20: -0.7872005478590154\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 223\n",
      "Steps done: 1447\n",
      "SV: [ 0.60141104  0.29303858 -1.6814126 ]\n",
      "Reward for action 3: -448.93294297970596\n",
      "[0, 1, 3]\n",
      "Steps done: 1448\n",
      "SV: [ 0.60141104  0.29303858 -1.6814126 ]\n",
      "Reward for action 11: -577.1244735131553\n",
      "[0, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1449\n",
      "SV: [ 0.60141104  0.29303858 -1.6814126 ]\n",
      "Reward for action 1: -448.932942979706\n",
      "[0, 3, 1]\n",
      "Steps done: 1450\n",
      "SV: [ 0.60141104  0.29303858 -1.6814126 ]\n",
      "Reward for action 20: -444.932942979706\n",
      "[0, 3, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 224\n",
      "Steps done: 1451\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 7: -29.26696428077166\n",
      "[0, 1, 7]\n",
      "Steps done: 1452\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 4: -37.041487366975836\n",
      "[0, 1, 7, 4]\n",
      "Steps done: 1453\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 14: -29.26696428077166\n",
      "[0, 1, 7]\n",
      "Steps done: 1454\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 3: -51.686687522194134\n",
      "[0, 1, 7, 3]\n",
      "Steps done: 1455\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 5: -26.067608021366727\n",
      "[0, 1, 7, 3, 5]\n",
      "Steps done: 1456\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 17: -20.59620734512\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 1457\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 4: -28.21355633812296\n",
      "[0, 1, 3, 5, 4]\n",
      "Steps done: 1458\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 15: -54.781179895773896\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 1459\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 7: -47.95817077629138\n",
      "[0, 1, 3, 4, 7]\n",
      "Steps done: 1460\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 17: -54.781179895773896\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 1461\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 8: -36.30973598354824\n",
      "[0, 1, 3, 4, 8]\n",
      "Steps done: 1462\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 6: -38.36098997498506\n",
      "[0, 1, 3, 4, 8, 6]\n",
      "Steps done: 1463\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 16: -36.30973598354824\n",
      "[0, 1, 3, 4, 8]\n",
      "Steps done: 1464\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 11: -52.22210382458257\n",
      "[0, 3, 4, 8]\n",
      "Steps done: 1465\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 10: -109.44200291679972\n",
      "[3, 4, 8]\n",
      "Steps done: 1466\n",
      "SV: [-0.02702871  0.49886146  0.31819347]\n",
      "Reward for action 20: -105.44200291679972\n",
      "[3, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 225\n",
      "Steps done: 1467\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 7: -19.170564034995287\n",
      "[0, 1, 7]\n",
      "Steps done: 1468\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 2: -16.109967208851106\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 1469\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 11: -14.57009985582259\n",
      "[0, 7, 2]\n",
      "Steps done: 1470\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 5: -10.37091234052496\n",
      "[0, 7, 2, 5]\n",
      "Steps done: 1471\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 15: -14.57009985582259\n",
      "[0, 7, 2]\n",
      "Steps done: 1472\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 4: -33.87087830443595\n",
      "[0, 7, 2, 4]\n",
      "Steps done: 1473\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 14: -14.57009985582259\n",
      "[0, 7, 2]\n",
      "Steps done: 1474\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 12: -20.521091626829087\n",
      "[0, 7]\n",
      "Steps done: 1475\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 6: -46.03024702587125\n",
      "[0, 7, 6]\n",
      "Steps done: 1476\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 10: -76.18394743466484\n",
      "[7, 6]\n",
      "Steps done: 1477\n",
      "SV: [ 0.29460186 -0.02070009 -0.35471734]\n",
      "Reward for action 20: -72.18394743466484\n",
      "[7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 226\n",
      "Steps done: 1478\n",
      "SV: [-0.2668787  -0.16157104  0.65627235]\n",
      "Reward for action 20: 0.3452868094287642\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 227\n",
      "Steps done: 1479\n",
      "SV: [0.05962416 0.00459041 0.5084021 ]\n",
      "Reward for action 5: -22.87761451446238\n",
      "[0, 1, 5]\n",
      "Steps done: 1480\n",
      "SV: [0.05962416 0.00459041 0.5084021 ]\n",
      "Reward for action 11: -21.669506481671107\n",
      "[0, 5]\n",
      "Steps done: 1481\n",
      "SV: [0.05962416 0.00459041 0.5084021 ]\n",
      "Reward for action 20: -17.669506481671107\n",
      "[0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 228\n",
      "Steps done: 1482\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 2: -413.9104958814793\n",
      "[0, 1, 2]\n",
      "Steps done: 1483\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 11: -386.15886628185774\n",
      "[0, 2]\n",
      "Steps done: 1484\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 4: -5.553205968861631\n",
      "[0, 2, 4]\n",
      "Steps done: 1485\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 5: -8.450970824626395\n",
      "[0, 2, 4, 5]\n",
      "Steps done: 1486\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 3: -6.081929235839271\n",
      "[0, 2, 4, 5, 3]\n",
      "Steps done: 1487\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 1: -19.825336774790337\n",
      "[0, 2, 4, 5, 3, 1]\n",
      "Steps done: 1488\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 13: -21.874417822987674\n",
      "[0, 2, 4, 5, 1]\n",
      "Steps done: 1489\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 14: -357.23793079056315\n",
      "[0, 2, 5, 1]\n",
      "Steps done: 1490\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 3: -230.49043814021087\n",
      "[0, 2, 5, 1, 3]\n",
      "Steps done: 1491\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 10: -160.35400754982646\n",
      "[2, 5, 1, 3]\n",
      "Steps done: 1492\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 15: -299.16872832929414\n",
      "[2, 1, 3]\n",
      "Steps done: 1493\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 11: -389.02427400014494\n",
      "[2, 3]\n",
      "Steps done: 1494\n",
      "SV: [0.7365907 1.3555468 2.8647807]\n",
      "Reward for action 20: -385.02427400014494\n",
      "[2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 229\n",
      "Steps done: 1495\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 7: -68.37787701219784\n",
      "[0, 1, 7]\n",
      "Steps done: 1496\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 6: -27.725953351475304\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 1497\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 2: -38.63543817157848\n",
      "[0, 1, 7, 6, 2]\n",
      "Steps done: 1498\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 16: -68.26810472908024\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 1499\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 12: -68.37787701219784\n",
      "[0, 1, 7]\n",
      "Did target update\n",
      "Steps done: 1500\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 8: -40.79439467417674\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 1501\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 11: -36.783288110049526\n",
      "[0, 7, 8]\n",
      "Steps done: 1502\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 3: -46.75652795689337\n",
      "[0, 7, 8, 3]\n",
      "Steps done: 1503\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 6: -23.669425364128085\n",
      "[0, 7, 8, 3, 6]\n",
      "Steps done: 1504\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 13: -12.684894277578792\n",
      "[0, 7, 8, 6]\n",
      "Steps done: 1505\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 3: -23.66942536412808\n",
      "[0, 7, 8, 6, 3]\n",
      "Steps done: 1506\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 16: -46.75652795689337\n",
      "[0, 7, 8, 3]\n",
      "Steps done: 1507\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 6: -23.669425364128085\n",
      "[0, 7, 8, 3, 6]\n",
      "Steps done: 1508\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 17: -12.917902061565902\n",
      "[0, 8, 3, 6]\n",
      "Steps done: 1509\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 18: -10.37725360095683\n",
      "[0, 3, 6]\n",
      "Steps done: 1510\n",
      "SV: [ 0.17964354 -0.25921804  0.84736204]\n",
      "Reward for action 20: -6.377253600956831\n",
      "[0, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 230\n",
      "Steps done: 1511\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 3: -5.320370443209113\n",
      "[0, 1, 3]\n",
      "Steps done: 1512\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 13: -138.63257585377647\n",
      "[0, 1]\n",
      "Steps done: 1513\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 4: -32.05129097852266\n",
      "[0, 1, 4]\n",
      "Steps done: 1514\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 3: -1.6186132947036158\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 1515\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 14: -5.320370443209113\n",
      "[0, 1, 3]\n",
      "Steps done: 1516\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 10: -107.23276531321535\n",
      "[1, 3]\n",
      "Steps done: 1517\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 0: -5.320370443209111\n",
      "[1, 3, 0]\n",
      "Steps done: 1518\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 13: -138.63257585377647\n",
      "[1, 0]\n",
      "Steps done: 1519\n",
      "SV: [-0.2213742   0.47238523 -2.2659757 ]\n",
      "Reward for action 20: -134.63257585377647\n",
      "[1, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at event: 231\n",
      "Steps done: 1520\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 3: -6.207042991074459\n",
      "[0, 1, 3]\n",
      "Steps done: 1521\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 4: -6.406236967351297\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 1522\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 8: -8.035542829969911\n",
      "[0, 1, 3, 4, 8]\n",
      "Steps done: 1523\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 10: -7.835188883921367\n",
      "[1, 3, 4, 8]\n",
      "Steps done: 1524\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 5: -7.141301747867651\n",
      "[1, 3, 4, 8, 5]\n",
      "Steps done: 1525\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 18: -5.893197114189899\n",
      "[1, 3, 4, 5]\n",
      "Steps done: 1526\n",
      "SV: [-0.21502106 -0.07757919 -0.22876322]\n",
      "Reward for action 20: -1.893197114189899\n",
      "[1, 3, 4, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 232\n",
      "Steps done: 1527\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 8: -30.094137790562346\n",
      "[0, 1, 8]\n",
      "Steps done: 1528\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 3: -20.81696163750055\n",
      "[0, 1, 8, 3]\n",
      "Steps done: 1529\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 10: -25.119581253887763\n",
      "[1, 8, 3]\n",
      "Steps done: 1530\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 13: -69.59678905851983\n",
      "[1, 8]\n",
      "Steps done: 1531\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 9: -35.16880602001943\n",
      "[1, 8, 9]\n",
      "Steps done: 1532\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 6: -10.940747493972847\n",
      "[1, 8, 9, 6]\n",
      "Steps done: 1533\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 11: -9.630248301159842\n",
      "[8, 9, 6]\n",
      "Steps done: 1534\n",
      "SV: [ 0.13190678  0.3811333  -0.22255158]\n",
      "Reward for action 20: -5.630248301159842\n",
      "[8, 9, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 233\n",
      "Steps done: 1535\n",
      "SV: [-0.31996635 -0.05786283 -0.7332038 ]\n",
      "Reward for action 20: -20.564286230714707\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 234\n",
      "Steps done: 1536\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 4: -58.30727640368581\n",
      "[0, 1, 4]\n",
      "Steps done: 1537\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 3: -56.320733317384644\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 1538\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 10: -54.98840092019383\n",
      "[1, 4, 3]\n",
      "Steps done: 1539\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 0: -56.320733317384615\n",
      "[1, 4, 3, 0]\n",
      "Steps done: 1540\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 11: -54.56193564482851\n",
      "[4, 3, 0]\n",
      "Steps done: 1541\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 14: -63.9147298925762\n",
      "[3, 0]\n",
      "Steps done: 1542\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 2: -56.43370787571077\n",
      "[3, 0, 2]\n",
      "Steps done: 1543\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 13: -54.236990014008335\n",
      "[0, 2]\n",
      "Steps done: 1544\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 3: -56.43370787571077\n",
      "[0, 2, 3]\n",
      "Steps done: 1545\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 6: -54.614180818851615\n",
      "[0, 2, 3, 6]\n",
      "Steps done: 1546\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 10: -52.25712255693979\n",
      "[2, 3, 6]\n",
      "Steps done: 1547\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 5: -54.813721691442446\n",
      "[2, 3, 6, 5]\n",
      "Steps done: 1548\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 9: -53.6319630293228\n",
      "[2, 3, 6, 5, 9]\n",
      "Steps done: 1549\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 8: -65.79616578741958\n",
      "[2, 3, 6, 5, 9, 8]\n",
      "Steps done: 1550\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 12: -53.63494298464918\n",
      "[3, 6, 5, 9, 8]\n",
      "Steps done: 1551\n",
      "SV: [ 0.02391404 -0.00174721 -0.83028203]\n",
      "Reward for action 20: -49.63494298464918\n",
      "[3, 6, 5, 9, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 235\n",
      "Steps done: 1552\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 3: -10.946978615138493\n",
      "[0, 1, 3]\n",
      "Steps done: 1553\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 13: -10.911391665672179\n",
      "[0, 1]\n",
      "Steps done: 1554\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 3: -10.946978615138493\n",
      "[0, 1, 3]\n",
      "Steps done: 1555\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 11: -112.85862040457658\n",
      "[0, 3]\n",
      "Steps done: 1556\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 2: -14.357225178575792\n",
      "[0, 3, 2]\n",
      "Steps done: 1557\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 12: -112.85862040457658\n",
      "[0, 3]\n",
      "Steps done: 1558\n",
      "SV: [-1.039213   1.1264173  1.9990752]\n",
      "Reward for action 20: -108.85862040457658\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 236\n",
      "Steps done: 1559\n",
      "SV: [ 0.01212886  0.0212189  -0.77381605]\n",
      "Reward for action 5: -97.56980527487428\n",
      "[0, 1, 5]\n",
      "Steps done: 1560\n",
      "SV: [ 0.01212886  0.0212189  -0.77381605]\n",
      "Reward for action 20: -93.56980527487428\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 237\n",
      "Steps done: 1561\n",
      "SV: [0.07902405 0.08384234 0.1354661 ]\n",
      "Reward for action 7: -2.598253148442796\n",
      "[0, 1, 7]\n",
      "Steps done: 1562\n",
      "SV: [0.07902405 0.08384234 0.1354661 ]\n",
      "Reward for action 6: -2.6428569634423096\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 1563\n",
      "SV: [0.07902405 0.08384234 0.1354661 ]\n",
      "Reward for action 10: -3.043620764158357\n",
      "[1, 7, 6]\n",
      "Steps done: 1564\n",
      "SV: [0.07902405 0.08384234 0.1354661 ]\n",
      "Reward for action 20: 0.9563792358416432\n",
      "[1, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 238\n",
      "Steps done: 1565\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 6: -17.428086396760914\n",
      "[0, 1, 6]\n",
      "Steps done: 1566\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 2: -3.189081363934335\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 1567\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 5: -2.1498377134964115\n",
      "[0, 1, 6, 2, 5]\n",
      "Steps done: 1568\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 3: -0.6721663471729591\n",
      "[0, 1, 6, 2, 5, 3]\n",
      "Steps done: 1569\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 7: -0.319872881449538\n",
      "[0, 1, 6, 2, 5, 3, 7]\n",
      "Steps done: 1570\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 8: -0.574086463216487\n",
      "[0, 1, 6, 2, 5, 3, 7, 8]\n",
      "Steps done: 1571\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 13: -2.352452071749668\n",
      "[0, 1, 6, 2, 5, 7, 8]\n",
      "Steps done: 1572\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 17: -2.649597430615686\n",
      "[0, 1, 6, 2, 5, 8]\n",
      "Steps done: 1573\n",
      "SV: [-0.08313364 -0.07778415 -0.08002516]\n",
      "Reward for action 20: 1.3504025693843138\n",
      "[0, 1, 6, 2, 5, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 239\n",
      "Steps done: 1574\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 2: -15.816104547418599\n",
      "[0, 1, 2]\n",
      "Steps done: 1575\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 11: -32.701202539555666\n",
      "[0, 2]\n",
      "Steps done: 1576\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 3: -60.32590612789754\n",
      "[0, 2, 3]\n",
      "Steps done: 1577\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 1: -36.10912439073641\n",
      "[0, 2, 3, 1]\n",
      "Steps done: 1578\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 12: -65.66260355211553\n",
      "[0, 3, 1]\n",
      "Steps done: 1579\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 4: -17.070583130840976\n",
      "[0, 3, 1, 4]\n",
      "Steps done: 1580\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 2: -15.15198458546586\n",
      "[0, 3, 1, 4, 2]\n",
      "Steps done: 1581\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 12: -17.070583130840976\n",
      "[0, 3, 1, 4]\n",
      "Steps done: 1582\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 2: -15.15198458546586\n",
      "[0, 3, 1, 4, 2]\n",
      "Steps done: 1583\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 5: -4.301119369666017\n",
      "[0, 3, 1, 4, 2, 5]\n",
      "Steps done: 1584\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 15: -15.15198458546586\n",
      "[0, 3, 1, 4, 2]\n",
      "Steps done: 1585\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 12: -17.070583130840976\n",
      "[0, 3, 1, 4]\n",
      "Steps done: 1586\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 2: -15.15198458546586\n",
      "[0, 3, 1, 4, 2]\n",
      "Steps done: 1587\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 5: -4.301119369666017\n",
      "[0, 3, 1, 4, 2, 5]\n",
      "Steps done: 1588\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 13: -0.8662096270832927\n",
      "[0, 1, 4, 2, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1589\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 14: -1.7975799301288662\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 1590\n",
      "SV: [-0.58086157  0.31009087  0.10625368]\n",
      "Reward for action 20: 2.202420069871134\n",
      "[0, 1, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 240\n",
      "Steps done: 1591\n",
      "SV: [-1.1241743e-03  1.4626533e-01 -1.2469033e+00]\n",
      "Reward for action 3: -82.41375190769925\n",
      "[0, 1, 3]\n",
      "Steps done: 1592\n",
      "SV: [-1.1241743e-03  1.4626533e-01 -1.2469033e+00]\n",
      "Reward for action 11: -128.67742534153552\n",
      "[0, 3]\n",
      "Steps done: 1593\n",
      "SV: [-1.1241743e-03  1.4626533e-01 -1.2469033e+00]\n",
      "Reward for action 20: -124.67742534153552\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 241\n",
      "Steps done: 1594\n",
      "SV: [0.00632961 0.00290616 0.3628005 ]\n",
      "Reward for action 3: -2.677620969586717\n",
      "[0, 1, 3]\n",
      "Steps done: 1595\n",
      "SV: [0.00632961 0.00290616 0.3628005 ]\n",
      "Reward for action 11: -8.428220829497931\n",
      "[0, 3]\n",
      "Steps done: 1596\n",
      "SV: [0.00632961 0.00290616 0.3628005 ]\n",
      "Reward for action 4: -9.48666779725069\n",
      "[0, 3, 4]\n",
      "Steps done: 1597\n",
      "SV: [0.00632961 0.00290616 0.3628005 ]\n",
      "Reward for action 6: -8.14413872938312\n",
      "[0, 3, 4, 6]\n",
      "Steps done: 1598\n",
      "SV: [0.00632961 0.00290616 0.3628005 ]\n",
      "Reward for action 20: -4.144138729383119\n",
      "[0, 3, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 242\n",
      "Steps done: 1599\n",
      "SV: [ 0.49836728 -0.8002555   1.8752397 ]\n",
      "Reward for action 20: -445.907840867166\n",
      "[0, 1]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 243\n",
      "Steps done: 1600\n",
      "SV: [1.3615693  0.5576994  0.26071957]\n",
      "Reward for action 2: -14.820844128504003\n",
      "[0, 1, 2]\n",
      "Steps done: 1601\n",
      "SV: [1.3615693  0.5576994  0.26071957]\n",
      "Reward for action 11: -48.62411969708911\n",
      "[0, 2]\n",
      "Steps done: 1602\n",
      "SV: [1.3615693  0.5576994  0.26071957]\n",
      "Reward for action 20: -44.62411969708911\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 244\n",
      "Steps done: 1603\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 5: -9.611108614311549\n",
      "[0, 1, 5]\n",
      "Steps done: 1604\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 2: -9.901599913105358\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 1605\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 15: -11.744426449347667\n",
      "[0, 1, 2]\n",
      "Steps done: 1606\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 9: -12.110622853847937\n",
      "[0, 1, 2, 9]\n",
      "Steps done: 1607\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 7: -16.808261307723342\n",
      "[0, 1, 2, 9, 7]\n",
      "Steps done: 1608\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 19: -20.990917214425647\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 1609\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 6: -13.683515700534322\n",
      "[0, 1, 2, 7, 6]\n",
      "Steps done: 1610\n",
      "SV: [ 0.00181879 -0.06168414  0.5088505 ]\n",
      "Reward for action 20: -9.683515700534322\n",
      "[0, 1, 2, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 245\n",
      "Steps done: 1611\n",
      "SV: [-1.0348116 -0.9469781  5.996676 ]\n",
      "Reward for action 20: -599.9221336858394\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 246\n",
      "Steps done: 1612\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 5: -42.74641428808015\n",
      "[0, 1, 5]\n",
      "Steps done: 1613\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 15: -7.967292835834455\n",
      "[0, 1]\n",
      "Steps done: 1614\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 2: -80.16917743272168\n",
      "[0, 1, 2]\n",
      "Steps done: 1615\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 11: -129.15136363648935\n",
      "[0, 2]\n",
      "Steps done: 1616\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 5: -34.17332817370969\n",
      "[0, 2, 5]\n",
      "Steps done: 1617\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 4: -52.78824859782708\n",
      "[0, 2, 5, 4]\n",
      "Steps done: 1618\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 7: -31.15414626214907\n",
      "[0, 2, 5, 4, 7]\n",
      "Steps done: 1619\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 6: -29.012507342864545\n",
      "[0, 2, 5, 4, 7, 6]\n",
      "Steps done: 1620\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 14: -27.829875230975002\n",
      "[0, 2, 5, 7, 6]\n",
      "Steps done: 1621\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 15: -77.8785886586863\n",
      "[0, 2, 7, 6]\n",
      "Steps done: 1622\n",
      "SV: [-0.24379726  0.6385536   0.10337479]\n",
      "Reward for action 20: -73.8785886586863\n",
      "[0, 2, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 247\n",
      "Steps done: 1623\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 7: -2.5644843530502883\n",
      "[0, 1, 7]\n",
      "Steps done: 1624\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 10: -2.3371598485918965\n",
      "[1, 7]\n",
      "Steps done: 1625\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 6: -24.377919033995425\n",
      "[1, 7, 6]\n",
      "Steps done: 1626\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 3: -38.4412464643183\n",
      "[1, 7, 6, 3]\n",
      "Steps done: 1627\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 5: -23.459923511712727\n",
      "[1, 7, 6, 3, 5]\n",
      "Steps done: 1628\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 13: -18.337401544903216\n",
      "[1, 7, 6, 5]\n",
      "Steps done: 1629\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 0: -14.13000378054331\n",
      "[1, 7, 6, 5, 0]\n",
      "Steps done: 1630\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 16: -1.7887203210107943\n",
      "[1, 7, 5, 0]\n",
      "Steps done: 1631\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 4: -3.572026260914084\n",
      "[1, 7, 5, 0, 4]\n",
      "Steps done: 1632\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 15: -3.9459524507183823\n",
      "[1, 7, 0, 4]\n",
      "Steps done: 1633\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 5: -3.5720262609140843\n",
      "[1, 7, 0, 4, 5]\n",
      "Steps done: 1634\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 6: -18.91032471234891\n",
      "[1, 7, 0, 4, 5, 6]\n",
      "Steps done: 1635\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 16: -3.5720262609140843\n",
      "[1, 7, 0, 4, 5]\n",
      "Steps done: 1636\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 17: -6.04420682481728\n",
      "[1, 0, 4, 5]\n",
      "Steps done: 1637\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 14: -2.8926976972663363\n",
      "[1, 0, 5]\n",
      "Steps done: 1638\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 2: -0.6171250054640625\n",
      "[1, 0, 5, 2]\n",
      "Steps done: 1639\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 10: -1.7139531554081355\n",
      "[1, 5, 2]\n",
      "Steps done: 1640\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 4: -1.266064447581523\n",
      "[1, 5, 2, 4]\n",
      "Steps done: 1641\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 0: -2.3216185249334\n",
      "[1, 5, 2, 4, 0]\n",
      "Steps done: 1642\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 11: -1.475894117770081\n",
      "[5, 2, 4, 0]\n",
      "Steps done: 1643\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 14: -0.7763555252117413\n",
      "[5, 2, 0]\n",
      "Steps done: 1644\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 7: -0.4225021226383048\n",
      "[5, 2, 0, 7]\n",
      "Steps done: 1645\n",
      "SV: [ 0.00328217 -0.03496838 -0.08162554]\n",
      "Reward for action 20: 3.5774978773616954\n",
      "[5, 2, 0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 248\n",
      "Steps done: 1646\n",
      "SV: [-0.00655071  0.33566824 -0.06067744]\n",
      "Reward for action 2: -20.753041244325495\n",
      "[0, 1, 2]\n",
      "Steps done: 1647\n",
      "SV: [-0.00655071  0.33566824 -0.06067744]\n",
      "Reward for action 12: -43.496291605062915\n",
      "[0, 1]\n",
      "Steps done: 1648\n",
      "SV: [-0.00655071  0.33566824 -0.06067744]\n",
      "Reward for action 20: -39.496291605062915\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 249\n",
      "Steps done: 1649\n",
      "SV: [ 0.9622602  -0.60707486  4.6702533 ]\n",
      "Reward for action 20: -36.88523003261179\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 250\n",
      "Steps done: 1650\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 5: -33.402256340035336\n",
      "[0, 1, 5]\n",
      "Steps done: 1651\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 8: -10.110209996075259\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 1652\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 9: -1.3546860172300472\n",
      "[0, 1, 5, 8, 9]\n",
      "Steps done: 1653\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 4: -1.6788813332173103\n",
      "[0, 1, 5, 8, 9, 4]\n",
      "Steps done: 1654\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 14: -1.3546860172300472\n",
      "[0, 1, 5, 8, 9]\n",
      "Steps done: 1655\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 19: -10.110209996075259\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 1656\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 9: -1.3546860172300472\n",
      "[0, 1, 5, 8, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1657\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 11: -1.8913238423160645\n",
      "[0, 5, 8, 9]\n",
      "Steps done: 1658\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 4: -1.791596501307926\n",
      "[0, 5, 8, 9, 4]\n",
      "Steps done: 1659\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 15: -15.193536125244824\n",
      "[0, 8, 9, 4]\n",
      "Steps done: 1660\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 6: -18.190229599130717\n",
      "[0, 8, 9, 4, 6]\n",
      "Steps done: 1661\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 19: -15.666635088883513\n",
      "[0, 8, 4, 6]\n",
      "Steps done: 1662\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 1: -5.526398210185239\n",
      "[0, 8, 4, 6, 1]\n",
      "Steps done: 1663\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 5: -3.3274490185111705\n",
      "[0, 8, 4, 6, 1, 5]\n",
      "Steps done: 1664\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 9: -2.362977374071636\n",
      "[0, 8, 4, 6, 1, 5, 9]\n",
      "Steps done: 1665\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 18: -2.9244599015051485\n",
      "[0, 4, 6, 1, 5, 9]\n",
      "Steps done: 1666\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 16: -3.0234873978108885\n",
      "[0, 4, 1, 5, 9]\n",
      "Steps done: 1667\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 6: -2.9244599015051493\n",
      "[0, 4, 1, 5, 9, 6]\n",
      "Steps done: 1668\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 14: -3.0423901126484627\n",
      "[0, 1, 5, 9, 6]\n",
      "Steps done: 1669\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 15: -15.125256095710878\n",
      "[0, 1, 9, 6]\n",
      "Steps done: 1670\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 3: -12.167136643953718\n",
      "[0, 1, 9, 6, 3]\n",
      "Steps done: 1671\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 5: -3.139549671306921\n",
      "[0, 1, 9, 6, 3, 5]\n",
      "Steps done: 1672\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 19: -9.075171703490474\n",
      "[0, 1, 6, 3, 5]\n",
      "Steps done: 1673\n",
      "SV: [-0.23222701  0.01228978  0.02515126]\n",
      "Reward for action 20: -5.075171703490474\n",
      "[0, 1, 6, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 251\n",
      "Steps done: 1674\n",
      "SV: [0.07250806 0.08583303 0.66221654]\n",
      "Reward for action 7: -30.33570215963881\n",
      "[0, 1, 7]\n",
      "Steps done: 1675\n",
      "SV: [0.07250806 0.08583303 0.66221654]\n",
      "Reward for action 10: -23.969325301401348\n",
      "[1, 7]\n",
      "Steps done: 1676\n",
      "SV: [0.07250806 0.08583303 0.66221654]\n",
      "Reward for action 2: -32.557675971695076\n",
      "[1, 7, 2]\n",
      "Steps done: 1677\n",
      "SV: [0.07250806 0.08583303 0.66221654]\n",
      "Reward for action 11: -37.99335301191462\n",
      "[7, 2]\n",
      "Steps done: 1678\n",
      "SV: [0.07250806 0.08583303 0.66221654]\n",
      "Reward for action 3: -41.73691090406045\n",
      "[7, 2, 3]\n",
      "Steps done: 1679\n",
      "SV: [0.07250806 0.08583303 0.66221654]\n",
      "Reward for action 20: -37.73691090406045\n",
      "[7, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 252\n",
      "Steps done: 1680\n",
      "SV: [-0.0522275   0.4698157   0.04997545]\n",
      "Reward for action 20: -14.143811401191797\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 253\n",
      "Steps done: 1681\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 6: -6.04123704712442\n",
      "[0, 1, 6]\n",
      "Steps done: 1682\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 7: -9.270924701579178\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 1683\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 10: -10.7258726473218\n",
      "[1, 6, 7]\n",
      "Steps done: 1684\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 11: -8.810163277209195\n",
      "[6, 7]\n",
      "Steps done: 1685\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 9: -15.71094111850128\n",
      "[6, 7, 9]\n",
      "Steps done: 1686\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 16: -19.625769008100637\n",
      "[7, 9]\n",
      "Steps done: 1687\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 6: -15.710941118501282\n",
      "[7, 9, 6]\n",
      "Steps done: 1688\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 5: -17.455188430193353\n",
      "[7, 9, 6, 5]\n",
      "Steps done: 1689\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 16: -22.280835957821232\n",
      "[7, 9, 5]\n",
      "Steps done: 1690\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 2: -15.267907413062131\n",
      "[7, 9, 5, 2]\n",
      "Steps done: 1691\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 17: -10.24223386911287\n",
      "[9, 5, 2]\n",
      "Steps done: 1692\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 6: -7.838928772145432\n",
      "[9, 5, 2, 6]\n",
      "Steps done: 1693\n",
      "SV: [ 0.01089023 -0.31531367  0.2907543 ]\n",
      "Reward for action 20: -3.838928772145432\n",
      "[9, 5, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 254\n",
      "Steps done: 1694\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 6: -4.812229639329773\n",
      "[0, 1, 6]\n",
      "Steps done: 1695\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 2: -4.233234712634195\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 1696\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 7: -21.379419846299985\n",
      "[0, 1, 6, 2, 7]\n",
      "Steps done: 1697\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 4: -14.129748146058335\n",
      "[0, 1, 6, 2, 7, 4]\n",
      "Steps done: 1698\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 12: -13.450537339708028\n",
      "[0, 1, 6, 7, 4]\n",
      "Steps done: 1699\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 16: -8.57035521090638\n",
      "[0, 1, 7, 4]\n",
      "Did target update\n",
      "Steps done: 1700\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 6: -13.450537339708028\n",
      "[0, 1, 7, 4, 6]\n",
      "Steps done: 1701\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 11: -20.603584717373508\n",
      "[0, 7, 4, 6]\n",
      "Steps done: 1702\n",
      "SV: [-0.04373351  0.01292848  0.36569747]\n",
      "Reward for action 20: -16.603584717373508\n",
      "[0, 7, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 255\n",
      "Steps done: 1703\n",
      "SV: [-0.36574605  1.270873   -0.01793781]\n",
      "Reward for action 9: -72.81998981364438\n",
      "[0, 1, 9]\n",
      "Steps done: 1704\n",
      "SV: [-0.36574605  1.270873   -0.01793781]\n",
      "Reward for action 5: -40.76294630431972\n",
      "[0, 1, 9, 5]\n",
      "Steps done: 1705\n",
      "SV: [-0.36574605  1.270873   -0.01793781]\n",
      "Reward for action 20: -36.76294630431972\n",
      "[0, 1, 9, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 256\n",
      "Steps done: 1706\n",
      "SV: [ 1.354572   -0.03158016  3.009096  ]\n",
      "Reward for action 7: -109.87777538124297\n",
      "[0, 1, 7]\n",
      "Steps done: 1707\n",
      "SV: [ 1.354572   -0.03158016  3.009096  ]\n",
      "Reward for action 20: -105.87777538124297\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 257\n",
      "Steps done: 1708\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 4: -94.07885860711633\n",
      "[0, 1, 4]\n",
      "Steps done: 1709\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 6: -85.77570133495176\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 1710\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 9: -61.73127830846104\n",
      "[0, 1, 4, 6, 9]\n",
      "Steps done: 1711\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 5: -70.86803684423086\n",
      "[0, 1, 4, 6, 9, 5]\n",
      "Steps done: 1712\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 19: -83.40768852265666\n",
      "[0, 1, 4, 6, 5]\n",
      "Steps done: 1713\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 9: -70.86803684423086\n",
      "[0, 1, 4, 6, 5, 9]\n",
      "Steps done: 1714\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 10: -69.43233009533269\n",
      "[1, 4, 6, 5, 9]\n",
      "Steps done: 1715\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 2: -30.616285320497756\n",
      "[1, 4, 6, 5, 9, 2]\n",
      "Steps done: 1716\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 7: -30.390638913784255\n",
      "[1, 4, 6, 5, 9, 2, 7]\n",
      "Steps done: 1717\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 15: -43.925467246372996\n",
      "[1, 4, 6, 9, 2, 7]\n",
      "Steps done: 1718\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 19: -35.158021617240585\n",
      "[1, 4, 6, 2, 7]\n",
      "Steps done: 1719\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 14: -12.562273359282386\n",
      "[1, 6, 2, 7]\n",
      "Steps done: 1720\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 4: -35.15802161724058\n",
      "[1, 6, 2, 7, 4]\n",
      "Steps done: 1721\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 0: -47.191541327692065\n",
      "[1, 6, 2, 7, 4, 0]\n",
      "Steps done: 1722\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 12: -82.2669634757996\n",
      "[1, 6, 7, 4, 0]\n",
      "Steps done: 1723\n",
      "SV: [ 0.12751396 -0.36914152  0.8796031 ]\n",
      "Reward for action 20: -78.2669634757996\n",
      "[1, 6, 7, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 258\n",
      "Steps done: 1724\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 7: -12.90215145892051\n",
      "[0, 1, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1725\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 2: -11.248218417965504\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 1726\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 12: -12.90215145892051\n",
      "[0, 1, 7]\n",
      "Steps done: 1727\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 10: -10.561176339357411\n",
      "[1, 7]\n",
      "Steps done: 1728\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 8: -12.400635987617132\n",
      "[1, 7, 8]\n",
      "Steps done: 1729\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 0: -10.457733436034006\n",
      "[1, 7, 8, 0]\n",
      "Steps done: 1730\n",
      "SV: [-0.14398418  0.01397985  0.33539432]\n",
      "Reward for action 20: -6.457733436034006\n",
      "[1, 7, 8, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 259\n",
      "Steps done: 1731\n",
      "SV: [ 6.6508408   0.07175791 -3.1715338 ]\n",
      "Reward for action 3: -138.61769454094647\n",
      "[0, 1, 3]\n",
      "Steps done: 1732\n",
      "SV: [ 6.6508408   0.07175791 -3.1715338 ]\n",
      "Reward for action 6: -105.67468538451939\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 1733\n",
      "SV: [ 6.6508408   0.07175791 -3.1715338 ]\n",
      "Reward for action 20: -101.67468538451939\n",
      "[0, 1, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 260\n",
      "Steps done: 1734\n",
      "SV: [ 0.3263996   0.12049345 -0.85661215]\n",
      "Reward for action 3: -9.696552261416349\n",
      "[0, 1, 3]\n",
      "Steps done: 1735\n",
      "SV: [ 0.3263996   0.12049345 -0.85661215]\n",
      "Reward for action 10: -25.274893592972226\n",
      "[1, 3]\n",
      "Steps done: 1736\n",
      "SV: [ 0.3263996   0.12049345 -0.85661215]\n",
      "Reward for action 20: -21.274893592972226\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 261\n",
      "Steps done: 1737\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 8: -12.110359320531195\n",
      "[0, 1, 8]\n",
      "Steps done: 1738\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 10: -10.541734922613161\n",
      "[1, 8]\n",
      "Steps done: 1739\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 2: -10.12804002555686\n",
      "[1, 8, 2]\n",
      "Steps done: 1740\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 11: -10.79100067135054\n",
      "[8, 2]\n",
      "Steps done: 1741\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 7: -59.63028411146512\n",
      "[8, 2, 7]\n",
      "Steps done: 1742\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 6: -38.6353056364152\n",
      "[8, 2, 7, 6]\n",
      "Steps done: 1743\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 9: -79.00363251866975\n",
      "[8, 2, 7, 6, 9]\n",
      "Steps done: 1744\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 5: -66.88941093727155\n",
      "[8, 2, 7, 6, 9, 5]\n",
      "Steps done: 1745\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 12: -89.2522536212712\n",
      "[8, 7, 6, 9, 5]\n",
      "Steps done: 1746\n",
      "SV: [ 0.07166582  0.2237023  -0.2950132 ]\n",
      "Reward for action 20: -85.2522536212712\n",
      "[8, 7, 6, 9, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 262\n",
      "Steps done: 1747\n",
      "SV: [-0.05350422 -0.09898323 -0.08852854]\n",
      "Reward for action 5: -1.8456653533977052\n",
      "[0, 1, 5]\n",
      "Steps done: 1748\n",
      "SV: [-0.05350422 -0.09898323 -0.08852854]\n",
      "Reward for action 11: -9.161263604638124\n",
      "[0, 5]\n",
      "Steps done: 1749\n",
      "SV: [-0.05350422 -0.09898323 -0.08852854]\n",
      "Reward for action 20: -5.161263604638124\n",
      "[0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 263\n",
      "Steps done: 1750\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 6: -96.18463179784581\n",
      "[0, 1, 6]\n",
      "Steps done: 1751\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 5: -37.4537161504425\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 1752\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 3: -24.33880874895931\n",
      "[0, 1, 6, 5, 3]\n",
      "Steps done: 1753\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 11: -16.72399724344968\n",
      "[0, 6, 5, 3]\n",
      "Steps done: 1754\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 10: -12.029636941837365\n",
      "[6, 5, 3]\n",
      "Steps done: 1755\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 1: -28.205399275070217\n",
      "[6, 5, 3, 1]\n",
      "Steps done: 1756\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 7: -32.13701442587879\n",
      "[6, 5, 3, 1, 7]\n",
      "Steps done: 1757\n",
      "SV: [-0.00797784  0.03799985 -0.29348892]\n",
      "Reward for action 20: -28.13701442587879\n",
      "[6, 5, 3, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 264\n",
      "Steps done: 1758\n",
      "SV: [ 0.311386  -0.7037548  1.2172294]\n",
      "Reward for action 3: -536.5144581875097\n",
      "[0, 1, 3]\n",
      "Steps done: 1759\n",
      "SV: [ 0.311386  -0.7037548  1.2172294]\n",
      "Reward for action 20: -532.5144581875097\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 265\n",
      "Steps done: 1760\n",
      "SV: [-0.95506716  0.08919515  1.9218895 ]\n",
      "Reward for action 20: -391.099662237983\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 266\n",
      "Steps done: 1761\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 2: -6.137250032115543\n",
      "[0, 1, 2]\n",
      "Steps done: 1762\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 11: -3.991218952802277\n",
      "[0, 2]\n",
      "Steps done: 1763\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 1: -6.137250032115541\n",
      "[0, 2, 1]\n",
      "Steps done: 1764\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 12: -295.67183890511143\n",
      "[0, 1]\n",
      "Steps done: 1765\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 2: -6.137250032115543\n",
      "[0, 1, 2]\n",
      "Steps done: 1766\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 11: -3.991218952802277\n",
      "[0, 2]\n",
      "Steps done: 1767\n",
      "SV: [-0.6862265  0.3719829  1.9614134]\n",
      "Reward for action 20: 0.008781047197722991\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 267\n",
      "Steps done: 1768\n",
      "SV: [ 0.46257302 -0.4142092  -1.4694408 ]\n",
      "Reward for action 3: -37.366573671197074\n",
      "[0, 1, 3]\n",
      "Steps done: 1769\n",
      "SV: [ 0.46257302 -0.4142092  -1.4694408 ]\n",
      "Reward for action 13: -249.83460021641358\n",
      "[0, 1]\n",
      "Steps done: 1770\n",
      "SV: [ 0.46257302 -0.4142092  -1.4694408 ]\n",
      "Reward for action 20: -245.83460021641358\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 268\n",
      "Steps done: 1771\n",
      "SV: [-0.1417635  -0.3007354   0.26944733]\n",
      "Reward for action 2: -8.301863983919148\n",
      "[0, 1, 2]\n",
      "Steps done: 1772\n",
      "SV: [-0.1417635  -0.3007354   0.26944733]\n",
      "Reward for action 20: -4.301863983919148\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 269\n",
      "Steps done: 1773\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 3: -128.39386224450155\n",
      "[0, 1, 3]\n",
      "Steps done: 1774\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 8: -38.692836905313335\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 1775\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 11: -26.393239151997534\n",
      "[0, 3, 8]\n",
      "Steps done: 1776\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 13: -45.78857496869323\n",
      "[0, 8]\n",
      "Steps done: 1777\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 6: -22.987276886295977\n",
      "[0, 8, 6]\n",
      "Steps done: 1778\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 18: -54.097004415537526\n",
      "[0, 6]\n",
      "Steps done: 1779\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 5: -35.02319727782111\n",
      "[0, 6, 5]\n",
      "Steps done: 1780\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 15: -54.097004415537526\n",
      "[0, 6]\n",
      "Steps done: 1781\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 9: -22.465090059729917\n",
      "[0, 6, 9]\n",
      "Steps done: 1782\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 1: -25.956487032041494\n",
      "[0, 6, 9, 1]\n",
      "Steps done: 1783\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 4: -25.73281928460159\n",
      "[0, 6, 9, 1, 4]\n",
      "Steps done: 1784\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 11: -23.715627966031516\n",
      "[0, 6, 9, 4]\n",
      "Steps done: 1785\n",
      "SV: [-0.00878267  0.35002336 -0.4149769 ]\n",
      "Reward for action 20: -19.715627966031516\n",
      "[0, 6, 9, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 270\n",
      "Steps done: 1786\n",
      "SV: [-0.38645196  0.62415856  0.86410713]\n",
      "Reward for action 2: -105.12662368009335\n",
      "[0, 1, 2]\n",
      "Steps done: 1787\n",
      "SV: [-0.38645196  0.62415856  0.86410713]\n",
      "Reward for action 11: -105.01734974085852\n",
      "[0, 2]\n",
      "Steps done: 1788\n",
      "SV: [-0.38645196  0.62415856  0.86410713]\n",
      "Reward for action 20: -101.01734974085852\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 271\n",
      "Steps done: 1789\n",
      "SV: [-1.502509   -0.10441875  2.7368019 ]\n",
      "Reward for action 2: -256.8873846889333\n",
      "[0, 1, 2]\n",
      "Steps done: 1790\n",
      "SV: [-1.502509   -0.10441875  2.7368019 ]\n",
      "Reward for action 20: -252.8873846889333\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 272\n",
      "Steps done: 1791\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 4: -64.04860492242034\n",
      "[0, 1, 4]\n",
      "Steps done: 1792\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 3: -16.89285132025852\n",
      "[0, 1, 4, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 1793\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 5: -3.932694598083593\n",
      "[0, 1, 4, 3, 5]\n",
      "Steps done: 1794\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 10: -43.901821011451275\n",
      "[1, 4, 3, 5]\n",
      "Steps done: 1795\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 13: -28.112597643078626\n",
      "[1, 4, 5]\n",
      "Steps done: 1796\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 2: -58.67846164749676\n",
      "[1, 4, 5, 2]\n",
      "Steps done: 1797\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 12: -28.112597643078626\n",
      "[1, 4, 5]\n",
      "Steps done: 1798\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 0: -3.5813131984399593\n",
      "[1, 4, 5, 0]\n",
      "Steps done: 1799\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 15: -64.04860492242034\n",
      "[1, 4, 0]\n",
      "Did target update\n",
      "Steps done: 1800\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 2: -7.718148596857871\n",
      "[1, 4, 0, 2]\n",
      "Steps done: 1801\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 10: -25.47731476077338\n",
      "[1, 4, 2]\n",
      "Steps done: 1802\n",
      "SV: [ 0.7314732  -0.14864133 -1.5422491 ]\n",
      "Reward for action 20: -21.47731476077338\n",
      "[1, 4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 273\n",
      "Steps done: 1803\n",
      "SV: [-0.5924244 -0.3670491  1.6117319]\n",
      "Reward for action 6: -333.13478955126084\n",
      "[0, 1, 6]\n",
      "Steps done: 1804\n",
      "SV: [-0.5924244 -0.3670491  1.6117319]\n",
      "Reward for action 5: -340.1269421398776\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 1805\n",
      "SV: [-0.5924244 -0.3670491  1.6117319]\n",
      "Reward for action 11: -416.27333319807605\n",
      "[0, 6, 5]\n",
      "Steps done: 1806\n",
      "SV: [-0.5924244 -0.3670491  1.6117319]\n",
      "Reward for action 20: -412.27333319807605\n",
      "[0, 6, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 274\n",
      "Steps done: 1807\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 7: -45.92739440162154\n",
      "[0, 1, 7]\n",
      "Steps done: 1808\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 6: -36.66833617459817\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 1809\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 9: -30.43880472396249\n",
      "[0, 1, 7, 6, 9]\n",
      "Steps done: 1810\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 2: -35.89859149008629\n",
      "[0, 1, 7, 6, 9, 2]\n",
      "Steps done: 1811\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 17: -37.049102833210426\n",
      "[0, 1, 6, 9, 2]\n",
      "Steps done: 1812\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 3: -30.79040842749521\n",
      "[0, 1, 6, 9, 2, 3]\n",
      "Steps done: 1813\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 7: -32.08247673617377\n",
      "[0, 1, 6, 9, 2, 3, 7]\n",
      "Steps done: 1814\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 10: -32.018224390635396\n",
      "[1, 6, 9, 2, 3, 7]\n",
      "Steps done: 1815\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 4: -37.26415221928022\n",
      "[1, 6, 9, 2, 3, 7, 4]\n",
      "Steps done: 1816\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 0: -38.13908246264843\n",
      "[1, 6, 9, 2, 3, 7, 4, 0]\n",
      "Steps done: 1817\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 14: -32.08247673617376\n",
      "[1, 6, 9, 2, 3, 7, 0]\n",
      "Steps done: 1818\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 5: -29.671279167253548\n",
      "[1, 6, 9, 2, 3, 7, 0, 5]\n",
      "Steps done: 1819\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 16: -29.496976252096815\n",
      "[1, 9, 2, 3, 7, 0, 5]\n",
      "Steps done: 1820\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 17: -28.338341324949617\n",
      "[1, 9, 2, 3, 0, 5]\n",
      "Steps done: 1821\n",
      "SV: [ 0.30125877  0.06058589 -0.55690634]\n",
      "Reward for action 20: -24.338341324949617\n",
      "[1, 9, 2, 3, 0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 275\n",
      "Steps done: 1822\n",
      "SV: [-0.04162838 -0.03189007 -0.22127472]\n",
      "Reward for action 5: -44.859487309384754\n",
      "[0, 1, 5]\n",
      "Steps done: 1823\n",
      "SV: [-0.04162838 -0.03189007 -0.22127472]\n",
      "Reward for action 3: -90.54567831648565\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 1824\n",
      "SV: [-0.04162838 -0.03189007 -0.22127472]\n",
      "Reward for action 20: -86.54567831648565\n",
      "[0, 1, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 276\n",
      "Steps done: 1825\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 6: -5.670389629947623\n",
      "[0, 1, 6]\n",
      "Steps done: 1826\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 2: -7.408702215936919\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 1827\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 4: -6.989115099293189\n",
      "[0, 1, 6, 2, 4]\n",
      "Steps done: 1828\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 14: -7.408702215936919\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 1829\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 5: -6.310037881664735\n",
      "[0, 1, 6, 2, 5]\n",
      "Steps done: 1830\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 11: -5.444131189683684\n",
      "[0, 6, 2, 5]\n",
      "Steps done: 1831\n",
      "SV: [-0.05674211 -0.00115736 -0.25327715]\n",
      "Reward for action 20: -1.4441311896836844\n",
      "[0, 6, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 277\n",
      "Steps done: 1832\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 8: -83.09249496836864\n",
      "[0, 1, 8]\n",
      "Steps done: 1833\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 9: -37.10608692365278\n",
      "[0, 1, 8, 9]\n",
      "Steps done: 1834\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 3: -35.39805376938776\n",
      "[0, 1, 8, 9, 3]\n",
      "Steps done: 1835\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 18: -11.615068957289484\n",
      "[0, 1, 9, 3]\n",
      "Steps done: 1836\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 13: -8.748263949798385\n",
      "[0, 1, 9]\n",
      "Steps done: 1837\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 5: -10.872055548661802\n",
      "[0, 1, 9, 5]\n",
      "Steps done: 1838\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 2: -14.619774242428843\n",
      "[0, 1, 9, 5, 2]\n",
      "Steps done: 1839\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 11: -16.232167534532977\n",
      "[0, 9, 5, 2]\n",
      "Steps done: 1840\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 3: -15.755955154470616\n",
      "[0, 9, 5, 2, 3]\n",
      "Steps done: 1841\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 7: -18.254818916379687\n",
      "[0, 9, 5, 2, 3, 7]\n",
      "Steps done: 1842\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 10: -16.293389233234453\n",
      "[9, 5, 2, 3, 7]\n",
      "Steps done: 1843\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 4: -27.95805819535819\n",
      "[9, 5, 2, 3, 7, 4]\n",
      "Steps done: 1844\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 15: -28.81212920756819\n",
      "[9, 2, 3, 7, 4]\n",
      "Steps done: 1845\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 1: -30.458234281588513\n",
      "[9, 2, 3, 7, 4, 1]\n",
      "Steps done: 1846\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 14: -17.929152016719936\n",
      "[9, 2, 3, 7, 1]\n",
      "Steps done: 1847\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 0: -17.886229049460006\n",
      "[9, 2, 3, 7, 1, 0]\n",
      "Steps done: 1848\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 4: -27.687964778685185\n",
      "[9, 2, 3, 7, 1, 0, 4]\n",
      "Steps done: 1849\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 10: -30.4582342815885\n",
      "[9, 2, 3, 7, 1, 4]\n",
      "Steps done: 1850\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 0: -27.687964778685185\n",
      "[9, 2, 3, 7, 1, 4, 0]\n",
      "Steps done: 1851\n",
      "SV: [-0.6183285  -0.22960623 -0.3392137 ]\n",
      "Reward for action 20: -23.687964778685185\n",
      "[9, 2, 3, 7, 1, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 278\n",
      "Steps done: 1852\n",
      "SV: [ 0.01508917  0.01900132 -0.14893584]\n",
      "Reward for action 4: -1.8546466664852312\n",
      "[0, 1, 4]\n",
      "Steps done: 1853\n",
      "SV: [ 0.01508917  0.01900132 -0.14893584]\n",
      "Reward for action 5: -1.7847119419658173\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 1854\n",
      "SV: [ 0.01508917  0.01900132 -0.14893584]\n",
      "Reward for action 11: -1.8283683085902933\n",
      "[0, 4, 5]\n",
      "Steps done: 1855\n",
      "SV: [ 0.01508917  0.01900132 -0.14893584]\n",
      "Reward for action 6: -1.8209846417011175\n",
      "[0, 4, 5, 6]\n",
      "Steps done: 1856\n",
      "SV: [ 0.01508917  0.01900132 -0.14893584]\n",
      "Reward for action 20: 2.1790153582988827\n",
      "[0, 4, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 279\n",
      "Steps done: 1857\n",
      "SV: [ 0.10079297  0.24430223 -0.08997659]\n",
      "Reward for action 5: -51.50224429211192\n",
      "[0, 1, 5]\n",
      "Steps done: 1858\n",
      "SV: [ 0.10079297  0.24430223 -0.08997659]\n",
      "Reward for action 20: -47.50224429211192\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 280\n",
      "Steps done: 1859\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n",
      "Reward for action 6: -198.1822358146208\n",
      "[0, 1, 6]\n",
      "Steps done: 1860\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 2: -153.50838486904087\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 1861\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n",
      "Reward for action 4: -70.41956944365691\n",
      "[0, 1, 6, 2, 4]\n",
      "Steps done: 1862\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n",
      "Reward for action 5: -55.74854138052555\n",
      "[0, 1, 6, 2, 4, 5]\n",
      "Steps done: 1863\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n",
      "Reward for action 3: -57.4300181285346\n",
      "[0, 1, 6, 2, 4, 5, 3]\n",
      "Steps done: 1864\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n",
      "Reward for action 8: -46.293424098230325\n",
      "[0, 1, 6, 2, 4, 5, 3, 8]\n",
      "Steps done: 1865\n",
      "SV: [-0.04068195 -0.22823    -0.2198917 ]\n",
      "Reward for action 20: -42.293424098230325\n",
      "[0, 1, 6, 2, 4, 5, 3, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 281\n",
      "Steps done: 1866\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 7: -2.7620240798360753\n",
      "[0, 1, 7]\n",
      "Steps done: 1867\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 6: -0.9354529691759331\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 1868\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 17: -18.582856698224234\n",
      "[0, 1, 6]\n",
      "Steps done: 1869\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 16: -7.470849868014263\n",
      "[0, 1]\n",
      "Steps done: 1870\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 8: -3.786961912652165\n",
      "[0, 1, 8]\n",
      "Steps done: 1871\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 10: -4.9253238995226845\n",
      "[1, 8]\n",
      "Steps done: 1872\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 4: -3.8511118619029565\n",
      "[1, 8, 4]\n",
      "Steps done: 1873\n",
      "SV: [0.11692369 0.03249229 0.13605373]\n",
      "Reward for action 20: 0.1488881380970435\n",
      "[1, 8, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 282\n",
      "Steps done: 1874\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 3: -4.590450735670995\n",
      "[0, 1, 3]\n",
      "Steps done: 1875\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 5: -11.829066012699451\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 1876\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 15: -4.590450735670995\n",
      "[0, 1, 3]\n",
      "Steps done: 1877\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 5: -11.829066012699451\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 1878\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 7: -5.672776318788519\n",
      "[0, 1, 3, 5, 7]\n",
      "Steps done: 1879\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 2: -12.681689506129535\n",
      "[0, 1, 3, 5, 7, 2]\n",
      "Steps done: 1880\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 11: -17.983483048966168\n",
      "[0, 3, 5, 7, 2]\n",
      "Steps done: 1881\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 1: -12.681689506129539\n",
      "[0, 3, 5, 7, 2, 1]\n",
      "Steps done: 1882\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 4: -10.162108321541336\n",
      "[0, 3, 5, 7, 2, 1, 4]\n",
      "Steps done: 1883\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 6: -7.373557427426416\n",
      "[0, 3, 5, 7, 2, 1, 4, 6]\n",
      "Steps done: 1884\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 13: -10.31191897818399\n",
      "[0, 5, 7, 2, 1, 4, 6]\n",
      "Steps done: 1885\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 14: -14.345723549250225\n",
      "[0, 5, 7, 2, 1, 6]\n",
      "Steps done: 1886\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 12: -17.512163546945878\n",
      "[0, 5, 7, 1, 6]\n",
      "Steps done: 1887\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 8: -26.539828863500237\n",
      "[0, 5, 7, 1, 6, 8]\n",
      "Steps done: 1888\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 4: -22.570324031241913\n",
      "[0, 5, 7, 1, 6, 8, 4]\n",
      "Steps done: 1889\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 15: -34.58722827194232\n",
      "[0, 7, 1, 6, 8, 4]\n",
      "Steps done: 1890\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 17: -22.77985848161067\n",
      "[0, 1, 6, 8, 4]\n",
      "Steps done: 1891\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 2: -10.743618163231803\n",
      "[0, 1, 6, 8, 4, 2]\n",
      "Steps done: 1892\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 12: -22.77985848161067\n",
      "[0, 1, 6, 8, 4]\n",
      "Steps done: 1893\n",
      "SV: [ 0.30038062 -0.37469587 -0.18270981]\n",
      "Reward for action 20: -18.77985848161067\n",
      "[0, 1, 6, 8, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 283\n",
      "Steps done: 1894\n",
      "SV: [-3.5957928  -0.12007187  7.740374  ]\n",
      "Reward for action 20: -935.0730965304174\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 284\n",
      "Steps done: 1895\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 2: -26.401027779137294\n",
      "[0, 1, 2]\n",
      "Steps done: 1896\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 3: -29.8811784734796\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 1897\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 10: -12.40747443930718\n",
      "[1, 2, 3]\n",
      "Steps done: 1898\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 13: -7.483867140430985\n",
      "[1, 2]\n",
      "Steps done: 1899\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 3: -12.40747443930718\n",
      "[1, 2, 3]\n",
      "Did target update\n",
      "Steps done: 1900\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 13: -7.483867140430985\n",
      "[1, 2]\n",
      "Steps done: 1901\n",
      "SV: [-0.77736837 -0.16899535 -0.7683579 ]\n",
      "Reward for action 20: -3.483867140430985\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 285\n",
      "Steps done: 1902\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 3: -42.157834448863376\n",
      "[0, 1, 3]\n",
      "Steps done: 1903\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 8: -17.094124229986868\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 1904\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 4: -16.566694938866313\n",
      "[0, 1, 3, 8, 4]\n",
      "Steps done: 1905\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 6: -35.154989641113346\n",
      "[0, 1, 3, 8, 4, 6]\n",
      "Steps done: 1906\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 16: -16.566694938866313\n",
      "[0, 1, 3, 8, 4]\n",
      "Steps done: 1907\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 14: -17.094124229986868\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 1908\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 18: -42.157834448863376\n",
      "[0, 1, 3]\n",
      "Steps done: 1909\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 6: -58.26871934212301\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 1910\n",
      "SV: [-0.29210868 -0.0397719  -0.6098247 ]\n",
      "Reward for action 20: -54.26871934212301\n",
      "[0, 1, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 286\n",
      "Steps done: 1911\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 3: -11.24325246004265\n",
      "[0, 1, 3]\n",
      "Steps done: 1912\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 5: -10.599028588531972\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 1913\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 6: -10.579797064694505\n",
      "[0, 1, 3, 5, 6]\n",
      "Steps done: 1914\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 10: -10.87134702751962\n",
      "[1, 3, 5, 6]\n",
      "Steps done: 1915\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 15: -10.991415585339437\n",
      "[1, 3, 6]\n",
      "Steps done: 1916\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 5: -10.871347027519624\n",
      "[1, 3, 6, 5]\n",
      "Steps done: 1917\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 16: -10.678404993558722\n",
      "[1, 3, 5]\n",
      "Steps done: 1918\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 7: -11.161742885694586\n",
      "[1, 3, 5, 7]\n",
      "Steps done: 1919\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 15: -11.48727831000964\n",
      "[1, 3, 7]\n",
      "Steps done: 1920\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 6: -11.343545060788754\n",
      "[1, 3, 7, 6]\n",
      "Steps done: 1921\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 5: -11.196068523498727\n",
      "[1, 3, 7, 6, 5]\n",
      "Steps done: 1922\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 2: -16.267878588967893\n",
      "[1, 3, 7, 6, 5, 2]\n",
      "Steps done: 1923\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 4: -13.982927110150532\n",
      "[1, 3, 7, 6, 5, 2, 4]\n",
      "Steps done: 1924\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 13: -15.994455124302306\n",
      "[1, 7, 6, 5, 2, 4]\n",
      "Steps done: 1925\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 12: -10.575388627941962\n",
      "[1, 7, 6, 5, 4]\n",
      "Steps done: 1926\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 11: -10.294041059972963\n",
      "[7, 6, 5, 4]\n",
      "Steps done: 1927\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 16: -9.826352280062462\n",
      "[7, 5, 4]\n",
      "Steps done: 1928\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 1: -10.39822451410453\n",
      "[7, 5, 4, 1]\n",
      "Steps done: 1929\n",
      "SV: [0.01191386 0.0160007  0.34562454]\n",
      "Reward for action 20: -6.39822451410453\n",
      "[7, 5, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 287\n",
      "Steps done: 1930\n",
      "SV: [-0.9052265  1.1553508  1.9604744]\n",
      "Reward for action 20: -256.6631330636808\n",
      "[0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 288\n",
      "Steps done: 1931\n",
      "SV: [ 0.2803417   0.08300589 -1.1840795 ]\n",
      "Reward for action 4: -93.89452002761784\n",
      "[0, 1, 4]\n",
      "Steps done: 1932\n",
      "SV: [ 0.2803417   0.08300589 -1.1840795 ]\n",
      "Reward for action 2: -110.21234466633999\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 1933\n",
      "SV: [ 0.2803417   0.08300589 -1.1840795 ]\n",
      "Reward for action 20: -106.21234466633999\n",
      "[0, 1, 4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 289\n",
      "Steps done: 1934\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 6: -15.00316759032569\n",
      "[0, 1, 6]\n",
      "Steps done: 1935\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 10: -545.4039001011525\n",
      "[1, 6]\n",
      "Steps done: 1936\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 4: -56.79888736955067\n",
      "[1, 6, 4]\n",
      "Steps done: 1937\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 16: -498.05153857627556\n",
      "[1, 4]\n",
      "Steps done: 1938\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 6: -56.79888736955067\n",
      "[1, 4, 6]\n",
      "Steps done: 1939\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 5: -7.841698192653715\n",
      "[1, 4, 6, 5]\n",
      "Steps done: 1940\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 11: -13.517995663407202\n",
      "[4, 6, 5]\n",
      "Steps done: 1941\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 14: -172.63587451654467\n",
      "[6, 5]\n",
      "Steps done: 1942\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 4: -13.517995663407207\n",
      "[6, 5, 4]\n",
      "Steps done: 1943\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 0: -21.351758760285755\n",
      "[6, 5, 4, 0]\n",
      "Steps done: 1944\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 10: -13.517995663407207\n",
      "[6, 5, 4]\n",
      "Steps done: 1945\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 0: -21.351758760285755\n",
      "[6, 5, 4, 0]\n",
      "Steps done: 1946\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 16: -15.015183213148797\n",
      "[5, 4, 0]\n",
      "Steps done: 1947\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 15: -497.44027407854867\n",
      "[4, 0]\n",
      "Steps done: 1948\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 1: -586.3600591751118\n",
      "[4, 0, 1]\n",
      "Steps done: 1949\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 5: -119.29733258043441\n",
      "[4, 0, 1, 5]\n",
      "Steps done: 1950\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 11: -15.015183213148802\n",
      "[4, 0, 5]\n",
      "Steps done: 1951\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 14: -175.66255796112804\n",
      "[0, 5]\n",
      "Steps done: 1952\n",
      "SV: [-1.2900542  -0.93824655 -3.719093  ]\n",
      "Reward for action 20: -171.66255796112804\n",
      "[0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 290\n",
      "Steps done: 1953\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 3: -12.665272435812252\n",
      "[0, 1, 3]\n",
      "Steps done: 1954\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 13: -147.99315791210148\n",
      "[0, 1]\n",
      "Steps done: 1955\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 3: -12.665272435812252\n",
      "[0, 1, 3]\n",
      "Steps done: 1956\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 11: -573.4380908363042\n",
      "[0, 3]\n",
      "Steps done: 1957\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 2: -14.542187207159808\n",
      "[0, 3, 2]\n",
      "Steps done: 1958\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 10: -55.2546366297281\n",
      "[3, 2]\n",
      "Steps done: 1959\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 4: -80.20987657995826\n",
      "[3, 2, 4]\n",
      "Steps done: 1960\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 14: -55.2546366297281\n",
      "[3, 2]\n",
      "Steps done: 1961\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 4: -80.20987657995826\n",
      "[3, 2, 4]\n",
      "Steps done: 1962\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 14: -55.2546366297281\n",
      "[3, 2]\n",
      "Steps done: 1963\n",
      "SV: [-0.23834167 -1.3187317   0.9839377 ]\n",
      "Reward for action 20: -51.2546366297281\n",
      "[3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 291\n",
      "Steps done: 1964\n",
      "SV: [-0.5366654   0.21436279  0.63287514]\n",
      "Reward for action 20: -82.56539397098067\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 292\n",
      "Steps done: 1965\n",
      "SV: [-0.33308992  1.1189216  -0.10822965]\n",
      "Reward for action 20: -19.202747940090735\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 293\n",
      "Steps done: 1966\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 2: -140.22126516536898\n",
      "[0, 1, 2]\n",
      "Steps done: 1967\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 11: -259.2891316667344\n",
      "[0, 2]\n",
      "Steps done: 1968\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 5: -123.1227132562849\n",
      "[0, 2, 5]\n",
      "Steps done: 1969\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 15: -259.2891316667344\n",
      "[0, 2]\n",
      "Steps done: 1970\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 4: -37.89072365553683\n",
      "[0, 2, 4]\n",
      "Steps done: 1971\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 5: -33.63529313081632\n",
      "[0, 2, 4, 5]\n",
      "Steps done: 1972\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 12: -15.0509007219555\n",
      "[0, 4, 5]\n",
      "Steps done: 1973\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 3: -15.561183920215381\n",
      "[0, 4, 5, 3]\n",
      "Steps done: 1974\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 14: -17.27943426978602\n",
      "[0, 5, 3]\n",
      "Steps done: 1975\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 13: -58.21303053363895\n",
      "[0, 5]\n",
      "Steps done: 1976\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 3: -17.27943426978602\n",
      "[0, 5, 3]\n",
      "Steps done: 1977\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 10: -17.06697755813388\n",
      "[5, 3]\n",
      "Steps done: 1978\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 1: -88.40215495801206\n",
      "[5, 3, 1]\n",
      "Steps done: 1979\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 0: -56.10503600362762\n",
      "[5, 3, 1, 0]\n",
      "Steps done: 1980\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 2: -74.02584005102156\n",
      "[5, 3, 1, 0, 2]\n",
      "Steps done: 1981\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 11: -47.32959058752512\n",
      "[5, 3, 0, 2]\n",
      "Steps done: 1982\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 4: -20.04633607541268\n",
      "[5, 3, 0, 2, 4]\n",
      "Steps done: 1983\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 14: -47.32959058752512\n",
      "[5, 3, 0, 2]\n",
      "Steps done: 1984\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 10: -36.27885148345544\n",
      "[5, 3, 2]\n",
      "Steps done: 1985\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 15: -31.829957014906864\n",
      "[3, 2]\n",
      "Steps done: 1986\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 4: -12.27326632663005\n",
      "[3, 2, 4]\n",
      "Steps done: 1987\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 5: -15.395899102551535\n",
      "[3, 2, 4, 5]\n",
      "Steps done: 1988\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 13: -19.487210314981652\n",
      "[2, 4, 5]\n",
      "Steps done: 1989\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 0: -33.63529313081632\n",
      "[2, 4, 5, 0]\n",
      "Steps done: 1990\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 1: -61.02241636948491\n",
      "[2, 4, 5, 0, 1]\n",
      "Steps done: 1991\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Reward for action 11: -33.63529313081632\n",
      "[2, 4, 5, 0]\n",
      "Steps done: 1992\n",
      "SV: [-0.3432847  -0.52539486 -0.8563448 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -29.635293130816322\n",
      "[2, 4, 5, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 294\n",
      "Steps done: 1993\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 5: -15.207902752391607\n",
      "[0, 1, 5]\n",
      "Steps done: 1994\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 4: -40.19185913588469\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 1995\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 2: -14.626087347811719\n",
      "[0, 1, 5, 4, 2]\n",
      "Steps done: 1996\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 11: -15.806591261517145\n",
      "[0, 5, 4, 2]\n",
      "Steps done: 1997\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 15: -19.505228109524197\n",
      "[0, 4, 2]\n",
      "Steps done: 1998\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 10: -122.21491457441043\n",
      "[4, 2]\n",
      "Steps done: 1999\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 9: -19.326794766950385\n",
      "[4, 2, 9]\n",
      "Did target update\n",
      "Steps done: 2000\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 0: -15.55786490608046\n",
      "[4, 2, 9, 0]\n",
      "Steps done: 2001\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 12: -61.298592444597844\n",
      "[4, 9, 0]\n",
      "Steps done: 2002\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 5: -29.721465840441017\n",
      "[4, 9, 0, 5]\n",
      "Steps done: 2003\n",
      "SV: [-0.0045747  -0.04349473 -0.53173435]\n",
      "Reward for action 20: -25.721465840441017\n",
      "[4, 9, 0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 295\n",
      "Steps done: 2004\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 2: -26.37199784528469\n",
      "[0, 1, 2]\n",
      "Steps done: 2005\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 5: -7.475553653712813\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 2006\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 6: -57.50075017244867\n",
      "[0, 1, 2, 5, 6]\n",
      "Steps done: 2007\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 16: -7.475553653712813\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 2008\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 6: -57.50075017244867\n",
      "[0, 1, 2, 5, 6]\n",
      "Steps done: 2009\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 4: -109.79895383226561\n",
      "[0, 1, 2, 5, 6, 4]\n",
      "Steps done: 2010\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 3: -52.08285513539221\n",
      "[0, 1, 2, 5, 6, 4, 3]\n",
      "Steps done: 2011\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 12: -105.4528793462154\n",
      "[0, 1, 5, 6, 4, 3]\n",
      "Steps done: 2012\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 14: -59.306688822729214\n",
      "[0, 1, 5, 6, 3]\n",
      "Steps done: 2013\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 2: -23.150544651864067\n",
      "[0, 1, 5, 6, 3, 2]\n",
      "Steps done: 2014\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 4: -52.0828551353922\n",
      "[0, 1, 5, 6, 3, 2, 4]\n",
      "Steps done: 2015\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 16: -6.073934425380612\n",
      "[0, 1, 5, 3, 2, 4]\n",
      "Steps done: 2016\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 6: -52.0828551353922\n",
      "[0, 1, 5, 3, 2, 4, 6]\n",
      "Steps done: 2017\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 14: -23.150544651864067\n",
      "[0, 1, 5, 3, 2, 6]\n",
      "Steps done: 2018\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 8: -22.433683087507227\n",
      "[0, 1, 5, 3, 2, 6, 8]\n",
      "Steps done: 2019\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 18: -23.150544651864067\n",
      "[0, 1, 5, 3, 2, 6]\n",
      "Steps done: 2020\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 11: -34.322796126598895\n",
      "[0, 5, 3, 2, 6]\n",
      "Steps done: 2021\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 9: -23.281790506318167\n",
      "[0, 5, 3, 2, 6, 9]\n",
      "Steps done: 2022\n",
      "SV: [-0.17752528  0.42024285 -1.8391072 ]\n",
      "Reward for action 20: -19.281790506318167\n",
      "[0, 5, 3, 2, 6, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 296\n",
      "Steps done: 2023\n",
      "SV: [ 0.5393583 -1.1731023 -2.050428 ]\n",
      "Reward for action 20: -504.6524245897949\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 297\n",
      "Steps done: 2024\n",
      "SV: [-0.27576742 -0.34411278  0.8818733 ]\n",
      "Reward for action 20: -49.9031087735565\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 298\n",
      "Steps done: 2025\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 2: -7.518144505269568\n",
      "[0, 1, 2]\n",
      "Steps done: 2026\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 11: -15.670850602096909\n",
      "[0, 2]\n",
      "Steps done: 2027\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 6: -15.852267739496007\n",
      "[0, 2, 6]\n",
      "Steps done: 2028\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 3: -14.84303571803773\n",
      "[0, 2, 6, 3]\n",
      "Steps done: 2029\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 8: -14.369775971463941\n",
      "[0, 2, 6, 3, 8]\n",
      "Steps done: 2030\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 13: -14.465461870114504\n",
      "[0, 2, 6, 8]\n",
      "Steps done: 2031\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 16: -13.69975522848381\n",
      "[0, 2, 8]\n",
      "Steps done: 2032\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 12: -13.822604758588211\n",
      "[0, 8]\n",
      "Steps done: 2033\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 9: -9.63620298461874\n",
      "[0, 8, 9]\n",
      "Steps done: 2034\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 7: -29.85414224335507\n",
      "[0, 8, 9, 7]\n",
      "Steps done: 2035\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 17: -9.63620298461874\n",
      "[0, 8, 9]\n",
      "Steps done: 2036\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 5: -10.684524117982667\n",
      "[0, 8, 9, 5]\n",
      "Steps done: 2037\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 4: -9.664432007993843\n",
      "[0, 8, 9, 5, 4]\n",
      "Steps done: 2038\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 7: -16.716927537917734\n",
      "[0, 8, 9, 5, 4, 7]\n",
      "Steps done: 2039\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 17: -9.664432007993843\n",
      "[0, 8, 9, 5, 4]\n",
      "Steps done: 2040\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 3: -10.621199365421402\n",
      "[0, 8, 9, 5, 4, 3]\n",
      "Steps done: 2041\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 14: -11.331890160153469\n",
      "[0, 8, 9, 5, 3]\n",
      "Steps done: 2042\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 19: -14.244371836717201\n",
      "[0, 8, 5, 3]\n",
      "Steps done: 2043\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 18: -14.4014881939508\n",
      "[0, 5, 3]\n",
      "Steps done: 2044\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 13: -15.359892901830156\n",
      "[0, 5]\n",
      "Steps done: 2045\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 8: -14.099430482448383\n",
      "[0, 5, 8]\n",
      "Steps done: 2046\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 9: -10.684524117982667\n",
      "[0, 5, 8, 9]\n",
      "Steps done: 2047\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 10: -8.217960012125728\n",
      "[5, 8, 9]\n",
      "Steps done: 2048\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 2: -8.593848943207373\n",
      "[5, 8, 9, 2]\n",
      "Steps done: 2049\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 19: -13.231014730657831\n",
      "[5, 8, 2]\n",
      "Steps done: 2050\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Reward for action 0: -14.070202405689388\n",
      "[5, 8, 2, 0]\n",
      "Steps done: 2051\n",
      "SV: [-0.03139927  0.11857553 -0.39659336]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -10.070202405689388\n",
      "[5, 8, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 299\n",
      "Steps done: 2052\n",
      "SV: [-0.1330771   0.4049587  -0.33304575]\n",
      "Reward for action 6: -28.822265464738898\n",
      "[0, 1, 6]\n",
      "Steps done: 2053\n",
      "SV: [-0.1330771   0.4049587  -0.33304575]\n",
      "Reward for action 7: -20.2552214894916\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 2054\n",
      "SV: [-0.1330771   0.4049587  -0.33304575]\n",
      "Reward for action 2: -1.7129061049211356\n",
      "[0, 1, 6, 7, 2]\n",
      "Steps done: 2055\n",
      "SV: [-0.1330771   0.4049587  -0.33304575]\n",
      "Reward for action 16: -35.93586536075069\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 2056\n",
      "SV: [-0.1330771   0.4049587  -0.33304575]\n",
      "Reward for action 12: -3.214413168577123\n",
      "[0, 1, 7]\n",
      "Steps done: 2057\n",
      "SV: [-0.1330771   0.4049587  -0.33304575]\n",
      "Reward for action 20: 0.7855868314228771\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 300\n",
      "Steps done: 2058\n",
      "SV: [-0.01111038 -0.25465062 -1.0970608 ]\n",
      "Reward for action 3: -123.21115010055519\n",
      "[0, 1, 3]\n",
      "Steps done: 2059\n",
      "SV: [-0.01111038 -0.25465062 -1.0970608 ]\n",
      "Reward for action 8: -179.65852528670672\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 2060\n",
      "SV: [-0.01111038 -0.25465062 -1.0970608 ]\n",
      "Reward for action 18: -123.21115010055519\n",
      "[0, 1, 3]\n",
      "Steps done: 2061\n",
      "SV: [-0.01111038 -0.25465062 -1.0970608 ]\n",
      "Reward for action 20: -119.21115010055519\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 301\n",
      "Steps done: 2062\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 5: -178.92310315440554\n",
      "[0, 1, 5]\n",
      "Steps done: 2063\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 11: -225.17207359820821\n",
      "[0, 5]\n",
      "Steps done: 2064\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 2: -146.61916855431824\n",
      "[0, 5, 2]\n",
      "Steps done: 2065\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 10: -73.89103135523277\n",
      "[5, 2]\n",
      "Steps done: 2066\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 6: -67.40361745867484\n",
      "[5, 2, 6]\n",
      "Steps done: 2067\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 16: -73.89103135523277\n",
      "[5, 2]\n",
      "Steps done: 2068\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 4: -61.27325456062784\n",
      "[5, 2, 4]\n",
      "Steps done: 2069\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 0: -107.16734752211411\n",
      "[5, 2, 4, 0]\n",
      "Steps done: 2070\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 10: -61.27325456062784\n",
      "[5, 2, 4]\n",
      "Steps done: 2071\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 1: -63.370445179699914\n",
      "[5, 2, 4, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2072\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 14: -68.27502811858318\n",
      "[5, 2, 1]\n",
      "Steps done: 2073\n",
      "SV: [-0.22619991 -0.61490625 -0.9603713 ]\n",
      "Reward for action 20: -64.27502811858318\n",
      "[5, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 302\n",
      "Steps done: 2074\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 2: -22.458959353220138\n",
      "[0, 1, 2]\n",
      "Steps done: 2075\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 11: -47.95263632008183\n",
      "[0, 2]\n",
      "Steps done: 2076\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 7: -16.785614615410832\n",
      "[0, 2, 7]\n",
      "Steps done: 2077\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 10: -14.044054512778597\n",
      "[2, 7]\n",
      "Steps done: 2078\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 4: -12.337923823290925\n",
      "[2, 7, 4]\n",
      "Steps done: 2079\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 14: -14.044054512778597\n",
      "[2, 7]\n",
      "Steps done: 2080\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 6: -17.35154983682807\n",
      "[2, 7, 6]\n",
      "Steps done: 2081\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 1: -19.704416254948395\n",
      "[2, 7, 6, 1]\n",
      "Steps done: 2082\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 0: -21.55852731329726\n",
      "[2, 7, 6, 1, 0]\n",
      "Steps done: 2083\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 9: -22.169756438847816\n",
      "[2, 7, 6, 1, 0, 9]\n",
      "Steps done: 2084\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 19: -21.55852731329726\n",
      "[2, 7, 6, 1, 0]\n",
      "Steps done: 2085\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 4: -15.575053458343271\n",
      "[2, 7, 6, 1, 0, 4]\n",
      "Steps done: 2086\n",
      "SV: [-0.04933641 -0.21267274 -0.60733837]\n",
      "Reward for action 20: -11.575053458343271\n",
      "[2, 7, 6, 1, 0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 303\n",
      "Steps done: 2087\n",
      "SV: [ 0.0029342   0.12665255 -0.4902527 ]\n",
      "Reward for action 20: -16.99659676093493\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 305\n",
      "Steps done: 2088\n",
      "SV: [ 0.5444166   0.46475473 -1.0823716 ]\n",
      "Reward for action 20: -204.46530480111463\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 306\n",
      "Steps done: 2089\n",
      "SV: [-0.02757805  0.11481636  1.7766126 ]\n",
      "Reward for action 4: -48.39184251368209\n",
      "[0, 1, 4]\n",
      "Steps done: 2090\n",
      "SV: [-0.02757805  0.11481636  1.7766126 ]\n",
      "Reward for action 11: -319.8546944577606\n",
      "[0, 4]\n",
      "Steps done: 2091\n",
      "SV: [-0.02757805  0.11481636  1.7766126 ]\n",
      "Reward for action 20: -315.8546944577606\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 307\n",
      "Steps done: 2092\n",
      "SV: [-0.75585073  0.5412221   0.00802355]\n",
      "Reward for action 4: -6.644684021212305\n",
      "[0, 1, 4]\n",
      "Steps done: 2093\n",
      "SV: [-0.75585073  0.5412221   0.00802355]\n",
      "Reward for action 20: -2.6446840212123046\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 308\n",
      "Steps done: 2094\n",
      "SV: [ 0.16091898  0.0636408  -0.11164216]\n",
      "Reward for action 3: -3.8714331008605503\n",
      "[0, 1, 3]\n",
      "Steps done: 2095\n",
      "SV: [ 0.16091898  0.0636408  -0.11164216]\n",
      "Reward for action 20: 0.12856689913944974\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 309\n",
      "Steps done: 2096\n",
      "SV: [-0.23546357  0.5085565   1.3229799 ]\n",
      "Reward for action 4: -11.485210065297117\n",
      "[0, 1, 4]\n",
      "Steps done: 2097\n",
      "SV: [-0.23546357  0.5085565   1.3229799 ]\n",
      "Reward for action 20: -7.485210065297117\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 310\n",
      "Steps done: 2098\n",
      "SV: [-0.54704946  0.11179166 -1.3053998 ]\n",
      "Reward for action 2: -163.82021936476224\n",
      "[0, 1, 2]\n",
      "Steps done: 2099\n",
      "SV: [-0.54704946  0.11179166 -1.3053998 ]\n",
      "Reward for action 11: -164.29797786033808\n",
      "[0, 2]\n",
      "Did target update\n",
      "Steps done: 2100\n",
      "SV: [-0.54704946  0.11179166 -1.3053998 ]\n",
      "Reward for action 20: -160.29797786033808\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 311\n",
      "Steps done: 2101\n",
      "SV: [-0.22319567  0.22612408 -0.43565965]\n",
      "Reward for action 2: -23.005756494508738\n",
      "[0, 1, 2]\n",
      "Steps done: 2102\n",
      "SV: [-0.22319567  0.22612408 -0.43565965]\n",
      "Reward for action 4: -23.591237333227827\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 2103\n",
      "SV: [-0.22319567  0.22612408 -0.43565965]\n",
      "Reward for action 8: -21.096977505405732\n",
      "[0, 1, 2, 4, 8]\n",
      "Steps done: 2104\n",
      "SV: [-0.22319567  0.22612408 -0.43565965]\n",
      "Reward for action 10: -21.08616926511159\n",
      "[1, 2, 4, 8]\n",
      "Steps done: 2105\n",
      "SV: [-0.22319567  0.22612408 -0.43565965]\n",
      "Reward for action 3: -20.73620386371154\n",
      "[1, 2, 4, 8, 3]\n",
      "Steps done: 2106\n",
      "SV: [-0.22319567  0.22612408 -0.43565965]\n",
      "Reward for action 20: -16.73620386371154\n",
      "[1, 2, 4, 8, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 312\n",
      "Steps done: 2107\n",
      "SV: [ 0.5869699  -0.36590585 -1.8561195 ]\n",
      "Reward for action 20: -613.8398328342071\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 313\n",
      "Steps done: 2108\n",
      "SV: [ 0.11573452 -0.05121875 -0.4814999 ]\n",
      "Reward for action 6: -22.93743170537376\n",
      "[0, 1, 6]\n",
      "Steps done: 2109\n",
      "SV: [ 0.11573452 -0.05121875 -0.4814999 ]\n",
      "Reward for action 4: -20.2150252824972\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 2110\n",
      "SV: [ 0.11573452 -0.05121875 -0.4814999 ]\n",
      "Reward for action 20: -16.2150252824972\n",
      "[0, 1, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 314\n",
      "Steps done: 2111\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 4: -139.95923747244154\n",
      "[0, 1, 4]\n",
      "Steps done: 2112\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 10: -271.66500467885356\n",
      "[1, 4]\n",
      "Steps done: 2113\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 2: -7.756913855621744\n",
      "[1, 4, 2]\n",
      "Steps done: 2114\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 6: -10.313186765898324\n",
      "[1, 4, 2, 6]\n",
      "Steps done: 2115\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 12: -5.844134103165707\n",
      "[1, 4, 6]\n",
      "Steps done: 2116\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 0: -9.448899041540585\n",
      "[1, 4, 6, 0]\n",
      "Steps done: 2117\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 11: -10.861060707667672\n",
      "[4, 6, 0]\n",
      "Steps done: 2118\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 2: -20.021509098711572\n",
      "[4, 6, 0, 2]\n",
      "Steps done: 2119\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 16: -8.9262395114127\n",
      "[4, 0, 2]\n",
      "Steps done: 2120\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 12: -50.99894391237208\n",
      "[4, 0]\n",
      "Steps done: 2121\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 1: -139.95923747244154\n",
      "[4, 0, 1]\n",
      "Steps done: 2122\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 6: -9.44889904154059\n",
      "[4, 0, 1, 6]\n",
      "Steps done: 2123\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 7: -6.934728621659095\n",
      "[4, 0, 1, 6, 7]\n",
      "Steps done: 2124\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 2: -8.000196293266725\n",
      "[4, 0, 1, 6, 7, 2]\n",
      "Steps done: 2125\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 10: -24.131880995268865\n",
      "[4, 1, 6, 7, 2]\n",
      "Steps done: 2126\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 5: -28.916938017609603\n",
      "[4, 1, 6, 7, 2, 5]\n",
      "Steps done: 2127\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 16: -14.853660768191912\n",
      "[4, 1, 7, 2, 5]\n",
      "Steps done: 2128\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 15: -6.234065942326857\n",
      "[4, 1, 7, 2]\n",
      "Steps done: 2129\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 5: -14.853660768191912\n",
      "[4, 1, 7, 2, 5]\n",
      "Steps done: 2130\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 6: -28.916938017609613\n",
      "[4, 1, 7, 2, 5, 6]\n",
      "Steps done: 2131\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 3: -43.689439247559946\n",
      "[4, 1, 7, 2, 5, 6, 3]\n",
      "Steps done: 2132\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 13: -28.916938017609613\n",
      "[4, 1, 7, 2, 5, 6]\n",
      "Steps done: 2133\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 15: -24.131880995268876\n",
      "[4, 1, 7, 2, 6]\n",
      "Steps done: 2134\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 12: -26.919079156097464\n",
      "[4, 1, 7, 6]\n",
      "Steps done: 2135\n",
      "SV: [ 0.01201328  1.0243396  -0.9718016 ]\n",
      "Reward for action 20: -22.919079156097464\n",
      "[4, 1, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 315\n",
      "Steps done: 2136\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 5: -53.56404931436098\n",
      "[0, 1, 5]\n",
      "Steps done: 2137\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 3: -53.57534325625862\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 2138\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 2: -53.85167728877986\n",
      "[0, 1, 5, 3, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2139\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 12: -53.57534325625862\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 2140\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 13: -53.56404931436098\n",
      "[0, 1, 5]\n",
      "Steps done: 2141\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 6: -53.530641806964915\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 2142\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 15: -53.52187130622765\n",
      "[0, 1, 6]\n",
      "Steps done: 2143\n",
      "SV: [ 0.01019116  0.01244777 -0.8143857 ]\n",
      "Reward for action 20: -49.52187130622765\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 316\n",
      "Steps done: 2144\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 6: -250.21484093175957\n",
      "[0, 1, 6]\n",
      "Steps done: 2145\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 2: -168.67587630848172\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 2146\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 10: -373.70785679792357\n",
      "[1, 6, 2]\n",
      "Steps done: 2147\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 11: -437.8837664663139\n",
      "[6, 2]\n",
      "Steps done: 2148\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 3: -111.05186714095952\n",
      "[6, 2, 3]\n",
      "Steps done: 2149\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 5: -151.4481613823425\n",
      "[6, 2, 3, 5]\n",
      "Steps done: 2150\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 15: -111.05186714095952\n",
      "[6, 2, 3]\n",
      "Steps done: 2151\n",
      "SV: [-0.48197603  0.3215386   1.5831994 ]\n",
      "Reward for action 20: -107.05186714095952\n",
      "[6, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 317\n",
      "Steps done: 2152\n",
      "SV: [-0.4876178 -0.5875096  1.6041325]\n",
      "Reward for action 3: -89.92580347449136\n",
      "[0, 1, 3]\n",
      "Steps done: 2153\n",
      "SV: [-0.4876178 -0.5875096  1.6041325]\n",
      "Reward for action 13: -170.33449916299801\n",
      "[0, 1]\n",
      "Steps done: 2154\n",
      "SV: [-0.4876178 -0.5875096  1.6041325]\n",
      "Reward for action 20: -166.33449916299801\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 318\n",
      "Steps done: 2155\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 2: -34.48382808145853\n",
      "[0, 1, 2]\n",
      "Steps done: 2156\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 4: -50.58259543283138\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 2157\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 11: -43.99942105664548\n",
      "[0, 2, 4]\n",
      "Steps done: 2158\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 10: -222.3591908133781\n",
      "[2, 4]\n",
      "Steps done: 2159\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 3: -330.76145591071054\n",
      "[2, 4, 3]\n",
      "Steps done: 2160\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 13: -222.3591908133781\n",
      "[2, 4]\n",
      "Steps done: 2161\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 0: -43.99942105664548\n",
      "[2, 4, 0]\n",
      "Steps done: 2162\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 10: -222.3591908133781\n",
      "[2, 4]\n",
      "Steps done: 2163\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 0: -43.99942105664548\n",
      "[2, 4, 0]\n",
      "Steps done: 2164\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 3: -150.567450793363\n",
      "[2, 4, 0, 3]\n",
      "Steps done: 2165\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 10: -330.76145591071054\n",
      "[2, 4, 3]\n",
      "Steps done: 2166\n",
      "SV: [0.4905087  0.02849927 0.5911345 ]\n",
      "Reward for action 20: -326.76145591071054\n",
      "[2, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 319\n",
      "Steps done: 2167\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 3: -80.46320798350084\n",
      "[0, 1, 3]\n",
      "Steps done: 2168\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 4: -51.44415366712484\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 2169\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 5: -49.07752335006855\n",
      "[0, 1, 3, 4, 5]\n",
      "Steps done: 2170\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 11: -51.52298856549292\n",
      "[0, 3, 4, 5]\n",
      "Steps done: 2171\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 15: -58.34065937856423\n",
      "[0, 3, 4]\n",
      "Steps done: 2172\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 2: -55.89920345480633\n",
      "[0, 3, 4, 2]\n",
      "Steps done: 2173\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 5: -52.97640393050916\n",
      "[0, 3, 4, 2, 5]\n",
      "Steps done: 2174\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 13: -48.40958201520198\n",
      "[0, 4, 2, 5]\n",
      "Steps done: 2175\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 14: -44.65145778243192\n",
      "[0, 2, 5]\n",
      "Steps done: 2176\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 4: -48.40958201520197\n",
      "[0, 2, 5, 4]\n",
      "Steps done: 2177\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 10: -57.81774889217521\n",
      "[2, 5, 4]\n",
      "Steps done: 2178\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 1: -55.38490093289087\n",
      "[2, 5, 4, 1]\n",
      "Steps done: 2179\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 3: -55.36097428792728\n",
      "[2, 5, 4, 1, 3]\n",
      "Steps done: 2180\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 11: -58.00744609330123\n",
      "[2, 5, 4, 3]\n",
      "Steps done: 2181\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 15: -71.36515670248106\n",
      "[2, 4, 3]\n",
      "Steps done: 2182\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 13: -77.98666997398084\n",
      "[2, 4]\n",
      "Steps done: 2183\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 1: -56.323784022413506\n",
      "[2, 4, 1]\n",
      "Steps done: 2184\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 5: -55.3849009328909\n",
      "[2, 4, 1, 5]\n",
      "Steps done: 2185\n",
      "SV: [-0.66062665  0.22185491 -0.46094033]\n",
      "Reward for action 20: -51.3849009328909\n",
      "[2, 4, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 320\n",
      "Steps done: 2186\n",
      "SV: [ 0.36920747  0.45290256 -0.4112025 ]\n",
      "Reward for action 5: -41.078271397901915\n",
      "[0, 1, 5]\n",
      "Steps done: 2187\n",
      "SV: [ 0.36920747  0.45290256 -0.4112025 ]\n",
      "Reward for action 11: -41.87478564493202\n",
      "[0, 5]\n",
      "Steps done: 2188\n",
      "SV: [ 0.36920747  0.45290256 -0.4112025 ]\n",
      "Reward for action 1: -41.078271397901915\n",
      "[0, 5, 1]\n",
      "Steps done: 2189\n",
      "SV: [ 0.36920747  0.45290256 -0.4112025 ]\n",
      "Reward for action 2: -41.673339392590975\n",
      "[0, 5, 1, 2]\n",
      "Steps done: 2190\n",
      "SV: [ 0.36920747  0.45290256 -0.4112025 ]\n",
      "Reward for action 15: -41.56069750558428\n",
      "[0, 1, 2]\n",
      "Steps done: 2191\n",
      "SV: [ 0.36920747  0.45290256 -0.4112025 ]\n",
      "Reward for action 20: -37.56069750558428\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 321\n",
      "Steps done: 2192\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 4: -36.88027161539632\n",
      "[0, 1, 4]\n",
      "Steps done: 2193\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 14: -78.05181436354376\n",
      "[0, 1]\n",
      "Steps done: 2194\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 9: -180.15254081967137\n",
      "[0, 1, 9]\n",
      "Steps done: 2195\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 7: -41.66235635705553\n",
      "[0, 1, 9, 7]\n",
      "Steps done: 2196\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 10: -223.68414112771052\n",
      "[1, 9, 7]\n",
      "Steps done: 2197\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 17: -19.30408484135472\n",
      "[1, 9]\n",
      "Steps done: 2198\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 4: -74.2423189197971\n",
      "[1, 9, 4]\n",
      "Steps done: 2199\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 5: -102.69190856866919\n",
      "[1, 9, 4, 5]\n",
      "Did target update\n",
      "Steps done: 2200\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 19: -286.54714945491\n",
      "[1, 4, 5]\n",
      "Steps done: 2201\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 8: -515.240503110194\n",
      "[1, 4, 5, 8]\n",
      "Steps done: 2202\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 9: -296.9316173073005\n",
      "[1, 4, 5, 8, 9]\n",
      "Steps done: 2203\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 2: -223.36606285363388\n",
      "[1, 4, 5, 8, 9, 2]\n",
      "Steps done: 2204\n",
      "SV: [ 0.28821012 -0.3156094  -1.1715714 ]\n",
      "Reward for action 20: -219.36606285363388\n",
      "[1, 4, 5, 8, 9, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 322\n",
      "Steps done: 2205\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 3: -151.53923180607245\n",
      "[0, 1, 3]\n",
      "Steps done: 2206\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 7: -95.88609562623634\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 2207\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 17: -151.53923180607245\n",
      "[0, 1, 3]\n",
      "Steps done: 2208\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 7: -95.88609562623634\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 2209\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 2: -76.77496663294434\n",
      "[0, 1, 3, 7, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2210\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 5: -59.39310118322044\n",
      "[0, 1, 3, 7, 2, 5]\n",
      "Steps done: 2211\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 6: -60.43026756239023\n",
      "[0, 1, 3, 7, 2, 5, 6]\n",
      "Steps done: 2212\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 13: -72.16689853600032\n",
      "[0, 1, 7, 2, 5, 6]\n",
      "Steps done: 2213\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 11: -60.62417621755269\n",
      "[0, 7, 2, 5, 6]\n",
      "Steps done: 2214\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 3: -50.44585594935202\n",
      "[0, 7, 2, 5, 6, 3]\n",
      "Steps done: 2215\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 16: -49.38305901128463\n",
      "[0, 7, 2, 5, 3]\n",
      "Steps done: 2216\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 6: -50.44585594935202\n",
      "[0, 7, 2, 5, 3, 6]\n",
      "Steps done: 2217\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 4: -40.395995617426316\n",
      "[0, 7, 2, 5, 3, 6, 4]\n",
      "Steps done: 2218\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 16: -36.998720389228986\n",
      "[0, 7, 2, 5, 3, 4]\n",
      "Steps done: 2219\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 13: -56.25484626894141\n",
      "[0, 7, 2, 5, 4]\n",
      "Steps done: 2220\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 6: -54.05116774975616\n",
      "[0, 7, 2, 5, 4, 6]\n",
      "Steps done: 2221\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 14: -60.62417621755269\n",
      "[0, 7, 2, 5, 6]\n",
      "Steps done: 2222\n",
      "SV: [0.7161004  0.21201113 0.42526588]\n",
      "Reward for action 20: -56.62417621755269\n",
      "[0, 7, 2, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 323\n",
      "Steps done: 2223\n",
      "SV: [-0.5643806  -0.00149383  0.6648099 ]\n",
      "Reward for action 20: 2.953925075338228\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 324\n",
      "Steps done: 2224\n",
      "SV: [-0.14118169 -0.01852742 -0.6083395 ]\n",
      "Reward for action 4: -32.14626287655818\n",
      "[0, 1, 4]\n",
      "Steps done: 2225\n",
      "SV: [-0.14118169 -0.01852742 -0.6083395 ]\n",
      "Reward for action 11: -31.86254636548234\n",
      "[0, 4]\n",
      "Steps done: 2226\n",
      "SV: [-0.14118169 -0.01852742 -0.6083395 ]\n",
      "Reward for action 1: -32.14626287655818\n",
      "[0, 4, 1]\n",
      "Steps done: 2227\n",
      "SV: [-0.14118169 -0.01852742 -0.6083395 ]\n",
      "Reward for action 20: -28.146262876558183\n",
      "[0, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 325\n",
      "Steps done: 2228\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 8: -37.75976834622698\n",
      "[0, 1, 8]\n",
      "Steps done: 2229\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 11: -20.830709368884307\n",
      "[0, 8]\n",
      "Steps done: 2230\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 1: -37.75976834622698\n",
      "[0, 8, 1]\n",
      "Steps done: 2231\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 11: -20.830709368884307\n",
      "[0, 8]\n",
      "Steps done: 2232\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 9: -64.14116917501975\n",
      "[0, 8, 9]\n",
      "Steps done: 2233\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 2: -47.829851256395116\n",
      "[0, 8, 9, 2]\n",
      "Steps done: 2234\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 19: -18.706684727862914\n",
      "[0, 8, 2]\n",
      "Steps done: 2235\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 9: -47.829851256395116\n",
      "[0, 8, 2, 9]\n",
      "Steps done: 2236\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 10: -64.76964365976207\n",
      "[8, 2, 9]\n",
      "Steps done: 2237\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 7: -32.191420904982635\n",
      "[8, 2, 9, 7]\n",
      "Steps done: 2238\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 1: -23.840952112689095\n",
      "[8, 2, 9, 7, 1]\n",
      "Steps done: 2239\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 17: -56.347341140349926\n",
      "[8, 2, 9, 1]\n",
      "Steps done: 2240\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 0: -44.90581090341814\n",
      "[8, 2, 9, 1, 0]\n",
      "Steps done: 2241\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 6: -30.451014499047783\n",
      "[8, 2, 9, 1, 0, 6]\n",
      "Steps done: 2242\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 4: -17.670668430619706\n",
      "[8, 2, 9, 1, 0, 6, 4]\n",
      "Steps done: 2243\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 18: -11.15440629452072\n",
      "[2, 9, 1, 0, 6, 4]\n",
      "Steps done: 2244\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 11: -9.510487697780862\n",
      "[2, 9, 0, 6, 4]\n",
      "Steps done: 2245\n",
      "SV: [ 0.10587361 -0.34502864 -0.28963065]\n",
      "Reward for action 20: -5.510487697780862\n",
      "[2, 9, 0, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 326\n",
      "Steps done: 2246\n",
      "SV: [-0.08317714  0.08497812 -0.90454674]\n",
      "Reward for action 20: -75.73371181799051\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 327\n",
      "Steps done: 2247\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 4: -92.26587192473573\n",
      "[0, 1, 4]\n",
      "Steps done: 2248\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 2: -112.49475690131857\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 2249\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 14: -27.970758020843867\n",
      "[0, 1, 2]\n",
      "Steps done: 2250\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 10: -297.1851187282797\n",
      "[1, 2]\n",
      "Steps done: 2251\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 4: -41.072312953411426\n",
      "[1, 2, 4]\n",
      "Steps done: 2252\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 12: -61.24164964883086\n",
      "[1, 4]\n",
      "Steps done: 2253\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 2: -41.072312953411426\n",
      "[1, 4, 2]\n",
      "Steps done: 2254\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 14: -297.1851187282797\n",
      "[1, 2]\n",
      "Steps done: 2255\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 6: -10.893083482156339\n",
      "[1, 2, 6]\n",
      "Steps done: 2256\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 12: -108.00791015068143\n",
      "[1, 6]\n",
      "Steps done: 2257\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 3: -10.77192816078588\n",
      "[1, 6, 3]\n",
      "Steps done: 2258\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 2: -16.17444393265134\n",
      "[1, 6, 3, 2]\n",
      "Steps done: 2259\n",
      "SV: [-0.5093546 -1.6763839 -4.736645 ]\n",
      "Reward for action 20: -12.17444393265134\n",
      "[1, 6, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 329\n",
      "Steps done: 2260\n",
      "SV: [0.07559565 0.00909248 0.02906647]\n",
      "Reward for action 2: -0.9519928343037193\n",
      "[0, 1, 2]\n",
      "Steps done: 2261\n",
      "SV: [0.07559565 0.00909248 0.02906647]\n",
      "Reward for action 12: -4.248763929751253\n",
      "[0, 1]\n",
      "Steps done: 2262\n",
      "SV: [0.07559565 0.00909248 0.02906647]\n",
      "Reward for action 20: -0.24876392975125317\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 330\n",
      "Steps done: 2263\n",
      "SV: [ 0.34126306  0.26776752 -2.9245358 ]\n",
      "Reward for action 9: -505.85583854359186\n",
      "[0, 1, 9]\n",
      "Steps done: 2264\n",
      "SV: [ 0.34126306  0.26776752 -2.9245358 ]\n",
      "Reward for action 11: -810.440305383732\n",
      "[0, 9]\n",
      "Steps done: 2265\n",
      "SV: [ 0.34126306  0.26776752 -2.9245358 ]\n",
      "Reward for action 5: -693.7369566040092\n",
      "[0, 9, 5]\n",
      "Steps done: 2266\n",
      "SV: [ 0.34126306  0.26776752 -2.9245358 ]\n",
      "Reward for action 1: -570.1159835452177\n",
      "[0, 9, 5, 1]\n",
      "Steps done: 2267\n",
      "SV: [ 0.34126306  0.26776752 -2.9245358 ]\n",
      "Reward for action 6: -600.7882709620643\n",
      "[0, 9, 5, 1, 6]\n",
      "Steps done: 2268\n",
      "SV: [ 0.34126306  0.26776752 -2.9245358 ]\n",
      "Reward for action 20: -596.7882709620643\n",
      "[0, 9, 5, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 331\n",
      "Steps done: 2269\n",
      "SV: [-1.1784791  0.6576705  3.2651684]\n",
      "Reward for action 3: -379.25707307712173\n",
      "[0, 1, 3]\n",
      "Steps done: 2270\n",
      "SV: [-1.1784791  0.6576705  3.2651684]\n",
      "Reward for action 11: -288.1431306413786\n",
      "[0, 3]\n",
      "Steps done: 2271\n",
      "SV: [-1.1784791  0.6576705  3.2651684]\n",
      "Reward for action 20: -284.1431306413786\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 332\n",
      "Steps done: 2272\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 5: -553.9467449998114\n",
      "[0, 1, 5]\n",
      "Steps done: 2273\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 15: -133.11800032850394\n",
      "[0, 1]\n",
      "Steps done: 2274\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 5: -553.9467449998114\n",
      "[0, 1, 5]\n",
      "Steps done: 2275\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 11: -1722.4095588489072\n",
      "[0, 5]\n",
      "Steps done: 2276\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 1: -553.9467449998114\n",
      "[0, 5, 1]\n",
      "Steps done: 2277\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 11: -1722.4095588489072\n",
      "[0, 5]\n",
      "Steps done: 2278\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 7: -651.2296724069215\n",
      "[0, 5, 7]\n",
      "Steps done: 2279\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 3: -165.96399077673988\n",
      "[0, 5, 7, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2280\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 17: -578.7450539907172\n",
      "[0, 5, 3]\n",
      "Steps done: 2281\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 1: -279.25879484021004\n",
      "[0, 5, 3, 1]\n",
      "Steps done: 2282\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 11: -578.7450539907172\n",
      "[0, 5, 3]\n",
      "Steps done: 2283\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 13: -1722.4095588489072\n",
      "[0, 5]\n",
      "Steps done: 2284\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 1: -553.9467449998114\n",
      "[0, 5, 1]\n",
      "Steps done: 2285\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 2: -431.39310595796945\n",
      "[0, 5, 1, 2]\n",
      "Steps done: 2286\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 11: -1036.4361477133205\n",
      "[0, 5, 2]\n",
      "Steps done: 2287\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 3: -417.9288356597868\n",
      "[0, 5, 2, 3]\n",
      "Steps done: 2288\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 10: -661.6712825546713\n",
      "[5, 2, 3]\n",
      "Steps done: 2289\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 15: -21.65350665522806\n",
      "[2, 3]\n",
      "Steps done: 2290\n",
      "SV: [ 0.12083808 -0.02501804 -0.18875141]\n",
      "Reward for action 20: -17.65350665522806\n",
      "[2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 333\n",
      "Steps done: 2291\n",
      "SV: [-0.15190928 -0.15526365  0.37623692]\n",
      "Reward for action 3: -123.45137653642361\n",
      "[0, 1, 3]\n",
      "Steps done: 2292\n",
      "SV: [-0.15190928 -0.15526365  0.37623692]\n",
      "Reward for action 10: -173.34771625010913\n",
      "[1, 3]\n",
      "Steps done: 2293\n",
      "SV: [-0.15190928 -0.15526365  0.37623692]\n",
      "Reward for action 4: -173.5105328752354\n",
      "[1, 3, 4]\n",
      "Steps done: 2294\n",
      "SV: [-0.15190928 -0.15526365  0.37623692]\n",
      "Reward for action 11: -198.9651540873499\n",
      "[3, 4]\n",
      "Steps done: 2295\n",
      "SV: [-0.15190928 -0.15526365  0.37623692]\n",
      "Reward for action 20: -194.9651540873499\n",
      "[3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 334\n",
      "Steps done: 2296\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 8: -18.749830819782158\n",
      "[0, 1, 8]\n",
      "Steps done: 2297\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 4: -11.662931568091603\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 2298\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 14: -18.749830819782158\n",
      "[0, 1, 8]\n",
      "Steps done: 2299\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 11: -40.867687917312615\n",
      "[0, 8]\n",
      "Did target update\n",
      "Steps done: 2300\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 4: -20.765983184883957\n",
      "[0, 8, 4]\n",
      "Steps done: 2301\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 2: -12.18312046684936\n",
      "[0, 8, 4, 2]\n",
      "Steps done: 2302\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 3: -8.359419243621167\n",
      "[0, 8, 4, 2, 3]\n",
      "Steps done: 2303\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 18: -3.9152049473942743\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 2304\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 9: -6.385733081684101\n",
      "[0, 4, 2, 3, 9]\n",
      "Steps done: 2305\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 19: -3.9152049473942743\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 2306\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 12: -3.7373276218069873\n",
      "[0, 4, 3]\n",
      "Steps done: 2307\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 7: -4.01579996363495\n",
      "[0, 4, 3, 7]\n",
      "Steps done: 2308\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 14: -3.8184257026235087\n",
      "[0, 3, 7]\n",
      "Steps done: 2309\n",
      "SV: [-0.00445494  0.02407357 -0.21968941]\n",
      "Reward for action 20: 0.18157429737649133\n",
      "[0, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 335\n",
      "Steps done: 2310\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 2: -22.348788770707337\n",
      "[0, 1, 2]\n",
      "Steps done: 2311\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 7: -21.755432512062754\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 2312\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 9: -19.426906681172817\n",
      "[0, 1, 2, 7, 9]\n",
      "Steps done: 2313\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 8: -13.365425802276642\n",
      "[0, 1, 2, 7, 9, 8]\n",
      "Steps done: 2314\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 10: -11.533737502676818\n",
      "[1, 2, 7, 9, 8]\n",
      "Steps done: 2315\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 6: -12.757288885860788\n",
      "[1, 2, 7, 9, 8, 6]\n",
      "Steps done: 2316\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 4: -14.464086889717453\n",
      "[1, 2, 7, 9, 8, 6, 4]\n",
      "Steps done: 2317\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 11: -16.159035876701935\n",
      "[2, 7, 9, 8, 6, 4]\n",
      "Steps done: 2318\n",
      "SV: [-0.03790398  0.17618003  0.4789349 ]\n",
      "Reward for action 20: -12.159035876701935\n",
      "[2, 7, 9, 8, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 336\n",
      "Steps done: 2319\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 5: -27.79896391930029\n",
      "[0, 1, 5]\n",
      "Steps done: 2320\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 15: -28.201568151717623\n",
      "[0, 1]\n",
      "Steps done: 2321\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 4: -28.23622467948537\n",
      "[0, 1, 4]\n",
      "Steps done: 2322\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 7: -28.595726242527206\n",
      "[0, 1, 4, 7]\n",
      "Steps done: 2323\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 6: -28.171761430177888\n",
      "[0, 1, 4, 7, 6]\n",
      "Steps done: 2324\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 9: -28.36017696079394\n",
      "[0, 1, 4, 7, 6, 9]\n",
      "Steps done: 2325\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 14: -28.555184933680913\n",
      "[0, 1, 7, 6, 9]\n",
      "Steps done: 2326\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 16: -28.97349407000682\n",
      "[0, 1, 7, 9]\n",
      "Steps done: 2327\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 11: -29.864604844716357\n",
      "[0, 7, 9]\n",
      "Steps done: 2328\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 2: -28.796036881416825\n",
      "[0, 7, 9, 2]\n",
      "Steps done: 2329\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 4: -28.639804602732053\n",
      "[0, 7, 9, 2, 4]\n",
      "Steps done: 2330\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 12: -29.0296709101614\n",
      "[0, 7, 9, 4]\n",
      "Steps done: 2331\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 3: -29.414324070553207\n",
      "[0, 7, 9, 4, 3]\n",
      "Steps done: 2332\n",
      "SV: [-0.09633519 -0.04019357  0.59415287]\n",
      "Reward for action 20: -25.414324070553207\n",
      "[0, 7, 9, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 337\n",
      "Steps done: 2333\n",
      "SV: [ 1.9486494  -0.82027495 -3.3533611 ]\n",
      "Reward for action 4: -457.56889169116806\n",
      "[0, 1, 4]\n",
      "Steps done: 2334\n",
      "SV: [ 1.9486494  -0.82027495 -3.3533611 ]\n",
      "Reward for action 14: -444.18794494797567\n",
      "[0, 1]\n",
      "Steps done: 2335\n",
      "SV: [ 1.9486494  -0.82027495 -3.3533611 ]\n",
      "Reward for action 20: -440.18794494797567\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 338\n",
      "Steps done: 2336\n",
      "SV: [-1.6017255   0.03452064 -1.7544184 ]\n",
      "Reward for action 3: -81.80646959247052\n",
      "[0, 1, 3]\n",
      "Steps done: 2337\n",
      "SV: [-1.6017255   0.03452064 -1.7544184 ]\n",
      "Reward for action 20: -77.80646959247052\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 339\n",
      "Steps done: 2338\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 5: -9.91587055164474\n",
      "[0, 1, 5]\n",
      "Steps done: 2339\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 4: -3.702402704285484\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 2340\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 3: -2.4355325273056847\n",
      "[0, 1, 5, 4, 3]\n",
      "Steps done: 2341\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 9: -4.155133279478003\n",
      "[0, 1, 5, 4, 3, 9]\n",
      "Steps done: 2342\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 7: -10.192107734076723\n",
      "[0, 1, 5, 4, 3, 9, 7]\n",
      "Steps done: 2343\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 11: -10.98393904471361\n",
      "[0, 5, 4, 3, 9, 7]\n",
      "Steps done: 2344\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 8: -19.20500477999979\n",
      "[0, 5, 4, 3, 9, 7, 8]\n",
      "Steps done: 2345\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 17: -108.98680329055699\n",
      "[0, 5, 4, 3, 9, 8]\n",
      "Steps done: 2346\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 6: -84.05921512273306\n",
      "[0, 5, 4, 3, 9, 8, 6]\n",
      "Steps done: 2347\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 13: -106.39386713153841\n",
      "[0, 5, 4, 9, 8, 6]\n",
      "Steps done: 2348\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 10: -137.55428136003758\n",
      "[5, 4, 9, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2349\n",
      "SV: [ 0.09378531 -0.03779132 -0.3078211 ]\n",
      "Reward for action 20: -133.55428136003758\n",
      "[5, 4, 9, 8, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 340\n",
      "Steps done: 2350\n",
      "SV: [ 0.13701296  0.01115285 -0.43006736]\n",
      "Reward for action 3: -16.564048485626543\n",
      "[0, 1, 3]\n",
      "Steps done: 2351\n",
      "SV: [ 0.13701296  0.01115285 -0.43006736]\n",
      "Reward for action 5: -16.93629994560347\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 2352\n",
      "SV: [ 0.13701296  0.01115285 -0.43006736]\n",
      "Reward for action 10: -16.305624614204227\n",
      "[1, 3, 5]\n",
      "Steps done: 2353\n",
      "SV: [ 0.13701296  0.01115285 -0.43006736]\n",
      "Reward for action 20: -12.305624614204227\n",
      "[1, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 341\n",
      "Steps done: 2354\n",
      "SV: [ 0.00840481  0.01608753 -0.3523819 ]\n",
      "Reward for action 9: -11.645025443894237\n",
      "[0, 1, 9]\n",
      "Steps done: 2355\n",
      "SV: [ 0.00840481  0.01608753 -0.3523819 ]\n",
      "Reward for action 20: -7.645025443894237\n",
      "[0, 1, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 342\n",
      "Steps done: 2356\n",
      "SV: [-0.04617795 -0.05755404  0.6948375 ]\n",
      "Reward for action 4: -25.365944602204696\n",
      "[0, 1, 4]\n",
      "Steps done: 2357\n",
      "SV: [-0.04617795 -0.05755404  0.6948375 ]\n",
      "Reward for action 20: -21.365944602204696\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 343\n",
      "Steps done: 2358\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 5: -85.06941058290295\n",
      "[0, 1, 5]\n",
      "Steps done: 2359\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 3: -94.91202555448609\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 2360\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 2: -150.20359581731915\n",
      "[0, 1, 5, 3, 2]\n",
      "Steps done: 2361\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 12: -94.91202555448609\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 2362\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 11: -39.95969016507648\n",
      "[0, 5, 3]\n",
      "Steps done: 2363\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 2: -87.30847334649651\n",
      "[0, 5, 3, 2]\n",
      "Steps done: 2364\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 15: -424.1272558370186\n",
      "[0, 3, 2]\n",
      "Steps done: 2365\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 12: -390.82390118320876\n",
      "[0, 3]\n",
      "Steps done: 2366\n",
      "SV: [ 1.079863    0.06488477 -1.8692706 ]\n",
      "Reward for action 20: -386.82390118320876\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 344\n",
      "Steps done: 2367\n",
      "SV: [-1.7186924 -0.7311354 -2.013623 ]\n",
      "Reward for action 20: -225.27101015698548\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 345\n",
      "Steps done: 2368\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 5: -29.088688340881802\n",
      "[0, 1, 5]\n",
      "Steps done: 2369\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 3: -21.68036951012147\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 2370\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 2: -23.842714016659937\n",
      "[0, 1, 5, 3, 2]\n",
      "Steps done: 2371\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 11: -20.651320194959677\n",
      "[0, 5, 3, 2]\n",
      "Steps done: 2372\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 8: -22.881534338059733\n",
      "[0, 5, 3, 2, 8]\n",
      "Steps done: 2373\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 6: -26.806013057824217\n",
      "[0, 5, 3, 2, 8, 6]\n",
      "Steps done: 2374\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 4: -26.011830585654025\n",
      "[0, 5, 3, 2, 8, 6, 4]\n",
      "Steps done: 2375\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 14: -26.806013057824217\n",
      "[0, 5, 3, 2, 8, 6]\n",
      "Steps done: 2376\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 18: -25.11031453695646\n",
      "[0, 5, 3, 2, 6]\n",
      "Steps done: 2377\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 7: -26.219090772963643\n",
      "[0, 5, 3, 2, 6, 7]\n",
      "Steps done: 2378\n",
      "SV: [-0.25263664  0.12181307 -0.6307917 ]\n",
      "Reward for action 20: -22.219090772963643\n",
      "[0, 5, 3, 2, 6, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 346\n",
      "Steps done: 2379\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 4: -133.88545893324877\n",
      "[0, 1, 4]\n",
      "Steps done: 2380\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 6: -2.111530094174635\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 2381\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 2: -2.1264047564459814\n",
      "[0, 1, 4, 6, 2]\n",
      "Steps done: 2382\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 16: -34.30994595078377\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 2383\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 8: -15.302007896252787\n",
      "[0, 1, 4, 2, 8]\n",
      "Steps done: 2384\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 5: -15.08056336547147\n",
      "[0, 1, 4, 2, 8, 5]\n",
      "Steps done: 2385\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 7: -2.298750901553119\n",
      "[0, 1, 4, 2, 8, 5, 7]\n",
      "Steps done: 2386\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 3: -0.03383159277957868\n",
      "[0, 1, 4, 2, 8, 5, 7, 3]\n",
      "Steps done: 2387\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 10: -2.475616392766944\n",
      "[1, 4, 2, 8, 5, 7, 3]\n",
      "Steps done: 2388\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 6: -10.422164141857236\n",
      "[1, 4, 2, 8, 5, 7, 3, 6]\n",
      "Steps done: 2389\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 11: -21.73817126224985\n",
      "[4, 2, 8, 5, 7, 3, 6]\n",
      "Steps done: 2390\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 12: -17.569375010757238\n",
      "[4, 8, 5, 7, 3, 6]\n",
      "Steps done: 2391\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 13: -79.40857747414802\n",
      "[4, 8, 5, 7, 6]\n",
      "Steps done: 2392\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 16: -44.78410741220605\n",
      "[4, 8, 5, 7]\n",
      "Steps done: 2393\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 1: -9.517191305067607\n",
      "[4, 8, 5, 7, 1]\n",
      "Steps done: 2394\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 0: -1.4462523216985341\n",
      "[4, 8, 5, 7, 1, 0]\n",
      "Steps done: 2395\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 6: -11.89651326582673\n",
      "[4, 8, 5, 7, 1, 0, 6]\n",
      "Steps done: 2396\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 15: -19.741795039369247\n",
      "[4, 8, 7, 1, 0, 6]\n",
      "Steps done: 2397\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 14: -42.15277441009334\n",
      "[8, 7, 1, 0, 6]\n",
      "Steps done: 2398\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 11: -82.67956224349292\n",
      "[8, 7, 0, 6]\n",
      "Steps done: 2399\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 4: -44.55373890492914\n",
      "[8, 7, 0, 6, 4]\n",
      "Did target update\n",
      "Steps done: 2400\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 17: -5.90176402832887\n",
      "[8, 0, 6, 4]\n",
      "Steps done: 2401\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 5: -3.286587114007304\n",
      "[8, 0, 6, 4, 5]\n",
      "Steps done: 2402\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 15: -5.90176402832887\n",
      "[8, 0, 6, 4]\n",
      "Steps done: 2403\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 18: -18.801852110691076\n",
      "[0, 6, 4]\n",
      "Steps done: 2404\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Reward for action 8: -5.90176402832887\n",
      "[0, 6, 4, 8]\n",
      "Steps done: 2405\n",
      "SV: [-0.1319011  1.2659717  0.5655151]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -1.90176402832887\n",
      "[0, 6, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 347\n",
      "Steps done: 2406\n",
      "SV: [-0.29480967 -0.31165966  0.894042  ]\n",
      "Reward for action 5: -291.6393538967183\n",
      "[0, 1, 5]\n",
      "Steps done: 2407\n",
      "SV: [-0.29480967 -0.31165966  0.894042  ]\n",
      "Reward for action 7: -354.3374641917533\n",
      "[0, 1, 5, 7]\n",
      "Steps done: 2408\n",
      "SV: [-0.29480967 -0.31165966  0.894042  ]\n",
      "Reward for action 6: -209.89743843140405\n",
      "[0, 1, 5, 7, 6]\n",
      "Steps done: 2409\n",
      "SV: [-0.29480967 -0.31165966  0.894042  ]\n",
      "Reward for action 8: -81.16322210105994\n",
      "[0, 1, 5, 7, 6, 8]\n",
      "Steps done: 2410\n",
      "SV: [-0.29480967 -0.31165966  0.894042  ]\n",
      "Reward for action 20: -77.16322210105994\n",
      "[0, 1, 5, 7, 6, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 348\n",
      "Steps done: 2411\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 5: -11.822241151002007\n",
      "[0, 1, 5]\n",
      "Steps done: 2412\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 6: -0.9447193013172553\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 2413\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 16: -11.822241151002007\n",
      "[0, 1, 5]\n",
      "Steps done: 2414\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 8: -18.591233498535082\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 2415\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 4: -4.966753617530378\n",
      "[0, 1, 5, 8, 4]\n",
      "Steps done: 2416\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 6: -1.2136230612736885\n",
      "[0, 1, 5, 8, 4, 6]\n",
      "Steps done: 2417\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 3: -2.161892628265051\n",
      "[0, 1, 5, 8, 4, 6, 3]\n",
      "Steps done: 2418\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 16: -5.518892903514577\n",
      "[0, 1, 5, 8, 4, 3]\n",
      "Steps done: 2419\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 13: -4.966753617530378\n",
      "[0, 1, 5, 8, 4]\n",
      "Steps done: 2420\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 15: -0.921440396644251\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 2421\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 7: -1.154523237823633\n",
      "[0, 1, 8, 4, 7]\n",
      "Steps done: 2422\n",
      "SV: [-0.14376058 -0.06178519 -0.02171495]\n",
      "Reward for action 20: 2.845476762176367\n",
      "[0, 1, 8, 4, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 349\n",
      "Steps done: 2423\n",
      "SV: [-0.14170134  0.05014131  0.01378306]\n",
      "Reward for action 2: -0.4886968927742663\n",
      "[0, 1, 2]\n",
      "Steps done: 2424\n",
      "SV: [-0.14170134  0.05014131  0.01378306]\n",
      "Reward for action 5: -0.2681327156643291\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 2425\n",
      "SV: [-0.14170134  0.05014131  0.01378306]\n",
      "Reward for action 11: -0.38884036670492683\n",
      "[0, 2, 5]\n",
      "Steps done: 2426\n",
      "SV: [-0.14170134  0.05014131  0.01378306]\n",
      "Reward for action 10: -2.0579729090279746\n",
      "[2, 5]\n",
      "Steps done: 2427\n",
      "SV: [-0.14170134  0.05014131  0.01378306]\n",
      "Reward for action 20: 1.9420270909720254\n",
      "[2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 350\n",
      "Steps done: 2428\n",
      "SV: [-0.597179   0.2254941  1.0350703]\n",
      "Reward for action 20: -441.9345539185146\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 351\n",
      "Steps done: 2429\n",
      "SV: [1.630887   0.05399666 3.7310753 ]\n",
      "Reward for action 20: -275.56603545092355\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 352\n",
      "Steps done: 2430\n",
      "SV: [ 0.132488   -0.16568962 -0.62422734]\n",
      "Reward for action 4: -18.209060755372796\n",
      "[0, 1, 4]\n",
      "Steps done: 2431\n",
      "SV: [ 0.132488   -0.16568962 -0.62422734]\n",
      "Reward for action 20: -14.209060755372796\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 353\n",
      "Steps done: 2432\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 3: -159.9599617426893\n",
      "[0, 1, 3]\n",
      "Steps done: 2433\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 7: -6.7129775008879875\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 2434\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 11: -2.267598016537579\n",
      "[0, 3, 7]\n",
      "Steps done: 2435\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 13: -235.86504126118282\n",
      "[0, 7]\n",
      "Steps done: 2436\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 5: -5.003664932983905\n",
      "[0, 7, 5]\n",
      "Steps done: 2437\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 1: -8.229664813034784\n",
      "[0, 7, 5, 1]\n",
      "Steps done: 2438\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 10: -25.966385809963516\n",
      "[7, 5, 1]\n",
      "Steps done: 2439\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 6: -17.143930221316786\n",
      "[7, 5, 1, 6]\n",
      "Steps done: 2440\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 15: -41.76332635966631\n",
      "[7, 1, 6]\n",
      "Steps done: 2441\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 9: -46.84473272257527\n",
      "[7, 1, 6, 9]\n",
      "Steps done: 2442\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 19: -41.76332635966631\n",
      "[7, 1, 6]\n",
      "Steps done: 2443\n",
      "SV: [ 0.07650627  0.40715104 -0.31447884]\n",
      "Reward for action 20: -37.76332635966631\n",
      "[7, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 354\n",
      "Steps done: 2444\n",
      "SV: [-0.38989347 -0.02271925  1.1039039 ]\n",
      "Reward for action 2: -49.07393102469659\n",
      "[0, 1, 2]\n",
      "Steps done: 2445\n",
      "SV: [-0.38989347 -0.02271925  1.1039039 ]\n",
      "Reward for action 12: -14.561669838051344\n",
      "[0, 1]\n",
      "Steps done: 2446\n",
      "SV: [-0.38989347 -0.02271925  1.1039039 ]\n",
      "Reward for action 20: -10.561669838051344\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 355\n",
      "Steps done: 2447\n",
      "SV: [-0.95772105  0.08333874  1.037946  ]\n",
      "Reward for action 20: -101.40133334185768\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 356\n",
      "Steps done: 2448\n",
      "SV: [ 1.2710669  1.1216577 -2.413425 ]\n",
      "Reward for action 20: -453.6638980050377\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 357\n",
      "Steps done: 2449\n",
      "SV: [1.0177866 0.8534805 0.4844783]\n",
      "Reward for action 3: -35.078697931196345\n",
      "[0, 1, 3]\n",
      "Steps done: 2450\n",
      "SV: [1.0177866 0.8534805 0.4844783]\n",
      "Reward for action 6: -10.851380127022887\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 2451\n",
      "SV: [1.0177866 0.8534805 0.4844783]\n",
      "Reward for action 5: -6.726119281508195\n",
      "[0, 1, 3, 6, 5]\n",
      "Steps done: 2452\n",
      "SV: [1.0177866 0.8534805 0.4844783]\n",
      "Reward for action 11: -5.243369222884447\n",
      "[0, 3, 6, 5]\n",
      "Steps done: 2453\n",
      "SV: [1.0177866 0.8534805 0.4844783]\n",
      "Reward for action 20: -1.2433692228844473\n",
      "[0, 3, 6, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 358\n",
      "Steps done: 2454\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 4: -1.6845107326234112\n",
      "[0, 1, 4]\n",
      "Steps done: 2455\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 8: -22.915418469269962\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 2456\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 2: -45.25369585382175\n",
      "[0, 1, 4, 8, 2]\n",
      "Steps done: 2457\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 10: -99.60921545696621\n",
      "[1, 4, 8, 2]\n",
      "Steps done: 2458\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 18: -52.887509297463986\n",
      "[1, 4, 2]\n",
      "Steps done: 2459\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 3: -6.801222488787666\n",
      "[1, 4, 2, 3]\n",
      "Steps done: 2460\n",
      "SV: [ 0.23758025 -0.28406832 -0.3014547 ]\n",
      "Reward for action 20: -2.8012224887876664\n",
      "[1, 4, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 359\n",
      "Steps done: 2461\n",
      "SV: [ 0.29293948  0.28555986 -1.0166496 ]\n",
      "Reward for action 2: -102.25996773794043\n",
      "[0, 1, 2]\n",
      "Steps done: 2462\n",
      "SV: [ 0.29293948  0.28555986 -1.0166496 ]\n",
      "Reward for action 11: -97.57616849048314\n",
      "[0, 2]\n",
      "Steps done: 2463\n",
      "SV: [ 0.29293948  0.28555986 -1.0166496 ]\n",
      "Reward for action 20: -93.57616849048314\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 360\n",
      "Steps done: 2464\n",
      "SV: [ 0.39279136 -0.597874    0.74307746]\n",
      "Reward for action 2: -85.70696617188739\n",
      "[0, 1, 2]\n",
      "Steps done: 2465\n",
      "SV: [ 0.39279136 -0.597874    0.74307746]\n",
      "Reward for action 12: -89.25108391761142\n",
      "[0, 1]\n",
      "Steps done: 2466\n",
      "SV: [ 0.39279136 -0.597874    0.74307746]\n",
      "Reward for action 4: -89.50850806641999\n",
      "[0, 1, 4]\n",
      "Steps done: 2467\n",
      "SV: [ 0.39279136 -0.597874    0.74307746]\n",
      "Reward for action 10: -91.09821006150928\n",
      "[1, 4]\n",
      "Steps done: 2468\n",
      "SV: [ 0.39279136 -0.597874    0.74307746]\n",
      "Reward for action 20: -87.09821006150928\n",
      "[1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 361\n",
      "Steps done: 2469\n",
      "SV: [0.35023263 0.05296052 0.14315376]\n",
      "Reward for action 2: -40.10343629119555\n",
      "[0, 1, 2]\n",
      "Steps done: 2470\n",
      "SV: [0.35023263 0.05296052 0.14315376]\n",
      "Reward for action 11: -136.62094837495297\n",
      "[0, 2]\n",
      "Steps done: 2471\n",
      "SV: [0.35023263 0.05296052 0.14315376]\n",
      "Reward for action 1: -40.10343629119555\n",
      "[0, 2, 1]\n",
      "Steps done: 2472\n",
      "SV: [0.35023263 0.05296052 0.14315376]\n",
      "Reward for action 10: -6.5214285378984185\n",
      "[2, 1]\n",
      "Steps done: 2473\n",
      "SV: [0.35023263 0.05296052 0.14315376]\n",
      "Reward for action 20: -2.5214285378984185\n",
      "[2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 362\n",
      "Steps done: 2474\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 8: -6.774348551551167\n",
      "[0, 1, 8]\n",
      "Steps done: 2475\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 11: -26.74637105617107\n",
      "[0, 8]\n",
      "Steps done: 2476\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 3: -1.9839777007909607\n",
      "[0, 8, 3]\n",
      "Steps done: 2477\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 13: -26.74637105617107\n",
      "[0, 8]\n",
      "Steps done: 2478\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 2: -1.0147629784796923\n",
      "[0, 8, 2]\n",
      "Steps done: 2479\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 4: -0.7334571493612074\n",
      "[0, 8, 2, 4]\n",
      "Steps done: 2480\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 18: -18.15087397325003\n",
      "[0, 2, 4]\n",
      "Steps done: 2481\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 10: -42.29374600322188\n",
      "[2, 4]\n",
      "Steps done: 2482\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 3: -25.747199532953893\n",
      "[2, 4, 3]\n",
      "Steps done: 2483\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 8: -0.9812486944806562\n",
      "[2, 4, 3, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2484\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 0: -1.696798138962494\n",
      "[2, 4, 3, 8, 0]\n",
      "Steps done: 2485\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 18: -21.853223181529813\n",
      "[2, 4, 3, 0]\n",
      "Steps done: 2486\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 14: -33.87888921745762\n",
      "[2, 3, 0]\n",
      "Steps done: 2487\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 6: -13.06634988264372\n",
      "[2, 3, 0, 6]\n",
      "Steps done: 2488\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 13: -14.103110477119507\n",
      "[2, 0, 6]\n",
      "Steps done: 2489\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 16: -33.0791687030433\n",
      "[2, 0]\n",
      "Steps done: 2490\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 4: -18.15087397325002\n",
      "[2, 0, 4]\n",
      "Steps done: 2491\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 7: -39.73074626955345\n",
      "[2, 0, 4, 7]\n",
      "Steps done: 2492\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 8: -6.715639228802433\n",
      "[2, 0, 4, 7, 8]\n",
      "Steps done: 2493\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 5: -4.77285279450359\n",
      "[2, 0, 4, 7, 8, 5]\n",
      "Steps done: 2494\n",
      "SV: [0.6260919  0.11763044 0.37021908]\n",
      "Reward for action 20: -0.7728527945035903\n",
      "[2, 0, 4, 7, 8, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 363\n",
      "Steps done: 2495\n",
      "SV: [ 0.9841899  -0.589751    0.47447735]\n",
      "Reward for action 20: -174.68737188715255\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 364\n",
      "Steps done: 2496\n",
      "SV: [-0.8934765 -1.2354132 -1.7915329]\n",
      "Reward for action 2: -31.11605672153302\n",
      "[0, 1, 2]\n",
      "Steps done: 2497\n",
      "SV: [-0.8934765 -1.2354132 -1.7915329]\n",
      "Reward for action 12: -88.47237187277202\n",
      "[0, 1]\n",
      "Steps done: 2498\n",
      "SV: [-0.8934765 -1.2354132 -1.7915329]\n",
      "Reward for action 2: -31.11605672153302\n",
      "[0, 1, 2]\n",
      "Steps done: 2499\n",
      "SV: [-0.8934765 -1.2354132 -1.7915329]\n",
      "Reward for action 10: -311.97763859089946\n",
      "[1, 2]\n",
      "Did target update\n",
      "Steps done: 2500\n",
      "SV: [-0.8934765 -1.2354132 -1.7915329]\n",
      "Reward for action 0: -31.116056721533027\n",
      "[1, 2, 0]\n",
      "Steps done: 2501\n",
      "SV: [-0.8934765 -1.2354132 -1.7915329]\n",
      "Reward for action 20: -27.116056721533027\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 365\n",
      "Steps done: 2502\n",
      "SV: [ 0.37113765 -0.76006556 -0.60960907]\n",
      "Reward for action 6: -40.60003232852671\n",
      "[0, 1, 6]\n",
      "Steps done: 2503\n",
      "SV: [ 0.37113765 -0.76006556 -0.60960907]\n",
      "Reward for action 5: -67.14065275887793\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 2504\n",
      "SV: [ 0.37113765 -0.76006556 -0.60960907]\n",
      "Reward for action 20: -63.140652758877934\n",
      "[0, 1, 6, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 366\n",
      "Steps done: 2505\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 2: -0.6179505578199894\n",
      "[0, 1, 2]\n",
      "Steps done: 2506\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 12: -0.5065509188524537\n",
      "[0, 1]\n",
      "Steps done: 2507\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 8: -0.5354015164702441\n",
      "[0, 1, 8]\n",
      "Steps done: 2508\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 6: -0.5059655922554666\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 2509\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 18: -0.5220368048706775\n",
      "[0, 1, 6]\n",
      "Steps done: 2510\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 8: -0.5059655922554668\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 2511\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 7: -0.31718848479706774\n",
      "[0, 1, 6, 8, 7]\n",
      "Steps done: 2512\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 3: -1.218597163379156\n",
      "[0, 1, 6, 8, 7, 3]\n",
      "Steps done: 2513\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 11: -1.855927931599503\n",
      "[0, 6, 8, 7, 3]\n",
      "Steps done: 2514\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 10: -4.973816407616642\n",
      "[6, 8, 7, 3]\n",
      "Steps done: 2515\n",
      "SV: [ 0.01275305  0.03456567 -0.06452536]\n",
      "Reward for action 20: -0.973816407616642\n",
      "[6, 8, 7, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 367\n",
      "Steps done: 2516\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 2: -68.5579699134893\n",
      "[0, 1, 2]\n",
      "Steps done: 2517\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 11: -81.25147221938857\n",
      "[0, 2]\n",
      "Steps done: 2518\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 3: -29.77986684841646\n",
      "[0, 2, 3]\n",
      "Steps done: 2519\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 13: -81.25147221938857\n",
      "[0, 2]\n",
      "Steps done: 2520\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 3: -29.77986684841646\n",
      "[0, 2, 3]\n",
      "Steps done: 2521\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 13: -81.25147221938857\n",
      "[0, 2]\n",
      "Steps done: 2522\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 1: -68.5579699134893\n",
      "[0, 2, 1]\n",
      "Steps done: 2523\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 12: -200.6201402818662\n",
      "[0, 1]\n",
      "Steps done: 2524\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 3: -32.287753228036856\n",
      "[0, 1, 3]\n",
      "Steps done: 2525\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 10: -54.05034810806551\n",
      "[1, 3]\n",
      "Steps done: 2526\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 4: -173.5691934237996\n",
      "[1, 3, 4]\n",
      "Steps done: 2527\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 11: -272.1746570083573\n",
      "[3, 4]\n",
      "Steps done: 2528\n",
      "SV: [ 1.059051    0.09110168 -1.4106429 ]\n",
      "Reward for action 20: -268.1746570083573\n",
      "[3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 368\n",
      "Steps done: 2529\n",
      "SV: [ 0.15399751  0.0866101  -0.7796533 ]\n",
      "Reward for action 5: -37.63115474165747\n",
      "[0, 1, 5]\n",
      "Steps done: 2530\n",
      "SV: [ 0.15399751  0.0866101  -0.7796533 ]\n",
      "Reward for action 6: -38.1686795642317\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 2531\n",
      "SV: [ 0.15399751  0.0866101  -0.7796533 ]\n",
      "Reward for action 20: -34.1686795642317\n",
      "[0, 1, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 369\n",
      "Steps done: 2532\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 5: -30.72701693045068\n",
      "[0, 1, 5]\n",
      "Steps done: 2533\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 11: -33.273545632127686\n",
      "[0, 5]\n",
      "Steps done: 2534\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 6: -30.432423367498988\n",
      "[0, 5, 6]\n",
      "Steps done: 2535\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 10: -27.699972474155402\n",
      "[5, 6]\n",
      "Steps done: 2536\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 4: -29.413466524473844\n",
      "[5, 6, 4]\n",
      "Steps done: 2537\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 2: -28.957579421930372\n",
      "[5, 6, 4, 2]\n",
      "Steps done: 2538\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 3: -27.710989762400466\n",
      "[5, 6, 4, 2, 3]\n",
      "Steps done: 2539\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 13: -28.957579421930372\n",
      "[5, 6, 4, 2]\n",
      "Steps done: 2540\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 7: -28.577544705269972\n",
      "[5, 6, 4, 2, 7]\n",
      "Steps done: 2541\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 16: -27.474475359445005\n",
      "[5, 4, 2, 7]\n",
      "Steps done: 2542\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 6: -28.577544705269965\n",
      "[5, 4, 2, 7, 6]\n",
      "Steps done: 2543\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 8: -24.68087491582999\n",
      "[5, 4, 2, 7, 6, 8]\n",
      "Steps done: 2544\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 17: -23.66044607861785\n",
      "[5, 4, 2, 6, 8]\n",
      "Steps done: 2545\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 9: -27.397604177759\n",
      "[5, 4, 2, 6, 8, 9]\n",
      "Steps done: 2546\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 12: -28.721364948373346\n",
      "[5, 4, 6, 8, 9]\n",
      "Steps done: 2547\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 7: -28.257402767736117\n",
      "[5, 4, 6, 8, 9, 7]\n",
      "Steps done: 2548\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 18: -29.27608619700672\n",
      "[5, 4, 6, 9, 7]\n",
      "Steps done: 2549\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 16: -28.837937894873797\n",
      "[5, 4, 9, 7]\n",
      "Steps done: 2550\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 8: -29.418365042264224\n",
      "[5, 4, 9, 7, 8]\n",
      "Steps done: 2551\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 17: -31.230308903573878\n",
      "[5, 4, 9, 8]\n",
      "Steps done: 2552\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 3: -28.668109240215387\n",
      "[5, 4, 9, 8, 3]\n",
      "Steps done: 2553\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 19: -24.013875345872872\n",
      "[5, 4, 8, 3]\n",
      "Steps done: 2554\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 13: -24.07712239681771\n",
      "[5, 4, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2555\n",
      "SV: [-0.24711464  0.18200813  0.5338792 ]\n",
      "Reward for action 20: -20.07712239681771\n",
      "[5, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 370\n",
      "Steps done: 2556\n",
      "SV: [0.14458905 0.740856   0.20366894]\n",
      "Reward for action 3: -6.886997388291858\n",
      "[0, 1, 3]\n",
      "Steps done: 2557\n",
      "SV: [0.14458905 0.740856   0.20366894]\n",
      "Reward for action 11: -27.40889918242474\n",
      "[0, 3]\n",
      "Steps done: 2558\n",
      "SV: [0.14458905 0.740856   0.20366894]\n",
      "Reward for action 2: -25.846933582957863\n",
      "[0, 3, 2]\n",
      "Steps done: 2559\n",
      "SV: [0.14458905 0.740856   0.20366894]\n",
      "Reward for action 1: -27.482891403168008\n",
      "[0, 3, 2, 1]\n",
      "Steps done: 2560\n",
      "SV: [0.14458905 0.740856   0.20366894]\n",
      "Reward for action 20: -23.482891403168008\n",
      "[0, 3, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 371\n",
      "Steps done: 2561\n",
      "SV: [0.06994601 0.03712306 0.21216944]\n",
      "Reward for action 20: -0.48193758185894975\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 372\n",
      "Steps done: 2562\n",
      "SV: [ 0.20977855 -0.08894718 -0.41820565]\n",
      "Reward for action 2: -93.69844220894755\n",
      "[0, 1, 2]\n",
      "Steps done: 2563\n",
      "SV: [ 0.20977855 -0.08894718 -0.41820565]\n",
      "Reward for action 4: -129.41596196295095\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 2564\n",
      "SV: [ 0.20977855 -0.08894718 -0.41820565]\n",
      "Reward for action 14: -93.69844220894755\n",
      "[0, 1, 2]\n",
      "Steps done: 2565\n",
      "SV: [ 0.20977855 -0.08894718 -0.41820565]\n",
      "Reward for action 20: -89.69844220894755\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 373\n",
      "Steps done: 2566\n",
      "SV: [-1.5498807  -0.75555384 -2.3998494 ]\n",
      "Reward for action 2: -233.46430531040036\n",
      "[0, 1, 2]\n",
      "Steps done: 2567\n",
      "SV: [-1.5498807  -0.75555384 -2.3998494 ]\n",
      "Reward for action 11: -488.0204317012384\n",
      "[0, 2]\n",
      "Steps done: 2568\n",
      "SV: [-1.5498807  -0.75555384 -2.3998494 ]\n",
      "Reward for action 20: -484.0204317012384\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 374\n",
      "Steps done: 2569\n",
      "SV: [ 1.2465596   1.1272362  -0.00885821]\n",
      "Reward for action 20: -115.68813246427722\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 375\n",
      "Steps done: 2570\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 4: -1.7455020700432455\n",
      "[0, 1, 4]\n",
      "Steps done: 2571\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 11: -0.056990647552699614\n",
      "[0, 4]\n",
      "Steps done: 2572\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 1: -1.7455020700432455\n",
      "[0, 4, 1]\n",
      "Steps done: 2573\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 5: -0.3524239259711384\n",
      "[0, 4, 1, 5]\n",
      "Steps done: 2574\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 11: -2.0938372881729643\n",
      "[0, 4, 5]\n",
      "Steps done: 2575\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 15: -0.056990647552699614\n",
      "[0, 4]\n",
      "Steps done: 2576\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 5: -2.0938372881729643\n",
      "[0, 4, 5]\n",
      "Steps done: 2577\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 2: -1.6495143092755638\n",
      "[0, 4, 5, 2]\n",
      "Steps done: 2578\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 12: -2.0938372881729643\n",
      "[0, 4, 5]\n",
      "Steps done: 2579\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 3: -0.5721551957262179\n",
      "[0, 4, 5, 3]\n",
      "Steps done: 2580\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 14: -0.5557907165461066\n",
      "[0, 5, 3]\n",
      "Steps done: 2581\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 2: -0.2881177977631978\n",
      "[0, 5, 3, 2]\n",
      "Steps done: 2582\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 10: -0.5404792769406384\n",
      "[5, 3, 2]\n",
      "Steps done: 2583\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 13: -8.527709658536875\n",
      "[5, 2]\n",
      "Steps done: 2584\n",
      "SV: [-0.0905058   0.0845499  -0.14692514]\n",
      "Reward for action 20: -4.527709658536875\n",
      "[5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 376\n",
      "Steps done: 2585\n",
      "SV: [-0.03010994  0.26891837  0.571814  ]\n",
      "Reward for action 5: -34.34800367914893\n",
      "[0, 1, 5]\n",
      "Steps done: 2586\n",
      "SV: [-0.03010994  0.26891837  0.571814  ]\n",
      "Reward for action 6: -35.236245614436264\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 2587\n",
      "SV: [-0.03010994  0.26891837  0.571814  ]\n",
      "Reward for action 10: -36.7662652159497\n",
      "[1, 5, 6]\n",
      "Steps done: 2588\n",
      "SV: [-0.03010994  0.26891837  0.571814  ]\n",
      "Reward for action 20: -32.7662652159497\n",
      "[1, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 377\n",
      "Steps done: 2589\n",
      "SV: [-0.32838035 -0.29397708  0.01379486]\n",
      "Reward for action 7: -27.505724546459504\n",
      "[0, 1, 7]\n",
      "Steps done: 2590\n",
      "SV: [-0.32838035 -0.29397708  0.01379486]\n",
      "Reward for action 3: -23.777975988670143\n",
      "[0, 1, 7, 3]\n",
      "Steps done: 2591\n",
      "SV: [-0.32838035 -0.29397708  0.01379486]\n",
      "Reward for action 17: -12.1911831320028\n",
      "[0, 1, 3]\n",
      "Steps done: 2592\n",
      "SV: [-0.32838035 -0.29397708  0.01379486]\n",
      "Reward for action 13: -11.660085594524517\n",
      "[0, 1]\n",
      "Steps done: 2593\n",
      "SV: [-0.32838035 -0.29397708  0.01379486]\n",
      "Reward for action 20: -7.660085594524517\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 378\n",
      "Steps done: 2594\n",
      "SV: [0.2524627  0.02184949 1.0488957 ]\n",
      "Reward for action 20: -1.3087565692594882\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 379\n",
      "Steps done: 2595\n",
      "SV: [-0.03535432 -0.19098148  0.7574636 ]\n",
      "Reward for action 5: -47.047825501692856\n",
      "[0, 1, 5]\n",
      "Steps done: 2596\n",
      "SV: [-0.03535432 -0.19098148  0.7574636 ]\n",
      "Reward for action 4: -47.57376240474019\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 2597\n",
      "SV: [-0.03535432 -0.19098148  0.7574636 ]\n",
      "Reward for action 20: -43.57376240474019\n",
      "[0, 1, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 380\n",
      "Steps done: 2598\n",
      "SV: [ 0.2941338  -0.6784713   0.93477786]\n",
      "Reward for action 20: -31.69179822011968\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 381\n",
      "Steps done: 2599\n",
      "SV: [ 0.08913127 -0.09060787 -0.1518128 ]\n",
      "Reward for action 4: -3.6786131134184243\n",
      "[0, 1, 4]\n",
      "Did target update\n",
      "Steps done: 2600\n",
      "SV: [ 0.08913127 -0.09060787 -0.1518128 ]\n",
      "Reward for action 11: -2.9611909983761504\n",
      "[0, 4]\n",
      "Steps done: 2601\n",
      "SV: [ 0.08913127 -0.09060787 -0.1518128 ]\n",
      "Reward for action 2: -2.983037472986157\n",
      "[0, 4, 2]\n",
      "Steps done: 2602\n",
      "SV: [ 0.08913127 -0.09060787 -0.1518128 ]\n",
      "Reward for action 3: -1.7596842617788133\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 2603\n",
      "SV: [ 0.08913127 -0.09060787 -0.1518128 ]\n",
      "Reward for action 13: -2.983037472986157\n",
      "[0, 4, 2]\n",
      "Steps done: 2604\n",
      "SV: [ 0.08913127 -0.09060787 -0.1518128 ]\n",
      "Reward for action 20: 1.0169625270138432\n",
      "[0, 4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 382\n",
      "Steps done: 2605\n",
      "SV: [-1.2436198  -0.49445567  1.6485751 ]\n",
      "Reward for action 6: -312.36565046367434\n",
      "[0, 1, 6]\n",
      "Steps done: 2606\n",
      "SV: [-1.2436198  -0.49445567  1.6485751 ]\n",
      "Reward for action 2: -133.34199613779793\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 2607\n",
      "SV: [-1.2436198  -0.49445567  1.6485751 ]\n",
      "Reward for action 20: -129.34199613779793\n",
      "[0, 1, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 383\n",
      "Steps done: 2608\n",
      "SV: [0.7801277  0.25214648 1.6484158 ]\n",
      "Reward for action 2: -225.9104999228319\n",
      "[0, 1, 2]\n",
      "Steps done: 2609\n",
      "SV: [0.7801277  0.25214648 1.6484158 ]\n",
      "Reward for action 11: -442.00046640969794\n",
      "[0, 2]\n",
      "Steps done: 2610\n",
      "SV: [0.7801277  0.25214648 1.6484158 ]\n",
      "Reward for action 3: -47.058321307832784\n",
      "[0, 2, 3]\n",
      "Steps done: 2611\n",
      "SV: [0.7801277  0.25214648 1.6484158 ]\n",
      "Reward for action 4: -3.2272350970719073\n",
      "[0, 2, 3, 4]\n",
      "Steps done: 2612\n",
      "SV: [0.7801277  0.25214648 1.6484158 ]\n",
      "Reward for action 12: -3.452245042245985\n",
      "[0, 3, 4]\n",
      "Steps done: 2613\n",
      "SV: [0.7801277  0.25214648 1.6484158 ]\n",
      "Reward for action 20: 0.5477549577540151\n",
      "[0, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 384\n",
      "Steps done: 2614\n",
      "SV: [-0.02766087  4.177492   -2.5284388 ]\n",
      "Reward for action 6: -111.83142279671955\n",
      "[0, 1, 6]\n",
      "Steps done: 2615\n",
      "SV: [-0.02766087  4.177492   -2.5284388 ]\n",
      "Reward for action 16: -105.0271396111155\n",
      "[0, 1]\n",
      "Steps done: 2616\n",
      "SV: [-0.02766087  4.177492   -2.5284388 ]\n",
      "Reward for action 2: -98.42652277189387\n",
      "[0, 1, 2]\n",
      "Steps done: 2617\n",
      "SV: [-0.02766087  4.177492   -2.5284388 ]\n",
      "Reward for action 6: -47.5654814144705\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 2618\n",
      "SV: [-0.02766087  4.177492   -2.5284388 ]\n",
      "Reward for action 20: -43.5654814144705\n",
      "[0, 1, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 385\n",
      "Steps done: 2619\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 4: -19.234367916557545\n",
      "[0, 1, 4]\n",
      "Steps done: 2620\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 9: -11.170094763174237\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 2621\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 8: -9.561498052046693\n",
      "[0, 1, 4, 9, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2622\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 18: -11.170094763174237\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 2623\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 10: -6.444233061638936\n",
      "[1, 4, 9]\n",
      "Steps done: 2624\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 3: -10.131157800195771\n",
      "[1, 4, 9, 3]\n",
      "Steps done: 2625\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 2: -5.134086253395772\n",
      "[1, 4, 9, 3, 2]\n",
      "Steps done: 2626\n",
      "SV: [0.27333966 0.15708026 0.07465371]\n",
      "Reward for action 20: -1.1340862533957718\n",
      "[1, 4, 9, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 386\n",
      "Steps done: 2627\n",
      "SV: [0.06715408 0.00063035 0.28534493]\n",
      "Reward for action 4: -4.134088086930611\n",
      "[0, 1, 4]\n",
      "Steps done: 2628\n",
      "SV: [0.06715408 0.00063035 0.28534493]\n",
      "Reward for action 3: -3.972981197608483\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 2629\n",
      "SV: [0.06715408 0.00063035 0.28534493]\n",
      "Reward for action 11: -1.7786561767511342\n",
      "[0, 4, 3]\n",
      "Steps done: 2630\n",
      "SV: [0.06715408 0.00063035 0.28534493]\n",
      "Reward for action 20: 2.2213438232488656\n",
      "[0, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 387\n",
      "Steps done: 2631\n",
      "SV: [-0.48133174 -1.0917001  -2.1242359 ]\n",
      "Reward for action 20: -429.5490692285319\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 388\n",
      "Steps done: 2632\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 6: -63.25243411975465\n",
      "[0, 1, 6]\n",
      "Steps done: 2633\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 11: -63.4784421726953\n",
      "[0, 6]\n",
      "Steps done: 2634\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 3: -65.92193134688898\n",
      "[0, 6, 3]\n",
      "Steps done: 2635\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 8: -69.08344924295602\n",
      "[0, 6, 3, 8]\n",
      "Steps done: 2636\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 1: -66.79266577654418\n",
      "[0, 6, 3, 8, 1]\n",
      "Steps done: 2637\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 9: -65.65598357539876\n",
      "[0, 6, 3, 8, 1, 9]\n",
      "Steps done: 2638\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 11: -67.088555725322\n",
      "[0, 6, 3, 8, 9]\n",
      "Steps done: 2639\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 2: -65.63677099672898\n",
      "[0, 6, 3, 8, 9, 2]\n",
      "Steps done: 2640\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 4: -63.32075831159796\n",
      "[0, 6, 3, 8, 9, 2, 4]\n",
      "Steps done: 2641\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 18: -61.990650641626026\n",
      "[0, 6, 3, 9, 2, 4]\n",
      "Steps done: 2642\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 13: -61.02348310398303\n",
      "[0, 6, 9, 2, 4]\n",
      "Steps done: 2643\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 5: -60.39300559890837\n",
      "[0, 6, 9, 2, 4, 5]\n",
      "Steps done: 2644\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 1: -60.1141985025954\n",
      "[0, 6, 9, 2, 4, 5, 1]\n",
      "Steps done: 2645\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 15: -60.36607056608718\n",
      "[0, 6, 9, 2, 4, 1]\n",
      "Steps done: 2646\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 3: -61.340697116711034\n",
      "[0, 6, 9, 2, 4, 1, 3]\n",
      "Steps done: 2647\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 19: -61.571764413293735\n",
      "[0, 6, 2, 4, 1, 3]\n",
      "Steps done: 2648\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 16: -59.90762919451813\n",
      "[0, 2, 4, 1, 3]\n",
      "Steps done: 2649\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 9: -59.827317267421726\n",
      "[0, 2, 4, 1, 3, 9]\n",
      "Steps done: 2650\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 19: -59.90762919451813\n",
      "[0, 2, 4, 1, 3]\n",
      "Steps done: 2651\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 11: -60.685285226401966\n",
      "[0, 2, 4, 3]\n",
      "Steps done: 2652\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 5: -60.06874998031987\n",
      "[0, 2, 4, 3, 5]\n",
      "Steps done: 2653\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 14: -61.58995958325113\n",
      "[0, 2, 3, 5]\n",
      "Steps done: 2654\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 7: -60.81402476708838\n",
      "[0, 2, 3, 5, 7]\n",
      "Steps done: 2655\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 1: -60.654948202730466\n",
      "[0, 2, 3, 5, 7, 1]\n",
      "Steps done: 2656\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 4: -59.70196835508662\n",
      "[0, 2, 3, 5, 7, 1, 4]\n",
      "Steps done: 2657\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Reward for action 15: -59.62276784744576\n",
      "[0, 2, 3, 7, 1, 4]\n",
      "Steps done: 2658\n",
      "SV: [-0.03252749 -0.02383702 -0.8515329 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -55.62276784744576\n",
      "[0, 2, 3, 7, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 389\n",
      "Steps done: 2659\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 7: -32.774219201453796\n",
      "[0, 1, 7]\n",
      "Steps done: 2660\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 5: -50.047745674474896\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 2661\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 6: -54.65816602426925\n",
      "[0, 1, 7, 5, 6]\n",
      "Steps done: 2662\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 17: -76.98372851762264\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 2663\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 3: -63.85200457738119\n",
      "[0, 1, 5, 6, 3]\n",
      "Steps done: 2664\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 11: -75.62754501316182\n",
      "[0, 5, 6, 3]\n",
      "Steps done: 2665\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 4: -80.22058584890453\n",
      "[0, 5, 6, 3, 4]\n",
      "Steps done: 2666\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 8: -9.712177441177667\n",
      "[0, 5, 6, 3, 4, 8]\n",
      "Steps done: 2667\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 10: -9.39079502899019\n",
      "[5, 6, 3, 4, 8]\n",
      "Steps done: 2668\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 18: -121.56094429620268\n",
      "[5, 6, 3, 4]\n",
      "Steps done: 2669\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 13: -85.84993090915533\n",
      "[5, 6, 4]\n",
      "Steps done: 2670\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 8: -19.52798401893932\n",
      "[5, 6, 4, 8]\n",
      "Steps done: 2671\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 14: -50.85234894681756\n",
      "[5, 6, 8]\n",
      "Steps done: 2672\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 4: -19.527984018939314\n",
      "[5, 6, 8, 4]\n",
      "Steps done: 2673\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 14: -50.85234894681756\n",
      "[5, 6, 8]\n",
      "Steps done: 2674\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 7: -22.117324240214373\n",
      "[5, 6, 8, 7]\n",
      "Steps done: 2675\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 16: -45.81330942493908\n",
      "[5, 8, 7]\n",
      "Steps done: 2676\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 15: -119.59228049487\n",
      "[8, 7]\n",
      "Steps done: 2677\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 1: -39.684207236290675\n",
      "[8, 7, 1]\n",
      "Steps done: 2678\n",
      "SV: [ 0.28461584  0.03412282 -0.94339293]\n",
      "Reward for action 20: -35.684207236290675\n",
      "[8, 7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 390\n",
      "Steps done: 2679\n",
      "SV: [-0.17459868  0.5677154   0.9949545 ]\n",
      "Reward for action 20: -73.54951710530254\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 391\n",
      "Steps done: 2680\n",
      "SV: [-1.2007464 -2.68458   -2.4800022]\n",
      "Reward for action 5: -172.52596366229557\n",
      "[0, 1, 5]\n",
      "Steps done: 2681\n",
      "SV: [-1.2007464 -2.68458   -2.4800022]\n",
      "Reward for action 15: -254.90005647714239\n",
      "[0, 1]\n",
      "Steps done: 2682\n",
      "SV: [-1.2007464 -2.68458   -2.4800022]\n",
      "Reward for action 7: -332.35491037742133\n",
      "[0, 1, 7]\n",
      "Steps done: 2683\n",
      "SV: [-1.2007464 -2.68458   -2.4800022]\n",
      "Reward for action 6: -232.79082208412018\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 2684\n",
      "SV: [-1.2007464 -2.68458   -2.4800022]\n",
      "Reward for action 20: -228.79082208412018\n",
      "[0, 1, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 392\n",
      "Steps done: 2685\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 3: -24.078865557028127\n",
      "[0, 1, 3]\n",
      "Steps done: 2686\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 7: -23.809927741128803\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 2687\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 5: -10.526470932906799\n",
      "[0, 1, 3, 7, 5]\n",
      "Steps done: 2688\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 9: -10.582987818349782\n",
      "[0, 1, 3, 7, 5, 9]\n",
      "Steps done: 2689\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 13: -10.82998129527281\n",
      "[0, 1, 7, 5, 9]\n",
      "Steps done: 2690\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 8: -10.385888821955021\n",
      "[0, 1, 7, 5, 9, 8]\n",
      "Steps done: 2691\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 4: -13.385348014887771\n",
      "[0, 1, 7, 5, 9, 8, 4]\n",
      "Steps done: 2692\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 18: -15.036931199283897\n",
      "[0, 1, 7, 5, 9, 4]\n",
      "Steps done: 2693\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 19: -22.769154403157806\n",
      "[0, 1, 7, 5, 4]\n",
      "Steps done: 2694\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 10: -25.528384788382223\n",
      "[1, 7, 5, 4]\n",
      "Steps done: 2695\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 6: -20.991442102315187\n",
      "[1, 7, 5, 4, 6]\n",
      "Steps done: 2696\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 9: -16.424200988673064\n",
      "[1, 7, 5, 4, 6, 9]\n",
      "Steps done: 2697\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 8: -15.127908911689799\n",
      "[1, 7, 5, 4, 6, 9, 8]\n",
      "Steps done: 2698\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 11: -15.130701257063265\n",
      "[7, 5, 4, 6, 9, 8]\n",
      "Steps done: 2699\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 16: -8.811781781691408\n",
      "[7, 5, 4, 9, 8]\n",
      "Did target update\n",
      "Steps done: 2700\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 2: -13.337329732575677\n",
      "[7, 5, 4, 9, 8, 2]\n",
      "Steps done: 2701\n",
      "SV: [-0.07273671 -0.05520834  0.53987265]\n",
      "Reward for action 20: -9.337329732575677\n",
      "[7, 5, 4, 9, 8, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 393\n",
      "Steps done: 2702\n",
      "SV: [ 0.44713265 -0.29341525  0.16541597]\n",
      "Reward for action 2: -14.422004259431576\n",
      "[0, 1, 2]\n",
      "Steps done: 2703\n",
      "SV: [ 0.44713265 -0.29341525  0.16541597]\n",
      "Reward for action 20: -10.422004259431576\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 394\n",
      "Steps done: 2704\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 6: -25.5722667588256\n",
      "[0, 1, 6]\n",
      "Steps done: 2705\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 2: -34.177676648540086\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 2706\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 3: -35.62542520908126\n",
      "[0, 1, 6, 2, 3]\n",
      "Steps done: 2707\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 7: -25.172593454197756\n",
      "[0, 1, 6, 2, 3, 7]\n",
      "Steps done: 2708\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 13: -17.943702837915932\n",
      "[0, 1, 6, 2, 7]\n",
      "Steps done: 2709\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 16: -14.074804056902323\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 2710\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 3: -24.647165064809172\n",
      "[0, 1, 2, 7, 3]\n",
      "Steps done: 2711\n",
      "SV: [ 0.10010183 -0.16999756 -0.8645641 ]\n",
      "Reward for action 20: -20.647165064809172\n",
      "[0, 1, 2, 7, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 395\n",
      "Steps done: 2712\n",
      "SV: [-0.31741112 -0.35324377 -0.26842472]\n",
      "Reward for action 20: -81.708242393615\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 396\n",
      "Steps done: 2713\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 2: -41.11156877200494\n",
      "[0, 1, 2]\n",
      "Steps done: 2714\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 5: -52.69987563518057\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 2715\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 12: -64.65625100070402\n",
      "[0, 1, 5]\n",
      "Steps done: 2716\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 11: -36.48610353206214\n",
      "[0, 5]\n",
      "Steps done: 2717\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 7: -50.92253113597414\n",
      "[0, 5, 7]\n",
      "Steps done: 2718\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 15: -161.8828428858619\n",
      "[0, 7]\n",
      "Steps done: 2719\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 6: -114.91829391561753\n",
      "[0, 7, 6]\n",
      "Steps done: 2720\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 16: -161.8828428858619\n",
      "[0, 7]\n",
      "Steps done: 2721\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 4: -84.70662803235466\n",
      "[0, 7, 4]\n",
      "Steps done: 2722\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 5: -29.995719855350124\n",
      "[0, 7, 4, 5]\n",
      "Steps done: 2723\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 6: -76.90545419043356\n",
      "[0, 7, 4, 5, 6]\n",
      "Steps done: 2724\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 10: -125.2008772152825\n",
      "[7, 4, 5, 6]\n",
      "Steps done: 2725\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 16: -31.652270947340146\n",
      "[7, 4, 5]\n",
      "Steps done: 2726\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 15: -162.10473810578327\n",
      "[7, 4]\n",
      "Steps done: 2727\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 3: -167.5002668395007\n",
      "[7, 4, 3]\n",
      "Steps done: 2728\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 14: -173.92771970245806\n",
      "[7, 3]\n",
      "Steps done: 2729\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 2: -146.88724301658337\n",
      "[7, 3, 2]\n",
      "Steps done: 2730\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 0: -60.87981993748114\n",
      "[7, 3, 2, 0]\n",
      "Steps done: 2731\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 12: -69.30109747490465\n",
      "[7, 3, 0]\n",
      "Steps done: 2732\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 10: -173.92771970245806\n",
      "[7, 3]\n",
      "Steps done: 2733\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 0: -69.30109747490465\n",
      "[7, 3, 0]\n",
      "Steps done: 2734\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 1: -40.105393724251165\n",
      "[7, 3, 0, 1]\n",
      "Steps done: 2735\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 6: -72.36332963796337\n",
      "[7, 3, 0, 1, 6]\n",
      "Steps done: 2736\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 2: -64.30093424827844\n",
      "[7, 3, 0, 1, 6, 2]\n",
      "Steps done: 2737\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 4: -64.48380622595802\n",
      "[7, 3, 0, 1, 6, 2, 4]\n",
      "Steps done: 2738\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Reward for action 11: -102.94351177997841\n",
      "[7, 3, 0, 6, 2, 4]\n",
      "Steps done: 2739\n",
      "SV: [-0.6747312   0.24207795  0.13528563]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -98.94351177997841\n",
      "[7, 3, 0, 6, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 397\n",
      "Steps done: 2740\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 4: -3.3112540374936095\n",
      "[0, 1, 4]\n",
      "Steps done: 2741\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 6: -5.622120900541259\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 2742\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 10: -3.343423473060401\n",
      "[1, 4, 6]\n",
      "Steps done: 2743\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 0: -5.6221209005412565\n",
      "[1, 4, 6, 0]\n",
      "Steps done: 2744\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 5: -46.388304120404925\n",
      "[1, 4, 6, 0, 5]\n",
      "Steps done: 2745\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 15: -5.6221209005412565\n",
      "[1, 4, 6, 0]\n",
      "Steps done: 2746\n",
      "SV: [0.03227886 0.0019216  0.17527716]\n",
      "Reward for action 20: -1.6221209005412565\n",
      "[1, 4, 6, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 398\n",
      "Steps done: 2747\n",
      "SV: [ 0.2017807 -1.1782992  0.528397 ]\n",
      "Reward for action 8: -35.016907610400345\n",
      "[0, 1, 8]\n",
      "Steps done: 2748\n",
      "SV: [ 0.2017807 -1.1782992  0.528397 ]\n",
      "Reward for action 18: -101.54704655061158\n",
      "[0, 1]\n",
      "Steps done: 2749\n",
      "SV: [ 0.2017807 -1.1782992  0.528397 ]\n",
      "Reward for action 2: -11.9291977005309\n",
      "[0, 1, 2]\n",
      "Steps done: 2750\n",
      "SV: [ 0.2017807 -1.1782992  0.528397 ]\n",
      "Reward for action 10: -116.23582568455045\n",
      "[1, 2]\n",
      "Steps done: 2751\n",
      "SV: [ 0.2017807 -1.1782992  0.528397 ]\n",
      "Reward for action 20: -112.23582568455045\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 399\n",
      "Steps done: 2752\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 2: -2.987050106318929\n",
      "[0, 1, 2]\n",
      "Steps done: 2753\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 7: -107.9932859176091\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 2754\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 12: -185.9885785985547\n",
      "[0, 1, 7]\n",
      "Steps done: 2755\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 11: -399.7049332898514\n",
      "[0, 7]\n",
      "Steps done: 2756\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 2: -202.28486233817017\n",
      "[0, 7, 2]\n",
      "Steps done: 2757\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 12: -399.7049332898514\n",
      "[0, 7]\n",
      "Steps done: 2758\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 8: -1.3268123090934778\n",
      "[0, 7, 8]\n",
      "Steps done: 2759\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 17: -5.52855922627331\n",
      "[0, 8]\n",
      "Steps done: 2760\n",
      "SV: [ 0.20759657  0.11578102 -0.01883707]\n",
      "Reward for action 20: -1.5285592262733099\n",
      "[0, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 400\n",
      "Steps done: 2761\n",
      "SV: [-0.19057977 -0.10705157 -1.31978   ]\n",
      "Reward for action 20: -447.5169158682183\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 401\n",
      "Steps done: 2762\n",
      "SV: [ 0.70609444 -0.30547777  0.7308909 ]\n",
      "Reward for action 20: -79.76231168581819\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 402\n",
      "Steps done: 2763\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 3: -10.3524793006326\n",
      "[0, 1, 3]\n",
      "Steps done: 2764\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 11: -172.98103117360878\n",
      "[0, 3]\n",
      "Steps done: 2765\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 2: -57.25150019069339\n",
      "[0, 3, 2]\n",
      "Steps done: 2766\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 5: -80.64937419239152\n",
      "[0, 3, 2, 5]\n",
      "Steps done: 2767\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 12: -134.14837266745062\n",
      "[0, 3, 5]\n",
      "Steps done: 2768\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 13: -222.20218013297836\n",
      "[0, 5]\n",
      "Steps done: 2769\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 4: -162.3630391025363\n",
      "[0, 5, 4]\n",
      "Steps done: 2770\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 3: -143.76306897542935\n",
      "[0, 5, 4, 3]\n",
      "Steps done: 2771\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 15: -156.18793094184764\n",
      "[0, 4, 3]\n",
      "Steps done: 2772\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 13: -109.81046484588055\n",
      "[0, 4]\n",
      "Steps done: 2773\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 2: -26.336186566329125\n",
      "[0, 4, 2]\n",
      "Steps done: 2774\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 3: -77.05986780235108\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 2775\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 10: -102.2654569314902\n",
      "[4, 2, 3]\n",
      "Steps done: 2776\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 0: -77.05986780235106\n",
      "[4, 2, 3, 0]\n",
      "Steps done: 2777\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 1: -14.793338793355515\n",
      "[4, 2, 3, 0, 1]\n",
      "Steps done: 2778\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 13: -5.922807509765253\n",
      "[4, 2, 0, 1]\n",
      "Steps done: 2779\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 11: -26.336186566329133\n",
      "[4, 2, 0]\n",
      "Steps done: 2780\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 1: -5.922807509765253\n",
      "[4, 2, 0, 1]\n",
      "Steps done: 2781\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 10: -9.364792465456617\n",
      "[4, 2, 1]\n",
      "Steps done: 2782\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 12: -23.429144156975998\n",
      "[4, 1]\n",
      "Steps done: 2783\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 3: -9.470532327162813\n",
      "[4, 1, 3]\n",
      "Steps done: 2784\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 0: -16.72733655961781\n",
      "[4, 1, 3, 0]\n",
      "Steps done: 2785\n",
      "SV: [ 0.880249   -0.1122325   0.35960647]\n",
      "Reward for action 20: -12.727336559617811\n",
      "[4, 1, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 403\n",
      "Steps done: 2786\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 4: -52.2769998571744\n",
      "[0, 1, 4]\n",
      "Steps done: 2787\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 14: -40.299225292486874\n",
      "[0, 1]\n",
      "Steps done: 2788\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 4: -52.2769998571744\n",
      "[0, 1, 4]\n",
      "Steps done: 2789\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 2: -320.6911001528563\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 2790\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 12: -52.2769998571744\n",
      "[0, 1, 4]\n",
      "Steps done: 2791\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 3: -23.914275762595377\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 2792\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 2: -210.58852494013047\n",
      "[0, 1, 4, 3, 2]\n",
      "Steps done: 2793\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 14: -271.8509823607764\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 2794\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 11: -414.8233667905722\n",
      "[0, 3, 2]\n",
      "Steps done: 2795\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 13: -814.58617725137\n",
      "[0, 2]\n",
      "Steps done: 2796\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 3: -414.8233667905722\n",
      "[0, 2, 3]\n",
      "Steps done: 2797\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 1: -271.8509823607763\n",
      "[0, 2, 3, 1]\n",
      "Steps done: 2798\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 11: -414.8233667905722\n",
      "[0, 2, 3]\n",
      "Steps done: 2799\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 1: -271.8509823607763\n",
      "[0, 2, 3, 1]\n",
      "Did target update\n",
      "Steps done: 2800\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 11: -414.8233667905722\n",
      "[0, 2, 3]\n",
      "Steps done: 2801\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 13: -814.58617725137\n",
      "[0, 2]\n",
      "Steps done: 2802\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 1: -458.9625419588327\n",
      "[0, 2, 1]\n",
      "Steps done: 2803\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 11: -814.58617725137\n",
      "[0, 2]\n",
      "Steps done: 2804\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 4: -436.7197952516417\n",
      "[0, 2, 4]\n",
      "Steps done: 2805\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 12: -38.09591042523667\n",
      "[0, 4]\n",
      "Steps done: 2806\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 2: -436.7197952516417\n",
      "[0, 4, 2]\n",
      "Steps done: 2807\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 14: -814.58617725137\n",
      "[0, 2]\n",
      "Steps done: 2808\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 1: -458.9625419588327\n",
      "[0, 2, 1]\n",
      "Steps done: 2809\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 12: -40.299225292486874\n",
      "[0, 1]\n",
      "Steps done: 2810\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 3: -13.483011731328258\n",
      "[0, 1, 3]\n",
      "Steps done: 2811\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Reward for action 11: -6.959700105932634\n",
      "[0, 3]\n",
      "Steps done: 2812\n",
      "SV: [-0.2297335  -0.07558834  0.7708081 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -2.959700105932634\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 404\n",
      "Steps done: 2813\n",
      "SV: [ 0.09971414 -0.07494286  0.00019734]\n",
      "Reward for action 5: -6.91856928987162\n",
      "[0, 1, 5]\n",
      "Steps done: 2814\n",
      "SV: [ 0.09971414 -0.07494286  0.00019734]\n",
      "Reward for action 20: -2.91856928987162\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 405\n",
      "Steps done: 2815\n",
      "SV: [ 0.5684374  -0.13247569  0.16297078]\n",
      "Reward for action 9: -79.4341581666085\n",
      "[0, 1, 9]\n",
      "Steps done: 2816\n",
      "SV: [ 0.5684374  -0.13247569  0.16297078]\n",
      "Reward for action 5: -20.657094950374294\n",
      "[0, 1, 9, 5]\n",
      "Steps done: 2817\n",
      "SV: [ 0.5684374  -0.13247569  0.16297078]\n",
      "Reward for action 15: -79.4341581666085\n",
      "[0, 1, 9]\n",
      "Steps done: 2818\n",
      "SV: [ 0.5684374  -0.13247569  0.16297078]\n",
      "Reward for action 10: -137.41426380770983\n",
      "[1, 9]\n",
      "Steps done: 2819\n",
      "SV: [ 0.5684374  -0.13247569  0.16297078]\n",
      "Reward for action 3: -35.82481696634267\n",
      "[1, 9, 3]\n",
      "Steps done: 2820\n",
      "SV: [ 0.5684374  -0.13247569  0.16297078]\n",
      "Reward for action 20: -31.824816966342667\n",
      "[1, 9, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 406\n",
      "Steps done: 2821\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 7: -3.4636357301266725\n",
      "[0, 1, 7]\n",
      "Steps done: 2822\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 4: -4.369192682200884\n",
      "[0, 1, 7, 4]\n",
      "Steps done: 2823\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 14: -3.4636357301266725\n",
      "[0, 1, 7]\n",
      "Steps done: 2824\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 8: -5.205491220902208\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 2825\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 11: -7.439898955008067\n",
      "[0, 7, 8]\n",
      "Steps done: 2826\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 1: -5.205491220902206\n",
      "[0, 7, 8, 1]\n",
      "Steps done: 2827\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 4: -3.874183725756172\n",
      "[0, 7, 8, 1, 4]\n",
      "Steps done: 2828\n",
      "SV: [-0.24396269  0.24490456  0.09760094]\n",
      "Reward for action 20: 0.12581627424382802\n",
      "[0, 7, 8, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 407\n",
      "Steps done: 2829\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 3: -64.79624050551908\n",
      "[0, 1, 3]\n",
      "Steps done: 2830\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 13: -14.94165317014027\n",
      "[0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 2831\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 9: -11.488429248661905\n",
      "[0, 1, 9]\n",
      "Steps done: 2832\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 10: -8.674614377303948\n",
      "[1, 9]\n",
      "Steps done: 2833\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 6: -14.786094046105118\n",
      "[1, 9, 6]\n",
      "Steps done: 2834\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 16: -8.674614377303948\n",
      "[1, 9]\n",
      "Steps done: 2835\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 5: -95.47483156259477\n",
      "[1, 9, 5]\n",
      "Steps done: 2836\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 2: -37.42568336253815\n",
      "[1, 9, 5, 2]\n",
      "Steps done: 2837\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 19: -125.61944956907459\n",
      "[1, 5, 2]\n",
      "Steps done: 2838\n",
      "SV: [ 0.01020013  0.01660463 -0.3662158 ]\n",
      "Reward for action 20: -121.61944956907459\n",
      "[1, 5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 408\n",
      "Steps done: 2839\n",
      "SV: [-3.4802139 -0.7690907 -0.1718853]\n",
      "Reward for action 7: -7.982931322533126\n",
      "[0, 1, 7]\n",
      "Steps done: 2840\n",
      "SV: [-3.4802139 -0.7690907 -0.1718853]\n",
      "Reward for action 10: -58.515355465097414\n",
      "[1, 7]\n",
      "Steps done: 2841\n",
      "SV: [-3.4802139 -0.7690907 -0.1718853]\n",
      "Reward for action 20: -54.515355465097414\n",
      "[1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 409\n",
      "Steps done: 2842\n",
      "SV: [-0.21903117  0.02853839 -0.00183939]\n",
      "Reward for action 20: 0.3086622900820015\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 410\n",
      "Steps done: 2843\n",
      "SV: [ 0.29361045 -0.27090505  0.44341633]\n",
      "Reward for action 20: -14.79189089548635\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 411\n",
      "Steps done: 2844\n",
      "SV: [-0.10626768 -1.3816156  -1.3633071 ]\n",
      "Reward for action 3: -49.42305680323206\n",
      "[0, 1, 3]\n",
      "Steps done: 2845\n",
      "SV: [-0.10626768 -1.3816156  -1.3633071 ]\n",
      "Reward for action 10: -191.71634816156228\n",
      "[1, 3]\n",
      "Steps done: 2846\n",
      "SV: [-0.10626768 -1.3816156  -1.3633071 ]\n",
      "Reward for action 0: -49.42305680323206\n",
      "[1, 3, 0]\n",
      "Steps done: 2847\n",
      "SV: [-0.10626768 -1.3816156  -1.3633071 ]\n",
      "Reward for action 11: -166.77873222890344\n",
      "[3, 0]\n",
      "Steps done: 2848\n",
      "SV: [-0.10626768 -1.3816156  -1.3633071 ]\n",
      "Reward for action 2: -53.329696263629494\n",
      "[3, 0, 2]\n",
      "Steps done: 2849\n",
      "SV: [-0.10626768 -1.3816156  -1.3633071 ]\n",
      "Reward for action 20: -49.329696263629494\n",
      "[3, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 412\n",
      "Steps done: 2850\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 3: -334.4531863771327\n",
      "[0, 1, 3]\n",
      "Steps done: 2851\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 13: -278.08463576615026\n",
      "[0, 1]\n",
      "Steps done: 2852\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 3: -334.4531863771327\n",
      "[0, 1, 3]\n",
      "Steps done: 2853\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 2: -263.53212954823914\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 2854\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 13: -267.2933191808962\n",
      "[0, 1, 2]\n",
      "Steps done: 2855\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 8: -156.5108188449158\n",
      "[0, 1, 2, 8]\n",
      "Steps done: 2856\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 12: -129.3378957415011\n",
      "[0, 1, 8]\n",
      "Steps done: 2857\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 7: -105.00717643910926\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 2858\n",
      "SV: [-0.20757051  0.7162051  -1.607283  ]\n",
      "Reward for action 20: -101.00717643910926\n",
      "[0, 1, 8, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 413\n",
      "Steps done: 2859\n",
      "SV: [0.059235   0.05981273 0.3136745 ]\n",
      "Reward for action 20: -1.3954937434967416\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 414\n",
      "Steps done: 2860\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 4: -41.04246276691495\n",
      "[0, 1, 4]\n",
      "Steps done: 2861\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 2: -24.599359495005466\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 2862\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 6: -11.853536483835164\n",
      "[0, 1, 4, 2, 6]\n",
      "Steps done: 2863\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 14: -5.787429103391918\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 2864\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 9: -8.05724212267315\n",
      "[0, 1, 2, 6, 9]\n",
      "Steps done: 2865\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 3: -8.810552668847698\n",
      "[0, 1, 2, 6, 9, 3]\n",
      "Steps done: 2866\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 19: -7.04055356337754\n",
      "[0, 1, 2, 6, 3]\n",
      "Steps done: 2867\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 12: -7.01170311962249\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 2868\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 16: -11.867100281367978\n",
      "[0, 1, 3]\n",
      "Steps done: 2869\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 13: -16.18983459684785\n",
      "[0, 1]\n",
      "Steps done: 2870\n",
      "SV: [0.19309925 0.10276496 0.37246206]\n",
      "Reward for action 20: -12.189834596847849\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 415\n",
      "Steps done: 2871\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 4: -2.7567506131112234\n",
      "[0, 1, 4]\n",
      "Steps done: 2872\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 6: -2.892899517266951\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 2873\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 14: -2.8923161605277494\n",
      "[0, 1, 6]\n",
      "Steps done: 2874\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 4: -2.892899517266952\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 2875\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 8: -10.99072070454103\n",
      "[0, 1, 6, 4, 8]\n",
      "Steps done: 2876\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 2: -6.171708367540607\n",
      "[0, 1, 6, 4, 8, 2]\n",
      "Steps done: 2877\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 7: -7.484141829670218\n",
      "[0, 1, 6, 4, 8, 2, 7]\n",
      "Steps done: 2878\n",
      "SV: [ 0.03393085  0.02351015 -0.18305562]\n",
      "Reward for action 20: -3.484141829670218\n",
      "[0, 1, 6, 4, 8, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 416\n",
      "Steps done: 2879\n",
      "SV: [ 1.4074310e-02  1.9576385e-04 -5.2754515e-01]\n",
      "Reward for action 20: -21.56529367355212\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 417\n",
      "Steps done: 2880\n",
      "SV: [ 0.5263421  -0.12224402  0.09993508]\n",
      "Reward for action 3: -101.07780494256066\n",
      "[0, 1, 3]\n",
      "Steps done: 2881\n",
      "SV: [ 0.5263421  -0.12224402  0.09993508]\n",
      "Reward for action 20: -97.07780494256066\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 418\n",
      "Steps done: 2882\n",
      "SV: [0.19474353 2.0576913  1.7653512 ]\n",
      "Reward for action 2: -95.75153099333139\n",
      "[0, 1, 2]\n",
      "Steps done: 2883\n",
      "SV: [0.19474353 2.0576913  1.7653512 ]\n",
      "Reward for action 12: -105.22226001763943\n",
      "[0, 1]\n",
      "Steps done: 2884\n",
      "SV: [0.19474353 2.0576913  1.7653512 ]\n",
      "Reward for action 3: -193.79742036627965\n",
      "[0, 1, 3]\n",
      "Steps done: 2885\n",
      "SV: [0.19474353 2.0576913  1.7653512 ]\n",
      "Reward for action 20: -189.79742036627965\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 419\n",
      "Steps done: 2886\n",
      "SV: [0.21366425 0.21121943 1.6633551 ]\n",
      "Reward for action 20: -297.694283382162\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 420\n",
      "Steps done: 2887\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 8: -223.1191578643389\n",
      "[0, 1, 8]\n",
      "Steps done: 2888\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 4: -31.397298964649963\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 2889\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 11: -131.77054967584894\n",
      "[0, 8, 4]\n",
      "Steps done: 2890\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 7: -235.68662753152557\n",
      "[0, 8, 4, 7]\n",
      "Steps done: 2891\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 2: -94.88001039790275\n",
      "[0, 8, 4, 7, 2]\n",
      "Steps done: 2892\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 10: -349.84604483596115\n",
      "[8, 4, 7, 2]\n",
      "Steps done: 2893\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 17: -182.21283585914458\n",
      "[8, 4, 2]\n",
      "Steps done: 2894\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 0: -8.799368730217791\n",
      "[8, 4, 2, 0]\n",
      "Steps done: 2895\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 7: -94.88001039790275\n",
      "[8, 4, 2, 0, 7]\n",
      "Steps done: 2896\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 17: -8.799368730217791\n",
      "[8, 4, 2, 0]\n",
      "Steps done: 2897\n",
      "SV: [0.14597158 0.41865206 2.1958475 ]\n",
      "Reward for action 20: -4.799368730217791\n",
      "[8, 4, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 421\n",
      "Steps done: 2898\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 2: -34.64914735222429\n",
      "[0, 1, 2]\n",
      "Steps done: 2899\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 4: -27.06099858829822\n",
      "[0, 1, 2, 4]\n",
      "Did target update\n",
      "Steps done: 2900\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 10: -61.80568327091197\n",
      "[1, 2, 4]\n",
      "Steps done: 2901\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 11: -272.9763185560312\n",
      "[2, 4]\n",
      "Steps done: 2902\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 1: -61.805683270911985\n",
      "[2, 4, 1]\n",
      "Steps done: 2903\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 11: -272.9763185560312\n",
      "[2, 4]\n",
      "Steps done: 2904\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 0: -54.78852288805176\n",
      "[2, 4, 0]\n",
      "Steps done: 2905\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 10: -272.9763185560312\n",
      "[2, 4]\n",
      "Steps done: 2906\n",
      "SV: [-1.3216305   0.23967479 -0.99905616]\n",
      "Reward for action 20: -268.9763185560312\n",
      "[2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 422\n",
      "Steps done: 2907\n",
      "SV: [ 1.307525  -1.0089008 -3.4773908]\n",
      "Reward for action 20: -511.23343252716427\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 423\n",
      "Steps done: 2908\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 2: -1.1042488694088433\n",
      "[0, 1, 2]\n",
      "Steps done: 2909\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 4: -0.5947822575290899\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 2910\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 5: -0.1879149584282196\n",
      "[0, 1, 2, 4, 5]\n",
      "Steps done: 2911\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 11: -0.29450869291054277\n",
      "[0, 2, 4, 5]\n",
      "Steps done: 2912\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 15: -0.5593440727905284\n",
      "[0, 2, 4]\n",
      "Steps done: 2913\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 12: -0.16786079607206744\n",
      "[0, 4]\n",
      "Steps done: 2914\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 1: -0.3627213070699225\n",
      "[0, 4, 1]\n",
      "Steps done: 2915\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 2: -0.5947822575290899\n",
      "[0, 4, 1, 2]\n",
      "Steps done: 2916\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 10: -2.4959030119879504\n",
      "[4, 1, 2]\n",
      "Steps done: 2917\n",
      "SV: [-0.02411005  0.00013195 -0.03945955]\n",
      "Reward for action 20: 1.5040969880120496\n",
      "[4, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 424\n",
      "Steps done: 2918\n",
      "SV: [ 0.7315313 -1.4782255  1.8106155]\n",
      "Reward for action 20: -183.27218310591908\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 425\n",
      "Steps done: 2919\n",
      "SV: [ 1.026273   -0.44653374 -0.60224336]\n",
      "Reward for action 2: -54.04877452400708\n",
      "[0, 1, 2]\n",
      "Steps done: 2920\n",
      "SV: [ 1.026273   -0.44653374 -0.60224336]\n",
      "Reward for action 11: -151.26082475327695\n",
      "[0, 2]\n",
      "Steps done: 2921\n",
      "SV: [ 1.026273   -0.44653374 -0.60224336]\n",
      "Reward for action 1: -54.04877452400708\n",
      "[0, 2, 1]\n",
      "Steps done: 2922\n",
      "SV: [ 1.026273   -0.44653374 -0.60224336]\n",
      "Reward for action 20: -50.04877452400708\n",
      "[0, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 426\n",
      "Steps done: 2923\n",
      "SV: [ 0.5120692  -0.41221768  0.43175372]\n",
      "Reward for action 20: -174.50375117152527\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 427\n",
      "Steps done: 2924\n",
      "SV: [ 0.9319029  -0.39031944 -0.00936134]\n",
      "Reward for action 20: -133.16719573644502\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 428\n",
      "Steps done: 2925\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 3: -30.123461281829716\n",
      "[0, 1, 3]\n",
      "Steps done: 2926\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 2: -36.97239544952503\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 2927\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 11: -42.167280822193\n",
      "[0, 3, 2]\n",
      "Steps done: 2928\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 4: -38.409169754072565\n",
      "[0, 3, 2, 4]\n",
      "Steps done: 2929\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 8: -59.78416869260636\n",
      "[0, 3, 2, 4, 8]\n",
      "Steps done: 2930\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 9: -57.867366750585106\n",
      "[0, 3, 2, 4, 8, 9]\n",
      "Steps done: 2931\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 13: -55.74809763743044\n",
      "[0, 2, 4, 8, 9]\n",
      "Steps done: 2932\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 10: -63.27815806822049\n",
      "[2, 4, 8, 9]\n",
      "Steps done: 2933\n",
      "SV: [-0.11776292 -0.07878001 -0.6062619 ]\n",
      "Reward for action 20: -59.27815806822049\n",
      "[2, 4, 8, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 429\n",
      "Steps done: 2934\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 5: -14.676615729843771\n",
      "[0, 1, 5]\n",
      "Steps done: 2935\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 6: -2.4373241504613454\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 2936\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 3: -2.837750866798546\n",
      "[0, 1, 5, 6, 3]\n",
      "Steps done: 2937\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 10: -23.15146224793258\n",
      "[1, 5, 6, 3]\n",
      "Steps done: 2938\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 13: -22.248569582730916\n",
      "[1, 5, 6]\n",
      "Steps done: 2939\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 4: -10.766987969839496\n",
      "[1, 5, 6, 4]\n",
      "Steps done: 2940\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 11: -9.834286095592114\n",
      "[5, 6, 4]\n",
      "Steps done: 2941\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 0: -22.28419674432945\n",
      "[5, 6, 4, 0]\n",
      "Steps done: 2942\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 3: -17.183637161474653\n",
      "[5, 6, 4, 0, 3]\n",
      "Steps done: 2943\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 13: -22.28419674432945\n",
      "[5, 6, 4, 0]\n",
      "Steps done: 2944\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 1: -24.93562213630794\n",
      "[5, 6, 4, 0, 1]\n",
      "Steps done: 2945\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 2: -30.133565954617524\n",
      "[5, 6, 4, 0, 1, 2]\n",
      "Steps done: 2946\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 16: -18.00367437950236\n",
      "[5, 4, 0, 1, 2]\n",
      "Steps done: 2947\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 15: -163.34361052816593\n",
      "[4, 0, 1, 2]\n",
      "Steps done: 2948\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 10: -180.69331524264834\n",
      "[4, 1, 2]\n",
      "Steps done: 2949\n",
      "SV: [-0.7904725  -0.62005806 -0.81445974]\n",
      "Reward for action 20: -176.69331524264834\n",
      "[4, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 430\n",
      "Steps done: 2950\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 4: -519.0624477014239\n",
      "[0, 1, 4]\n",
      "Steps done: 2951\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 7: -247.83318925132437\n",
      "[0, 1, 4, 7]\n",
      "Steps done: 2952\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 10: -665.1552818743056\n",
      "[1, 4, 7]\n",
      "Steps done: 2953\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 14: -896.7142141728699\n",
      "[1, 7]\n",
      "Steps done: 2954\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 6: -350.563479090125\n",
      "[1, 7, 6]\n",
      "Steps done: 2955\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 9: -107.48642360930367\n",
      "[1, 7, 6, 9]\n",
      "Steps done: 2956\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 5: -3.277777809988824\n",
      "[1, 7, 6, 9, 5]\n",
      "Steps done: 2957\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 15: -107.48642360930367\n",
      "[1, 7, 6, 9]\n",
      "Steps done: 2958\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 16: -24.73073580660239\n",
      "[1, 7, 9]\n",
      "Steps done: 2959\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 0: -22.824188162866093\n",
      "[1, 7, 9, 0]\n",
      "Steps done: 2960\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 3: -37.69095964994122\n",
      "[1, 7, 9, 0, 3]\n",
      "Steps done: 2961\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 13: -22.824188162866093\n",
      "[1, 7, 9, 0]\n",
      "Steps done: 2962\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 2: -14.471678625793512\n",
      "[1, 7, 9, 0, 2]\n",
      "Steps done: 2963\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 11: -21.526963551951123\n",
      "[7, 9, 0, 2]\n",
      "Steps done: 2964\n",
      "SV: [0.09318299 0.29981932 0.6398484 ]\n",
      "Reward for action 20: -17.526963551951123\n",
      "[7, 9, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 431\n",
      "Steps done: 2965\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 5: -54.16371828934773\n",
      "[0, 1, 5]\n",
      "Steps done: 2966\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 10: -45.23269317044017\n",
      "[1, 5]\n",
      "Steps done: 2967\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 2: -36.5098574947907\n",
      "[1, 5, 2]\n",
      "Steps done: 2968\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 7: -31.20993885478381\n",
      "[1, 5, 2, 7]\n",
      "Steps done: 2969\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 15: -73.61626340009587\n",
      "[1, 2, 7]\n",
      "Steps done: 2970\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 6: -33.91286175370917\n",
      "[1, 2, 7, 6]\n",
      "Steps done: 2971\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 0: -33.405138453485996\n",
      "[1, 2, 7, 6, 0]\n",
      "Steps done: 2972\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 16: -51.297995971224545\n",
      "[1, 2, 7, 0]\n",
      "Steps done: 2973\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 11: -77.59454178808241\n",
      "[2, 7, 0]\n",
      "Steps done: 2974\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 17: -32.45963528794913\n",
      "[2, 0]\n",
      "Steps done: 2975\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 5: -33.89254191621219\n",
      "[2, 0, 5]\n",
      "Steps done: 2976\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 1: -39.72517222712201\n",
      "[2, 0, 5, 1]\n",
      "Steps done: 2977\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 6: -48.025120684327135\n",
      "[2, 0, 5, 1, 6]\n",
      "Steps done: 2978\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 15: -53.66550273379333\n",
      "[2, 0, 1, 6]\n",
      "Steps done: 2979\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 7: -33.405138453486\n",
      "[2, 0, 1, 6, 7]\n",
      "Steps done: 2980\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 4: -32.962872640565394\n",
      "[2, 0, 1, 6, 7, 4]\n",
      "Steps done: 2981\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 12: -34.149442765149935\n",
      "[0, 1, 6, 7, 4]\n",
      "Steps done: 2982\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 17: -48.83030999263504\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 2983\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 14: -64.99566162575775\n",
      "[0, 1, 6]\n",
      "Steps done: 2984\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 16: -79.4721243536432\n",
      "[0, 1]\n",
      "Steps done: 2985\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 6: -64.99566162575775\n",
      "[0, 1, 6]\n",
      "Steps done: 2986\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 4: -48.83030999263504\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 2987\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 11: -53.257475159633614\n",
      "[0, 6, 4]\n",
      "Steps done: 2988\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 1: -48.83030999263504\n",
      "[0, 6, 4, 1]\n",
      "Steps done: 2989\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 11: -53.257475159633614\n",
      "[0, 6, 4]\n",
      "Steps done: 2990\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Reward for action 16: -33.946089066562045\n",
      "[0, 4]\n",
      "Steps done: 2991\n",
      "SV: [ 0.29134277  0.3229233  -0.5204104 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -29.946089066562045\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 432\n",
      "Steps done: 2992\n",
      "SV: [0.09110815 0.7390048  0.37065604]\n",
      "Reward for action 20: 2.6119026638019687\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 433\n",
      "Steps done: 2993\n",
      "SV: [-0.05393144 -0.00687594 -0.18052937]\n",
      "Reward for action 4: -42.933510979853594\n",
      "[0, 1, 4]\n",
      "Steps done: 2994\n",
      "SV: [-0.05393144 -0.00687594 -0.18052937]\n",
      "Reward for action 14: -90.8794343921213\n",
      "[0, 1]\n",
      "Steps done: 2995\n",
      "SV: [-0.05393144 -0.00687594 -0.18052937]\n",
      "Reward for action 20: -86.8794343921213\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 434\n",
      "Steps done: 2996\n",
      "SV: [-0.14417265 -0.1251155   0.17726737]\n",
      "Reward for action 4: -0.4451503965893564\n",
      "[0, 1, 4]\n",
      "Steps done: 2997\n",
      "SV: [-0.14417265 -0.1251155   0.17726737]\n",
      "Reward for action 9: -1.2363014685063147\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 2998\n",
      "SV: [-0.14417265 -0.1251155   0.17726737]\n",
      "Reward for action 20: 2.7636985314936853\n",
      "[0, 1, 4, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 435\n",
      "Steps done: 2999\n",
      "SV: [-0.15335761  0.2056394  -0.5625748 ]\n",
      "Reward for action 3: -30.99803303880862\n",
      "[0, 1, 3]\n",
      "Did target update\n",
      "Steps done: 3000\n",
      "SV: [-0.15335761  0.2056394  -0.5625748 ]\n",
      "Reward for action 4: -9.307662987906749\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 3001\n",
      "SV: [-0.15335761  0.2056394  -0.5625748 ]\n",
      "Reward for action 6: -10.323986091351163\n",
      "[0, 1, 3, 4, 6]\n",
      "Steps done: 3002\n",
      "SV: [-0.15335761  0.2056394  -0.5625748 ]\n",
      "Reward for action 20: -6.323986091351163\n",
      "[0, 1, 3, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 436\n",
      "Steps done: 3003\n",
      "SV: [ 0.39216393 -0.13560933  1.1936866 ]\n",
      "Reward for action 20: -805.874233532413\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 437\n",
      "Steps done: 3004\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 2: -253.08000490297928\n",
      "[0, 1, 2]\n",
      "Steps done: 3005\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 12: -626.5059621435671\n",
      "[0, 1]\n",
      "Steps done: 3006\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 6: -612.0527462526851\n",
      "[0, 1, 6]\n",
      "Steps done: 3007\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 11: -604.2193235704722\n",
      "[0, 6]\n",
      "Steps done: 3008\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 2: -590.6152213030239\n",
      "[0, 6, 2]\n",
      "Steps done: 3009\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 1: -407.0147911642263\n",
      "[0, 6, 2, 1]\n",
      "Steps done: 3010\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 4: -131.9203378754891\n",
      "[0, 6, 2, 1, 4]\n",
      "Steps done: 3011\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 3: -7.144237410081237\n",
      "[0, 6, 2, 1, 4, 3]\n",
      "Steps done: 3012\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 13: -131.9203378754891\n",
      "[0, 6, 2, 1, 4]\n",
      "Steps done: 3013\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 14: -407.0147911642263\n",
      "[0, 6, 2, 1]\n",
      "Steps done: 3014\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 10: -248.8711824230199\n",
      "[6, 2, 1]\n",
      "Steps done: 3015\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 4: -164.61492010962363\n",
      "[6, 2, 1, 4]\n",
      "Steps done: 3016\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 14: -248.8711824230199\n",
      "[6, 2, 1]\n",
      "Steps done: 3017\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 0: -407.0147911642263\n",
      "[6, 2, 1, 0]\n",
      "Steps done: 3018\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 10: -248.8711824230199\n",
      "[6, 2, 1]\n",
      "Steps done: 3019\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 0: -407.0147911642263\n",
      "[6, 2, 1, 0]\n",
      "Steps done: 3020\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 10: -248.8711824230199\n",
      "[6, 2, 1]\n",
      "Steps done: 3021\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 12: -606.1554816878805\n",
      "[6, 1]\n",
      "Steps done: 3022\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 3: -9.933084027445645\n",
      "[6, 1, 3]\n",
      "Steps done: 3023\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 13: -606.1554816878805\n",
      "[6, 1]\n",
      "Steps done: 3024\n",
      "SV: [-1.1012396  1.0213     5.02581  ]\n",
      "Reward for action 20: -602.1554816878805\n",
      "[6, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 438\n",
      "Steps done: 3025\n",
      "SV: [-2.8424034  2.1858552  3.6396909]\n",
      "Reward for action 4: -18.38235433094644\n",
      "[0, 1, 4]\n",
      "Steps done: 3026\n",
      "SV: [-2.8424034  2.1858552  3.6396909]\n",
      "Reward for action 20: -14.382354330946441\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 439\n",
      "Steps done: 3027\n",
      "SV: [ 1.5797144  -0.80798656  2.301616  ]\n",
      "Reward for action 20: 1.514865654597159\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 440\n",
      "Steps done: 3028\n",
      "SV: [ 0.00534355 -0.01291043  0.6645281 ]\n",
      "Reward for action 6: -5.551424804322973\n",
      "[0, 1, 6]\n",
      "Steps done: 3029\n",
      "SV: [ 0.00534355 -0.01291043  0.6645281 ]\n",
      "Reward for action 10: -468.0259003961336\n",
      "[1, 6]\n",
      "Steps done: 3030\n",
      "SV: [ 0.00534355 -0.01291043  0.6645281 ]\n",
      "Reward for action 4: -10.598728016666911\n",
      "[1, 6, 4]\n",
      "Steps done: 3031\n",
      "SV: [ 0.00534355 -0.01291043  0.6645281 ]\n",
      "Reward for action 14: -468.0259003961336\n",
      "[1, 6]\n",
      "Steps done: 3032\n",
      "SV: [ 0.00534355 -0.01291043  0.6645281 ]\n",
      "Reward for action 20: -464.0259003961336\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 441\n",
      "Steps done: 3033\n",
      "SV: [-0.1489837  -0.33785826 -0.15245457]\n",
      "Reward for action 8: -14.56114192317584\n",
      "[0, 1, 8]\n",
      "Steps done: 3034\n",
      "SV: [-0.1489837  -0.33785826 -0.15245457]\n",
      "Reward for action 4: -80.79395667190414\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 3035\n",
      "SV: [-0.1489837  -0.33785826 -0.15245457]\n",
      "Reward for action 5: -33.21678271112103\n",
      "[0, 1, 8, 4, 5]\n",
      "Steps done: 3036\n",
      "SV: [-0.1489837  -0.33785826 -0.15245457]\n",
      "Reward for action 10: -44.36185855584646\n",
      "[1, 8, 4, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3037\n",
      "SV: [-0.1489837  -0.33785826 -0.15245457]\n",
      "Reward for action 15: -126.45971557983168\n",
      "[1, 8, 4]\n",
      "Steps done: 3038\n",
      "SV: [-0.1489837  -0.33785826 -0.15245457]\n",
      "Reward for action 20: -122.45971557983168\n",
      "[1, 8, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 442\n",
      "Steps done: 3039\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 5: -3.4300826475031636\n",
      "[0, 1, 5]\n",
      "Steps done: 3040\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 11: -0.836946626889396\n",
      "[0, 5]\n",
      "Steps done: 3041\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 3: -0.7628768938599321\n",
      "[0, 5, 3]\n",
      "Steps done: 3042\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 4: -0.5643431642009848\n",
      "[0, 5, 3, 4]\n",
      "Steps done: 3043\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 10: -0.65133332817907\n",
      "[5, 3, 4]\n",
      "Steps done: 3044\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 13: -0.6821315706778365\n",
      "[5, 4]\n",
      "Steps done: 3045\n",
      "SV: [-0.01329505  0.00528883  0.1061728 ]\n",
      "Reward for action 20: 3.3178684293221634\n",
      "[5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 443\n",
      "Steps done: 3046\n",
      "SV: [-0.10627877 -2.2915213   1.0428084 ]\n",
      "Reward for action 20: -111.65713253620258\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 444\n",
      "Steps done: 3047\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 5: -19.971032142098803\n",
      "[0, 1, 5]\n",
      "Steps done: 3048\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 11: -45.872630846955715\n",
      "[0, 5]\n",
      "Steps done: 3049\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 8: -73.99214914411917\n",
      "[0, 5, 8]\n",
      "Steps done: 3050\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 15: -133.24508759760326\n",
      "[0, 8]\n",
      "Steps done: 3051\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 6: -45.735851613014816\n",
      "[0, 8, 6]\n",
      "Steps done: 3052\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 16: -133.24508759760326\n",
      "[0, 8]\n",
      "Steps done: 3053\n",
      "SV: [ 0.12700488  0.050911   -0.7408932 ]\n",
      "Reward for action 20: -129.24508759760326\n",
      "[0, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 445\n",
      "Steps done: 3054\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 4: -56.098090273094215\n",
      "[0, 1, 4]\n",
      "Steps done: 3055\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 11: -49.526239612152246\n",
      "[0, 4]\n",
      "Steps done: 3056\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 3: -47.992310792825435\n",
      "[0, 4, 3]\n",
      "Steps done: 3057\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 2: -57.378864920658955\n",
      "[0, 4, 3, 2]\n",
      "Steps done: 3058\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 13: -66.73297224001561\n",
      "[0, 4, 2]\n",
      "Steps done: 3059\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 3: -57.378864920658984\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 3060\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 1: -55.89286095855818\n",
      "[0, 4, 2, 3, 1]\n",
      "Steps done: 3061\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 11: -57.378864920658984\n",
      "[0, 4, 2, 3]\n",
      "Steps done: 3062\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 12: -47.992310792825435\n",
      "[0, 4, 3]\n",
      "Steps done: 3063\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 10: -1.162385379549822\n",
      "[4, 3]\n",
      "Steps done: 3064\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 1: -3.595200362268231\n",
      "[4, 3, 1]\n",
      "Steps done: 3065\n",
      "SV: [-0.57878673 -0.24330533 -1.6994202 ]\n",
      "Reward for action 20: 0.40479963773176886\n",
      "[4, 3, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 446\n",
      "Steps done: 3066\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 3: -396.1391366997745\n",
      "[0, 1, 3]\n",
      "Steps done: 3067\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 6: -244.93456400743264\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 3068\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 8: -215.51360019486327\n",
      "[0, 1, 3, 6, 8]\n",
      "Steps done: 3069\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 13: -206.84741738891054\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 3070\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 11: -18.445493272228866\n",
      "[0, 6, 8]\n",
      "Steps done: 3071\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 4: -42.745188276297185\n",
      "[0, 6, 8, 4]\n",
      "Steps done: 3072\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 10: -198.4474221575643\n",
      "[6, 8, 4]\n",
      "Steps done: 3073\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 7: -74.38144807534087\n",
      "[6, 8, 4, 7]\n",
      "Steps done: 3074\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 17: -198.4474221575643\n",
      "[6, 8, 4]\n",
      "Steps done: 3075\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 14: -262.91603581896373\n",
      "[6, 8]\n",
      "Steps done: 3076\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 0: -18.44549327222886\n",
      "[6, 8, 0]\n",
      "Steps done: 3077\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 9: -0.6629749367881914\n",
      "[6, 8, 0, 9]\n",
      "Steps done: 3078\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 19: -18.44549327222886\n",
      "[6, 8, 0]\n",
      "Steps done: 3079\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 7: -4.014894242541267\n",
      "[6, 8, 0, 7]\n",
      "Steps done: 3080\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 5: -37.597410277224455\n",
      "[6, 8, 0, 7, 5]\n",
      "Steps done: 3081\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 10: -77.24896855310557\n",
      "[6, 8, 7, 5]\n",
      "Steps done: 3082\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 0: -37.59741027722447\n",
      "[6, 8, 7, 5, 0]\n",
      "Steps done: 3083\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 3: -63.15955100547034\n",
      "[6, 8, 7, 5, 0, 3]\n",
      "Steps done: 3084\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 13: -37.59741027722447\n",
      "[6, 8, 7, 5, 0]\n",
      "Steps done: 3085\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 1: -148.03268557759444\n",
      "[6, 8, 7, 5, 0, 1]\n",
      "Steps done: 3086\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 18: -155.8257424460546\n",
      "[6, 7, 5, 0, 1]\n",
      "Steps done: 3087\n",
      "SV: [-0.42309952  0.21418054  1.5374051 ]\n",
      "Reward for action 20: -151.8257424460546\n",
      "[6, 7, 5, 0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 447\n",
      "Steps done: 3088\n",
      "SV: [-0.2601156   0.19135842  1.171687  ]\n",
      "Reward for action 20: -272.76557866648864\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 448\n",
      "Steps done: 3089\n",
      "SV: [-0.1084692   0.03985327  0.9828253 ]\n",
      "Reward for action 20: -4.786500464556463\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 449\n",
      "Steps done: 3090\n",
      "SV: [ 0.06764255 -0.05043343  0.34321854]\n",
      "Reward for action 2: -10.826659232391242\n",
      "[0, 1, 2]\n",
      "Steps done: 3091\n",
      "SV: [ 0.06764255 -0.05043343  0.34321854]\n",
      "Reward for action 11: -17.533351668958193\n",
      "[0, 2]\n",
      "Steps done: 3092\n",
      "SV: [ 0.06764255 -0.05043343  0.34321854]\n",
      "Reward for action 20: -13.533351668958193\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 450\n",
      "Steps done: 3093\n",
      "SV: [0.81946355 0.07821374 1.2709    ]\n",
      "Reward for action 3: -6.038808484412855\n",
      "[0, 1, 3]\n",
      "Steps done: 3094\n",
      "SV: [0.81946355 0.07821374 1.2709    ]\n",
      "Reward for action 20: -2.038808484412855\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 451\n",
      "Steps done: 3095\n",
      "SV: [-0.7635996  -0.11617654  0.3961514 ]\n",
      "Reward for action 5: -46.182723785046754\n",
      "[0, 1, 5]\n",
      "Steps done: 3096\n",
      "SV: [-0.7635996  -0.11617654  0.3961514 ]\n",
      "Reward for action 6: -27.940776290895755\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 3097\n",
      "SV: [-0.7635996  -0.11617654  0.3961514 ]\n",
      "Reward for action 20: -23.940776290895755\n",
      "[0, 1, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 452\n",
      "Steps done: 3098\n",
      "SV: [ 0.14118946 -0.06014678 -0.38826331]\n",
      "Reward for action 20: -10.198227274059796\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 453\n",
      "Steps done: 3099\n",
      "SV: [-1.2170323  -1.0056973   0.12550655]\n",
      "Reward for action 20: -105.48808745538497\n",
      "[0, 1]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 454\n",
      "Steps done: 3100\n",
      "SV: [-0.0311405  -0.00287998 -0.17198397]\n",
      "Reward for action 5: -2.797864908959159\n",
      "[0, 1, 5]\n",
      "Steps done: 3101\n",
      "SV: [-0.0311405  -0.00287998 -0.17198397]\n",
      "Reward for action 4: -3.725593068013926\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 3102\n",
      "SV: [-0.0311405  -0.00287998 -0.17198397]\n",
      "Reward for action 20: 0.274406931986074\n",
      "[0, 1, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 455\n",
      "Steps done: 3103\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 3: -244.85697005865995\n",
      "[0, 1, 3]\n",
      "Steps done: 3104\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 6: -105.71677637488116\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 3105\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 7: -71.79484486203854\n",
      "[0, 1, 3, 6, 7]\n",
      "Steps done: 3106\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 2: -44.011681773740186\n",
      "[0, 1, 3, 6, 7, 2]\n",
      "Steps done: 3107\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 8: -24.02044137719041\n",
      "[0, 1, 3, 6, 7, 2, 8]\n",
      "Steps done: 3108\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 5: -34.50527081338095\n",
      "[0, 1, 3, 6, 7, 2, 8, 5]\n",
      "Steps done: 3109\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 12: -46.48449792668201\n",
      "[0, 1, 3, 6, 7, 8, 5]\n",
      "Steps done: 3110\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 15: -36.919471629353815\n",
      "[0, 1, 3, 6, 7, 8]\n",
      "Steps done: 3111\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 10: -26.917211290633958\n",
      "[1, 3, 6, 7, 8]\n",
      "Steps done: 3112\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 13: -9.543709610491254\n",
      "[1, 6, 7, 8]\n",
      "Steps done: 3113\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 16: -10.714748491602327\n",
      "[1, 7, 8]\n",
      "Steps done: 3114\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 11: -50.636140019105284\n",
      "[7, 8]\n",
      "Steps done: 3115\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 6: -13.455616673751432\n",
      "[7, 8, 6]\n",
      "Steps done: 3116\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 0: -14.062738883709406\n",
      "[7, 8, 6, 0]\n",
      "Steps done: 3117\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 3: -32.98531811179727\n",
      "[7, 8, 6, 0, 3]\n",
      "Steps done: 3118\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 18: -68.61438977713391\n",
      "[7, 6, 0, 3]\n",
      "Steps done: 3119\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 5: -64.87921545100399\n",
      "[7, 6, 0, 3, 5]\n",
      "Steps done: 3120\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 15: -68.61438977713391\n",
      "[7, 6, 0, 3]\n",
      "Steps done: 3121\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 13: -14.980976390797307\n",
      "[7, 6, 0]\n",
      "Steps done: 3122\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 1: -27.807252113395652\n",
      "[7, 6, 0, 1]\n",
      "Steps done: 3123\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 5: -43.41145098002937\n",
      "[7, 6, 0, 1, 5]\n",
      "Steps done: 3124\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 11: -41.429479903934784\n",
      "[7, 6, 0, 5]\n",
      "Steps done: 3125\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 17: -16.804461956212286\n",
      "[6, 0, 5]\n",
      "Steps done: 3126\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 4: -38.14280711664288\n",
      "[6, 0, 5, 4]\n",
      "Steps done: 3127\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 7: -57.720569173866146\n",
      "[6, 0, 5, 4, 7]\n",
      "Steps done: 3128\n",
      "SV: [ 0.7355961   0.46693712 -0.9037676 ]\n",
      "Reward for action 20: -53.720569173866146\n",
      "[6, 0, 5, 4, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 456\n",
      "Steps done: 3129\n",
      "SV: [ 0.24871488 -0.23919426  1.9771789 ]\n",
      "Reward for action 2: -170.82079929233905\n",
      "[0, 1, 2]\n",
      "Steps done: 3130\n",
      "SV: [ 0.24871488 -0.23919426  1.9771789 ]\n",
      "Reward for action 3: -173.8295941215446\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 3131\n",
      "SV: [ 0.24871488 -0.23919426  1.9771789 ]\n",
      "Reward for action 20: -169.8295941215446\n",
      "[0, 1, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 457\n",
      "Steps done: 3132\n",
      "SV: [-0.41657263  0.5781333   1.4298905 ]\n",
      "Reward for action 2: -3.0890115734323054\n",
      "[0, 1, 2]\n",
      "Steps done: 3133\n",
      "SV: [-0.41657263  0.5781333   1.4298905 ]\n",
      "Reward for action 10: -21.56750100016005\n",
      "[1, 2]\n",
      "Steps done: 3134\n",
      "SV: [-0.41657263  0.5781333   1.4298905 ]\n",
      "Reward for action 20: -17.56750100016005\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 458\n",
      "Steps done: 3135\n",
      "SV: [ 0.12079539 -0.2244631  -0.4601563 ]\n",
      "Reward for action 3: -46.344267391545316\n",
      "[0, 1, 3]\n",
      "Steps done: 3136\n",
      "SV: [ 0.12079539 -0.2244631  -0.4601563 ]\n",
      "Reward for action 7: -229.8325671172635\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 3137\n",
      "SV: [ 0.12079539 -0.2244631  -0.4601563 ]\n",
      "Reward for action 17: -46.344267391545316\n",
      "[0, 1, 3]\n",
      "Steps done: 3138\n",
      "SV: [ 0.12079539 -0.2244631  -0.4601563 ]\n",
      "Reward for action 20: -42.344267391545316\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 459\n",
      "Steps done: 3139\n",
      "SV: [ 0.16677614  0.26632062 -0.11367577]\n",
      "Reward for action 2: -34.856703737736964\n",
      "[0, 1, 2]\n",
      "Steps done: 3140\n",
      "SV: [ 0.16677614  0.26632062 -0.11367577]\n",
      "Reward for action 10: -14.817299673190117\n",
      "[1, 2]\n",
      "Steps done: 3141\n",
      "SV: [ 0.16677614  0.26632062 -0.11367577]\n",
      "Reward for action 0: -34.856703737736964\n",
      "[1, 2, 0]\n",
      "Steps done: 3142\n",
      "SV: [ 0.16677614  0.26632062 -0.11367577]\n",
      "Reward for action 20: -30.856703737736964\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 460\n",
      "Steps done: 3143\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 2: -17.71217217062239\n",
      "[0, 1, 2]\n",
      "Steps done: 3144\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 12: -17.62356190967227\n",
      "[0, 1]\n",
      "Steps done: 3145\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 4: -17.554253107581598\n",
      "[0, 1, 4]\n",
      "Steps done: 3146\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 11: -17.10768368525986\n",
      "[0, 4]\n",
      "Steps done: 3147\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 2: -17.364819796463056\n",
      "[0, 4, 2]\n",
      "Steps done: 3148\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 10: -17.050099570447053\n",
      "[4, 2]\n",
      "Steps done: 3149\n",
      "SV: [ 0.00259535  0.04040336 -0.4640336 ]\n",
      "Reward for action 20: -13.050099570447053\n",
      "[4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 461\n",
      "Steps done: 3150\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 7: -531.7264807345052\n",
      "[0, 1, 7]\n",
      "Steps done: 3151\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 2: -432.47343083590454\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 3152\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 3: -269.1863045428036\n",
      "[0, 1, 7, 2, 3]\n",
      "Steps done: 3153\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 13: -432.47343083590454\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 3154\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 17: -262.73783737918075\n",
      "[0, 1, 2]\n",
      "Steps done: 3155\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 8: -115.98083465877079\n",
      "[0, 1, 2, 8]\n",
      "Steps done: 3156\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 18: -262.73783737918075\n",
      "[0, 1, 2]\n",
      "Steps done: 3157\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 10: -239.28837621759965\n",
      "[1, 2]\n",
      "Steps done: 3158\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 7: -487.68060804966103\n",
      "[1, 2, 7]\n",
      "Steps done: 3159\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 17: -239.28837621759965\n",
      "[1, 2]\n",
      "Steps done: 3160\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 3: -122.07864182741113\n",
      "[1, 2, 3]\n",
      "Steps done: 3161\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 7: -332.22352854200363\n",
      "[1, 2, 3, 7]\n",
      "Steps done: 3162\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 9: -324.08227747559897\n",
      "[1, 2, 3, 7, 9]\n",
      "Steps done: 3163\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 12: -354.24244767200685\n",
      "[1, 3, 7, 9]\n",
      "Steps done: 3164\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 0: -290.3154728957716\n",
      "[1, 3, 7, 9, 0]\n",
      "Steps done: 3165\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 2: -282.6492570720128\n",
      "[1, 3, 7, 9, 0, 2]\n",
      "Steps done: 3166\n",
      "SV: [ 0.21662574 -0.23280619  0.5833661 ]\n",
      "Reward for action 20: -278.6492570720128\n",
      "[1, 3, 7, 9, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 462\n",
      "Steps done: 3167\n",
      "SV: [ 0.10083505 -0.05917716 -0.58368045]\n",
      "Reward for action 6: -26.719603604484888\n",
      "[0, 1, 6]\n",
      "Steps done: 3168\n",
      "SV: [ 0.10083505 -0.05917716 -0.58368045]\n",
      "Reward for action 11: -15.662576481908072\n",
      "[0, 6]\n",
      "Steps done: 3169\n",
      "SV: [ 0.10083505 -0.05917716 -0.58368045]\n",
      "Reward for action 20: -11.662576481908072\n",
      "[0, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 463\n",
      "Steps done: 3170\n",
      "SV: [ 0.17392713 -0.6305337  -2.1996346 ]\n",
      "Reward for action 7: -553.8296696969651\n",
      "[0, 1, 7]\n",
      "Steps done: 3171\n",
      "SV: [ 0.17392713 -0.6305337  -2.1996346 ]\n",
      "Reward for action 8: -24.809831590230655\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 3172\n",
      "SV: [ 0.17392713 -0.6305337  -2.1996346 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 2: -25.800436582103888\n",
      "[0, 1, 7, 8, 2]\n",
      "Steps done: 3173\n",
      "SV: [ 0.17392713 -0.6305337  -2.1996346 ]\n",
      "Reward for action 3: -84.93255221517258\n",
      "[0, 1, 7, 8, 2, 3]\n",
      "Steps done: 3174\n",
      "SV: [ 0.17392713 -0.6305337  -2.1996346 ]\n",
      "Reward for action 12: -98.34694308720354\n",
      "[0, 1, 7, 8, 3]\n",
      "Steps done: 3175\n",
      "SV: [ 0.17392713 -0.6305337  -2.1996346 ]\n",
      "Reward for action 20: -94.34694308720354\n",
      "[0, 1, 7, 8, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 464\n",
      "Steps done: 3176\n",
      "SV: [-0.42388564 -0.33899233  0.4147204 ]\n",
      "Reward for action 2: -26.762716394469525\n",
      "[0, 1, 2]\n",
      "Steps done: 3177\n",
      "SV: [-0.42388564 -0.33899233  0.4147204 ]\n",
      "Reward for action 11: -22.946941110774056\n",
      "[0, 2]\n",
      "Steps done: 3178\n",
      "SV: [-0.42388564 -0.33899233  0.4147204 ]\n",
      "Reward for action 1: -26.762716394469525\n",
      "[0, 2, 1]\n",
      "Steps done: 3179\n",
      "SV: [-0.42388564 -0.33899233  0.4147204 ]\n",
      "Reward for action 11: -22.946941110774056\n",
      "[0, 2]\n",
      "Steps done: 3180\n",
      "SV: [-0.42388564 -0.33899233  0.4147204 ]\n",
      "Reward for action 4: -50.12000373128812\n",
      "[0, 2, 4]\n",
      "Steps done: 3181\n",
      "SV: [-0.42388564 -0.33899233  0.4147204 ]\n",
      "Reward for action 20: -46.12000373128812\n",
      "[0, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 465\n",
      "Steps done: 3182\n",
      "SV: [0.67611426 0.20861223 1.7419147 ]\n",
      "Reward for action 20: -257.1822373073062\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 466\n",
      "Steps done: 3183\n",
      "SV: [-0.36950442 -0.05364068  0.897658  ]\n",
      "Reward for action 20: -78.04689495447505\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 467\n",
      "Steps done: 3184\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 7: -41.57390045705854\n",
      "[0, 1, 7]\n",
      "Steps done: 3185\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 10: -54.159451578379404\n",
      "[1, 7]\n",
      "Steps done: 3186\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 2: -10.164330374738046\n",
      "[1, 7, 2]\n",
      "Steps done: 3187\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 17: -7.877799891363425\n",
      "[1, 2]\n",
      "Steps done: 3188\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 6: -8.20217250175143\n",
      "[1, 2, 6]\n",
      "Steps done: 3189\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 12: -21.25078511052993\n",
      "[1, 6]\n",
      "Steps done: 3190\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 4: -49.63326607409154\n",
      "[1, 6, 4]\n",
      "Steps done: 3191\n",
      "SV: [ 0.52373904  0.14766254 -0.5074063 ]\n",
      "Reward for action 20: -45.63326607409154\n",
      "[1, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 468\n",
      "Steps done: 3192\n",
      "SV: [ 0.24936274 -0.278648    0.83318853]\n",
      "Reward for action 3: -2.0498203540687996\n",
      "[0, 1, 3]\n",
      "Steps done: 3193\n",
      "SV: [ 0.24936274 -0.278648    0.83318853]\n",
      "Reward for action 10: -17.91731981324827\n",
      "[1, 3]\n",
      "Steps done: 3194\n",
      "SV: [ 0.24936274 -0.278648    0.83318853]\n",
      "Reward for action 2: -3.533164598146215\n",
      "[1, 3, 2]\n",
      "Steps done: 3195\n",
      "SV: [ 0.24936274 -0.278648    0.83318853]\n",
      "Reward for action 4: -6.877816212248765\n",
      "[1, 3, 2, 4]\n",
      "Steps done: 3196\n",
      "SV: [ 0.24936274 -0.278648    0.83318853]\n",
      "Reward for action 13: -165.62942273526954\n",
      "[1, 2, 4]\n",
      "Steps done: 3197\n",
      "SV: [ 0.24936274 -0.278648    0.83318853]\n",
      "Reward for action 20: -161.62942273526954\n",
      "[1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 469\n",
      "Steps done: 3198\n",
      "SV: [ 0.00418486  0.01194002 -1.4127673 ]\n",
      "Reward for action 20: -146.6155103014382\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 470\n",
      "Steps done: 3199\n",
      "SV: [-0.33860314 -0.6954709  -0.69896173]\n",
      "Reward for action 2: -268.71428919893043\n",
      "[0, 1, 2]\n",
      "Did target update\n",
      "Steps done: 3200\n",
      "SV: [-0.33860314 -0.6954709  -0.69896173]\n",
      "Reward for action 11: -89.58998397421483\n",
      "[0, 2]\n",
      "Steps done: 3201\n",
      "SV: [-0.33860314 -0.6954709  -0.69896173]\n",
      "Reward for action 3: -187.05172361817174\n",
      "[0, 2, 3]\n",
      "Steps done: 3202\n",
      "SV: [-0.33860314 -0.6954709  -0.69896173]\n",
      "Reward for action 13: -89.58998397421483\n",
      "[0, 2]\n",
      "Steps done: 3203\n",
      "SV: [-0.33860314 -0.6954709  -0.69896173]\n",
      "Reward for action 20: -85.58998397421483\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 471\n",
      "Steps done: 3204\n",
      "SV: [ 0.16980158 -0.15837654  1.1074284 ]\n",
      "Reward for action 7: -153.30907811426474\n",
      "[0, 1, 7]\n",
      "Steps done: 3205\n",
      "SV: [ 0.16980158 -0.15837654  1.1074284 ]\n",
      "Reward for action 20: -149.30907811426474\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 472\n",
      "Steps done: 3206\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 8: -4.0682495513395125\n",
      "[0, 1, 8]\n",
      "Steps done: 3207\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 5: -4.0154678589867725\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 3208\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 2: -4.101500349211372\n",
      "[0, 1, 8, 5, 2]\n",
      "Steps done: 3209\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 15: -4.192804305657067\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 3210\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 11: -3.8846588579903316\n",
      "[0, 8, 2]\n",
      "Steps done: 3211\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 7: -7.861910957213557\n",
      "[0, 8, 2, 7]\n",
      "Steps done: 3212\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 17: -3.8846588579903316\n",
      "[0, 8, 2]\n",
      "Steps done: 3213\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 3: -4.074898697685057\n",
      "[0, 8, 2, 3]\n",
      "Steps done: 3214\n",
      "SV: [0.01150259 0.00365518 0.23606959]\n",
      "Reward for action 20: -0.07489869768505741\n",
      "[0, 8, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 473\n",
      "Steps done: 3215\n",
      "SV: [-0.8979767 -0.5336451 -2.8214648]\n",
      "Reward for action 3: -235.01392085541187\n",
      "[0, 1, 3]\n",
      "Steps done: 3216\n",
      "SV: [-0.8979767 -0.5336451 -2.8214648]\n",
      "Reward for action 10: -728.3211544249439\n",
      "[1, 3]\n",
      "Steps done: 3217\n",
      "SV: [-0.8979767 -0.5336451 -2.8214648]\n",
      "Reward for action 20: -724.3211544249439\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 474\n",
      "Steps done: 3218\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 3: -0.2291574610786483\n",
      "[0, 1, 3]\n",
      "Steps done: 3219\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 11: -0.2028914960162928\n",
      "[0, 3]\n",
      "Steps done: 3220\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 1: -0.22915746107864832\n",
      "[0, 3, 1]\n",
      "Steps done: 3221\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 9: -5.754386703001875\n",
      "[0, 3, 1, 9]\n",
      "Steps done: 3222\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 11: -12.307412729367908\n",
      "[0, 3, 9]\n",
      "Steps done: 3223\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 8: -7.162459797363689\n",
      "[0, 3, 9, 8]\n",
      "Steps done: 3224\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 18: -12.307412729367908\n",
      "[0, 3, 9]\n",
      "Steps done: 3225\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 8: -7.162459797363689\n",
      "[0, 3, 9, 8]\n",
      "Steps done: 3226\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 18: -12.307412729367908\n",
      "[0, 3, 9]\n",
      "Steps done: 3227\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 8: -7.162459797363689\n",
      "[0, 3, 9, 8]\n",
      "Steps done: 3228\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 13: -5.932961899510079\n",
      "[0, 9, 8]\n",
      "Steps done: 3229\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 6: -4.742935548527349\n",
      "[0, 9, 8, 6]\n",
      "Steps done: 3230\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 1: -3.2094003100346655\n",
      "[0, 9, 8, 6, 1]\n",
      "Steps done: 3231\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 10: -4.8257800926228756\n",
      "[9, 8, 6, 1]\n",
      "Steps done: 3232\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 5: -7.988787513952892\n",
      "[9, 8, 6, 1, 5]\n",
      "Steps done: 3233\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 16: -12.424432883503414\n",
      "[9, 8, 1, 5]\n",
      "Steps done: 3234\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 4: -9.78808653535033\n",
      "[9, 8, 1, 5, 4]\n",
      "Steps done: 3235\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 6: -5.722582432222597\n",
      "[9, 8, 1, 5, 4, 6]\n",
      "Steps done: 3236\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 18: -7.79476657084891\n",
      "[9, 1, 5, 4, 6]\n",
      "Steps done: 3237\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 2: -3.762699098637136\n",
      "[9, 1, 5, 4, 6, 2]\n",
      "Steps done: 3238\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 19: -0.5817804898437249\n",
      "[1, 5, 4, 6, 2]\n",
      "Steps done: 3239\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 8: -0.3559969225952658\n",
      "[1, 5, 4, 6, 2, 8]\n",
      "Steps done: 3240\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 14: -0.9263640082135187\n",
      "[1, 5, 6, 2, 8]\n",
      "Steps done: 3241\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 0: -0.6883117158271854\n",
      "[1, 5, 6, 2, 8, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3242\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 4: -0.319190833266777\n",
      "[1, 5, 6, 2, 8, 0, 4]\n",
      "Steps done: 3243\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Reward for action 14: -0.6883117158271854\n",
      "[1, 5, 6, 2, 8, 0]\n",
      "Steps done: 3244\n",
      "SV: [0.03858009 0.05229991 0.0394859 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: 3.3116882841728144\n",
      "[1, 5, 6, 2, 8, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 475\n",
      "Steps done: 3245\n",
      "SV: [0.16214447 0.01204199 0.06218741]\n",
      "Reward for action 2: -1.930986968263988\n",
      "[0, 1, 2]\n",
      "Steps done: 3246\n",
      "SV: [0.16214447 0.01204199 0.06218741]\n",
      "Reward for action 12: -2.8632941745528457\n",
      "[0, 1]\n",
      "Steps done: 3247\n",
      "SV: [0.16214447 0.01204199 0.06218741]\n",
      "Reward for action 4: -2.6649107426598433\n",
      "[0, 1, 4]\n",
      "Steps done: 3248\n",
      "SV: [0.16214447 0.01204199 0.06218741]\n",
      "Reward for action 3: -2.5266529311362795\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 3249\n",
      "SV: [0.16214447 0.01204199 0.06218741]\n",
      "Reward for action 6: -3.6148346041908654\n",
      "[0, 1, 4, 3, 6]\n",
      "Steps done: 3250\n",
      "SV: [0.16214447 0.01204199 0.06218741]\n",
      "Reward for action 20: 0.38516539580913456\n",
      "[0, 1, 4, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 476\n",
      "Steps done: 3251\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 8: -544.8813850203487\n",
      "[0, 1, 8]\n",
      "Steps done: 3252\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 3: -468.67446503874896\n",
      "[0, 1, 8, 3]\n",
      "Steps done: 3253\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 11: -614.5876317416374\n",
      "[0, 8, 3]\n",
      "Steps done: 3254\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 13: -827.1950699541554\n",
      "[0, 8]\n",
      "Steps done: 3255\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 2: -601.182437038504\n",
      "[0, 8, 2]\n",
      "Steps done: 3256\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 3: -505.77294860320876\n",
      "[0, 8, 2, 3]\n",
      "Steps done: 3257\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 13: -601.182437038504\n",
      "[0, 8, 2]\n",
      "Steps done: 3258\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 7: -693.0590683795274\n",
      "[0, 8, 2, 7]\n",
      "Steps done: 3259\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 9: -136.42052328330323\n",
      "[0, 8, 2, 7, 9]\n",
      "Steps done: 3260\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 19: -693.0590683795274\n",
      "[0, 8, 2, 7]\n",
      "Steps done: 3261\n",
      "SV: [ 0.06376686 -0.39019522  1.6997799 ]\n",
      "Reward for action 20: -689.0590683795274\n",
      "[0, 8, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 477\n",
      "Steps done: 3262\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 4: -31.902869397370473\n",
      "[0, 1, 4]\n",
      "Steps done: 3263\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 2: -34.769759392417534\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 3264\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 12: -31.902869397370473\n",
      "[0, 1, 4]\n",
      "Steps done: 3265\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 8: -22.406054526126542\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 3266\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 18: -31.902869397370473\n",
      "[0, 1, 4]\n",
      "Steps done: 3267\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 9: -52.212950465385944\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 3268\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 14: -46.45126876957747\n",
      "[0, 1, 9]\n",
      "Steps done: 3269\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 10: -38.58105192865868\n",
      "[1, 9]\n",
      "Steps done: 3270\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 6: -53.21650955147992\n",
      "[1, 9, 6]\n",
      "Steps done: 3271\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 2: -15.42860642290477\n",
      "[1, 9, 6, 2]\n",
      "Steps done: 3272\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 5: -12.217380612335926\n",
      "[1, 9, 6, 2, 5]\n",
      "Steps done: 3273\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 16: -5.4520396665319\n",
      "[1, 9, 2, 5]\n",
      "Steps done: 3274\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 0: -10.591344271194975\n",
      "[1, 9, 2, 5, 0]\n",
      "Steps done: 3275\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 12: -15.291679089105367\n",
      "[1, 9, 5, 0]\n",
      "Steps done: 3276\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 7: -3.720342562029785\n",
      "[1, 9, 5, 0, 7]\n",
      "Steps done: 3277\n",
      "SV: [-0.60060996 -0.23062642 -0.24929774]\n",
      "Reward for action 20: 0.27965743797021503\n",
      "[1, 9, 5, 0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 478\n",
      "Steps done: 3278\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 3: -41.070541230038685\n",
      "[0, 1, 3]\n",
      "Steps done: 3279\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 11: -37.27343746100859\n",
      "[0, 3]\n",
      "Steps done: 3280\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 6: -18.638015557069153\n",
      "[0, 3, 6]\n",
      "Steps done: 3281\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 2: -19.218454475363444\n",
      "[0, 3, 6, 2]\n",
      "Steps done: 3282\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 7: -30.035477470771333\n",
      "[0, 3, 6, 2, 7]\n",
      "Steps done: 3283\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 13: -38.476989031047104\n",
      "[0, 6, 2, 7]\n",
      "Steps done: 3284\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 12: -22.30458014228448\n",
      "[0, 6, 7]\n",
      "Steps done: 3285\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 4: -26.85311571484001\n",
      "[0, 6, 7, 4]\n",
      "Steps done: 3286\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 10: -21.646584314661503\n",
      "[6, 7, 4]\n",
      "Steps done: 3287\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 14: -14.410574844700212\n",
      "[6, 7]\n",
      "Steps done: 3288\n",
      "SV: [-0.08400957  0.29916868  0.64988065]\n",
      "Reward for action 20: -10.410574844700212\n",
      "[6, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 479\n",
      "Steps done: 3289\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 9: -17.99740427059844\n",
      "[0, 1, 9]\n",
      "Steps done: 3290\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 3: -26.699993623174255\n",
      "[0, 1, 9, 3]\n",
      "Steps done: 3291\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 8: -21.124882606067416\n",
      "[0, 1, 9, 3, 8]\n",
      "Steps done: 3292\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 19: -18.4199043602338\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 3293\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 18: -22.98934332432306\n",
      "[0, 1, 3]\n",
      "Steps done: 3294\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 9: -26.699993623174255\n",
      "[0, 1, 3, 9]\n",
      "Steps done: 3295\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 19: -22.98934332432306\n",
      "[0, 1, 3]\n",
      "Steps done: 3296\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 8: -18.4199043602338\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 3297\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 13: -21.377700555087735\n",
      "[0, 1, 8]\n",
      "Steps done: 3298\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 11: -19.04723884581943\n",
      "[0, 8]\n",
      "Steps done: 3299\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 6: -34.766694650843675\n",
      "[0, 8, 6]\n",
      "Did target update\n",
      "Steps done: 3300\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 16: -19.04723884581943\n",
      "[0, 8]\n",
      "Steps done: 3301\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 7: -70.34813950458351\n",
      "[0, 8, 7]\n",
      "Steps done: 3302\n",
      "SV: [-0.29572883  0.3649213   0.10582568]\n",
      "Reward for action 20: -66.34813950458351\n",
      "[0, 8, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 480\n",
      "Steps done: 3303\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 2: -9.661166670251475\n",
      "[0, 1, 2]\n",
      "Steps done: 3304\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 4: -15.734548613911072\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 3305\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 10: -12.08810828167578\n",
      "[1, 2, 4]\n",
      "Steps done: 3306\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 0: -15.734548613911079\n",
      "[1, 2, 4, 0]\n",
      "Steps done: 3307\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 12: -7.98994274195907\n",
      "[1, 4, 0]\n",
      "Steps done: 3308\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 5: -3.442868131300376\n",
      "[1, 4, 0, 5]\n",
      "Steps done: 3309\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 3: -1.8985358800723102\n",
      "[1, 4, 0, 5, 3]\n",
      "Steps done: 3310\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 13: -3.442868131300376\n",
      "[1, 4, 0, 5]\n",
      "Steps done: 3311\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 14: -22.94877468799625\n",
      "[1, 0, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3312\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 3: -2.578546996403251\n",
      "[1, 0, 5, 3]\n",
      "Steps done: 3313\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 15: -6.882577810932661\n",
      "[1, 0, 3]\n",
      "Steps done: 3314\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 11: -20.115855312068465\n",
      "[0, 3]\n",
      "Steps done: 3315\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 4: -23.54928180953595\n",
      "[0, 3, 4]\n",
      "Steps done: 3316\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 1: -11.298231277499205\n",
      "[0, 3, 4, 1]\n",
      "Steps done: 3317\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 10: -10.327554454983863\n",
      "[3, 4, 1]\n",
      "Steps done: 3318\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 14: -10.281480168215596\n",
      "[3, 1]\n",
      "Steps done: 3319\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 2: -9.372708285260721\n",
      "[3, 1, 2]\n",
      "Steps done: 3320\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 5: -1.8362377454746608\n",
      "[3, 1, 2, 5]\n",
      "Steps done: 3321\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 11: -1.1440081770814587\n",
      "[3, 2, 5]\n",
      "Steps done: 3322\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 0: -3.207793757128982\n",
      "[3, 2, 5, 0]\n",
      "Steps done: 3323\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 10: -1.1440081770814587\n",
      "[3, 2, 5]\n",
      "Steps done: 3324\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 15: -17.362079169877273\n",
      "[3, 2]\n",
      "Steps done: 3325\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 0: -28.528966848037538\n",
      "[3, 2, 0]\n",
      "Steps done: 3326\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 5: -3.2077937571289805\n",
      "[3, 2, 0, 5]\n",
      "Steps done: 3327\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 13: -2.0542077014935147\n",
      "[2, 0, 5]\n",
      "Steps done: 3328\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Reward for action 10: -16.687668427221\n",
      "[2, 5]\n",
      "Steps done: 3329\n",
      "SV: [-0.13014598 -0.36617935  0.05610607]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -12.687668427220999\n",
      "[2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 481\n",
      "Steps done: 3330\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 2: -1.274086398056801\n",
      "[0, 1, 2]\n",
      "Steps done: 3331\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 11: -10.220385060843217\n",
      "[0, 2]\n",
      "Steps done: 3332\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 6: -1.3253501489330135\n",
      "[0, 2, 6]\n",
      "Steps done: 3333\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 7: -0.8302998814763536\n",
      "[0, 2, 6, 7]\n",
      "Steps done: 3334\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 17: -1.3253501489330135\n",
      "[0, 2, 6]\n",
      "Steps done: 3335\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 4: -0.43889483285715486\n",
      "[0, 2, 6, 4]\n",
      "Steps done: 3336\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 14: -1.3253501489330135\n",
      "[0, 2, 6]\n",
      "Steps done: 3337\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 4: -0.43889483285715486\n",
      "[0, 2, 6, 4]\n",
      "Steps done: 3338\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 14: -1.3253501489330135\n",
      "[0, 2, 6]\n",
      "Steps done: 3339\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 4: -0.43889483285715486\n",
      "[0, 2, 6, 4]\n",
      "Steps done: 3340\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 8: -5.318136110225655\n",
      "[0, 2, 6, 4, 8]\n",
      "Steps done: 3341\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 14: -15.92468131934402\n",
      "[0, 2, 6, 8]\n",
      "Steps done: 3342\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 4: -5.318136110225657\n",
      "[0, 2, 6, 8, 4]\n",
      "Steps done: 3343\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 9: -1.4582441849911958\n",
      "[0, 2, 6, 8, 4, 9]\n",
      "Steps done: 3344\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 10: -0.7460396646141245\n",
      "[2, 6, 8, 4, 9]\n",
      "Steps done: 3345\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 12: -8.047220832623967\n",
      "[6, 8, 4, 9]\n",
      "Steps done: 3346\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 0: -6.335795174273641\n",
      "[6, 8, 4, 9, 0]\n",
      "Steps done: 3347\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 7: -3.926246432635536\n",
      "[6, 8, 4, 9, 0, 7]\n",
      "Steps done: 3348\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 10: -7.391268322984553\n",
      "[6, 8, 4, 9, 7]\n",
      "Steps done: 3349\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 5: -11.50527655104838\n",
      "[6, 8, 4, 9, 7, 5]\n",
      "Steps done: 3350\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 0: -9.573672233929102\n",
      "[6, 8, 4, 9, 7, 5, 0]\n",
      "Steps done: 3351\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 16: -11.0525076415199\n",
      "[8, 4, 9, 7, 5, 0]\n",
      "Steps done: 3352\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 6: -9.573672233929104\n",
      "[8, 4, 9, 7, 5, 0, 6]\n",
      "Steps done: 3353\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 16: -11.0525076415199\n",
      "[8, 4, 9, 7, 5, 0]\n",
      "Steps done: 3354\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 2: -6.311312447911269\n",
      "[8, 4, 9, 7, 5, 0, 2]\n",
      "Steps done: 3355\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Reward for action 19: -12.058050711641629\n",
      "[8, 4, 7, 5, 0, 2]\n",
      "Steps done: 3356\n",
      "SV: [-0.38883924 -0.4011663  -0.27208748]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -8.058050711641629\n",
      "[8, 4, 7, 5, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 482\n",
      "Steps done: 3357\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 3: -30.415121115866576\n",
      "[0, 1, 3]\n",
      "Steps done: 3358\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 8: -24.934385793996544\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 3359\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 13: -2.3383746650835713\n",
      "[0, 1, 8]\n",
      "Steps done: 3360\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 6: -44.604694462715244\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 3361\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 10: -82.20606376348998\n",
      "[1, 8, 6]\n",
      "Steps done: 3362\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 2: -12.432462025643128\n",
      "[1, 8, 6, 2]\n",
      "Steps done: 3363\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 16: -7.256559933766701\n",
      "[1, 8, 2]\n",
      "Steps done: 3364\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 0: -2.199887549466741\n",
      "[1, 8, 2, 0]\n",
      "Steps done: 3365\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 6: -13.060913435159126\n",
      "[1, 8, 2, 0, 6]\n",
      "Steps done: 3366\n",
      "SV: [-0.82661504 -0.27976882 -0.70713633]\n",
      "Reward for action 20: -9.060913435159126\n",
      "[1, 8, 2, 0, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 483\n",
      "Steps done: 3367\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 5: -343.0069706789925\n",
      "[0, 1, 5]\n",
      "Steps done: 3368\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 11: -902.1761869969017\n",
      "[0, 5]\n",
      "Steps done: 3369\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 3: -13.659691079494904\n",
      "[0, 5, 3]\n",
      "Steps done: 3370\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 15: -45.29976992581462\n",
      "[0, 3]\n",
      "Steps done: 3371\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 5: -13.659691079494902\n",
      "[0, 3, 5]\n",
      "Steps done: 3372\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 2: -6.914227587420067\n",
      "[0, 3, 5, 2]\n",
      "Steps done: 3373\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 13: -106.32296971228841\n",
      "[0, 5, 2]\n",
      "Steps done: 3374\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 12: -902.1761869969017\n",
      "[0, 5]\n",
      "Steps done: 3375\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 4: -706.7500390123594\n",
      "[0, 5, 4]\n",
      "Steps done: 3376\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 3: -86.23709066313936\n",
      "[0, 5, 4, 3]\n",
      "Steps done: 3377\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 13: -706.7500390123594\n",
      "[0, 5, 4]\n",
      "Steps done: 3378\n",
      "SV: [-2.8116224 -2.450091  -4.3355937]\n",
      "Reward for action 20: -702.7500390123594\n",
      "[0, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 484\n",
      "Steps done: 3379\n",
      "SV: [-0.5715772 -0.3591341  1.2685617]\n",
      "Reward for action 2: -216.8062055582348\n",
      "[0, 1, 2]\n",
      "Steps done: 3380\n",
      "SV: [-0.5715772 -0.3591341  1.2685617]\n",
      "Reward for action 4: -367.2166219255196\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 3381\n",
      "SV: [-0.5715772 -0.3591341  1.2685617]\n",
      "Reward for action 20: -363.2166219255196\n",
      "[0, 1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 485\n",
      "Steps done: 3382\n",
      "SV: [ 0.14342646  0.05096696 -0.494057  ]\n",
      "Reward for action 20: -7.399831585456161\n",
      "[0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 486\n",
      "Steps done: 3383\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 6: -24.823881195126123\n",
      "[0, 1, 6]\n",
      "Steps done: 3384\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 11: -44.38361557573266\n",
      "[0, 6]\n",
      "Steps done: 3385\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 5: -12.417157450395585\n",
      "[0, 6, 5]\n",
      "Steps done: 3386\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 15: -44.38361557573266\n",
      "[0, 6]\n",
      "Steps done: 3387\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 1: -24.82388119512612\n",
      "[0, 6, 1]\n",
      "Steps done: 3388\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 5: -16.063101941820463\n",
      "[0, 6, 1, 5]\n",
      "Steps done: 3389\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 10: -12.660974236388794\n",
      "[6, 1, 5]\n",
      "Steps done: 3390\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 16: -26.344832982087276\n",
      "[1, 5]\n",
      "Steps done: 3391\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 3: -20.989026623683483\n",
      "[1, 5, 3]\n",
      "Steps done: 3392\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 15: -10.15861024692478\n",
      "[1, 3]\n",
      "Steps done: 3393\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 6: -29.01331796840131\n",
      "[1, 3, 6]\n",
      "Steps done: 3394\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 16: -10.15861024692478\n",
      "[1, 3]\n",
      "Steps done: 3395\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 0: -9.863184650922245\n",
      "[1, 3, 0]\n",
      "Steps done: 3396\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 2: -17.903364648199535\n",
      "[1, 3, 0, 2]\n",
      "Steps done: 3397\n",
      "SV: [-0.514277    0.12431993 -0.56373453]\n",
      "Reward for action 20: -13.903364648199535\n",
      "[1, 3, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 487\n",
      "Steps done: 3398\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 3: -0.9481967197205549\n",
      "[0, 1, 3]\n",
      "Steps done: 3399\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 6: -7.848935471954665\n",
      "[0, 1, 3, 6]\n",
      "Did target update\n",
      "Steps done: 3400\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 10: -12.378990470382416\n",
      "[1, 3, 6]\n",
      "Steps done: 3401\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 2: -10.72864086713497\n",
      "[1, 3, 6, 2]\n",
      "Steps done: 3402\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 13: -10.602131710314403\n",
      "[1, 6, 2]\n",
      "Steps done: 3403\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 0: -7.979074769872955\n",
      "[1, 6, 2, 0]\n",
      "Steps done: 3404\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 10: -10.602131710314403\n",
      "[1, 6, 2]\n",
      "Steps done: 3405\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 12: -10.279174520949862\n",
      "[1, 6]\n",
      "Steps done: 3406\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 3: -12.378990470382416\n",
      "[1, 6, 3]\n",
      "Steps done: 3407\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 16: -1.4275873982534368\n",
      "[1, 3]\n",
      "Steps done: 3408\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 6: -12.378990470382416\n",
      "[1, 3, 6]\n",
      "Steps done: 3409\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 13: -10.279174520949862\n",
      "[1, 6]\n",
      "Steps done: 3410\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 2: -10.602131710314403\n",
      "[1, 6, 2]\n",
      "Steps done: 3411\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 11: -38.99330791334928\n",
      "[6, 2]\n",
      "Steps done: 3412\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 5: -80.83074842419497\n",
      "[6, 2, 5]\n",
      "Steps done: 3413\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 12: -48.09182893175074\n",
      "[6, 5]\n",
      "Steps done: 3414\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 3: -16.74083328331897\n",
      "[6, 5, 3]\n",
      "Steps done: 3415\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 4: -53.691481362789396\n",
      "[6, 5, 3, 4]\n",
      "Steps done: 3416\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 16: -81.8597881815213\n",
      "[5, 3, 4]\n",
      "Steps done: 3417\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 6: -53.691481362789396\n",
      "[5, 3, 4, 6]\n",
      "Steps done: 3418\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 0: -36.729295001287795\n",
      "[5, 3, 4, 6, 0]\n",
      "Steps done: 3419\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 10: -53.691481362789396\n",
      "[5, 3, 4, 6]\n",
      "Steps done: 3420\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 0: -36.729295001287795\n",
      "[5, 3, 4, 6, 0]\n",
      "Steps done: 3421\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 10: -53.691481362789396\n",
      "[5, 3, 4, 6]\n",
      "Steps done: 3422\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 13: -76.31499556346134\n",
      "[5, 4, 6]\n",
      "Steps done: 3423\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Reward for action 1: -24.050545250632087\n",
      "[5, 4, 6, 1]\n",
      "Steps done: 3424\n",
      "SV: [ 0.03424786 -0.35698518 -0.08797764]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -20.050545250632087\n",
      "[5, 4, 6, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 488\n",
      "Steps done: 3425\n",
      "SV: [ 0.10565525  0.14164634 -0.31391427]\n",
      "Reward for action 20: -6.437647852830953\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 489\n",
      "Steps done: 3426\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 3: -108.0920772977223\n",
      "[0, 1, 3]\n",
      "Steps done: 3427\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 2: -117.25899101563324\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 3428\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 6: -116.87079140327238\n",
      "[0, 1, 3, 2, 6]\n",
      "Steps done: 3429\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 13: -116.2915538658092\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 3430\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 8: -106.80482090360375\n",
      "[0, 1, 2, 6, 8]\n",
      "Steps done: 3431\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 12: -101.16352766250662\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 3432\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 10: -97.7293068083841\n",
      "[1, 6, 8]\n",
      "Steps done: 3433\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 5: -106.8485071049995\n",
      "[1, 6, 8, 5]\n",
      "Steps done: 3434\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 2: -112.18654609707963\n",
      "[1, 6, 8, 5, 2]\n",
      "Steps done: 3435\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 3: -111.46686268148567\n",
      "[1, 6, 8, 5, 2, 3]\n",
      "Steps done: 3436\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 12: -105.68546730699252\n",
      "[1, 6, 8, 5, 3]\n",
      "Steps done: 3437\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 0: -107.47354832268111\n",
      "[1, 6, 8, 5, 3, 0]\n",
      "Steps done: 3438\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 7: -111.51254175817265\n",
      "[1, 6, 8, 5, 3, 0, 7]\n",
      "Steps done: 3439\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 11: -113.70128644759775\n",
      "[6, 8, 5, 3, 0, 7]\n",
      "Steps done: 3440\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 9: -111.857088384789\n",
      "[6, 8, 5, 3, 0, 7, 9]\n",
      "Steps done: 3441\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 17: -107.92917571650166\n",
      "[6, 8, 5, 3, 0, 9]\n",
      "Steps done: 3442\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 7: -111.85708838478892\n",
      "[6, 8, 5, 3, 0, 9, 7]\n",
      "Steps done: 3443\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 17: -107.92917571650166\n",
      "[6, 8, 5, 3, 0, 9]\n",
      "Steps done: 3444\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 18: -108.64413971022555\n",
      "[6, 5, 3, 0, 9]\n",
      "Steps done: 3445\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 13: -108.59642961053167\n",
      "[6, 5, 0, 9]\n",
      "Steps done: 3446\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 3: -108.64413971022547\n",
      "[6, 5, 0, 9, 3]\n",
      "Steps done: 3447\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 19: -109.96575670462904\n",
      "[6, 5, 0, 3]\n",
      "Steps done: 3448\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 4: -98.76179150911915\n",
      "[6, 5, 0, 3, 4]\n",
      "Steps done: 3449\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 14: -109.96575670462904\n",
      "[6, 5, 0, 3]\n",
      "Steps done: 3450\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 2: -118.910339768812\n",
      "[6, 5, 0, 3, 2]\n",
      "Steps done: 3451\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Reward for action 4: -114.52333192135062\n",
      "[6, 5, 0, 3, 2, 4]\n",
      "Steps done: 3452\n",
      "SV: [-0.21141377  0.00755553  1.1082346 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -110.52333192135062\n",
      "[6, 5, 0, 3, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 490\n",
      "Steps done: 3453\n",
      "SV: [1.2418864  0.7352902  0.06267726]\n",
      "Reward for action 20: -235.17920188424887\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 491\n",
      "Steps done: 3454\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 4: -7.453823606202408\n",
      "[0, 1, 4]\n",
      "Steps done: 3455\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 7: -10.44012419139589\n",
      "[0, 1, 4, 7]\n",
      "Steps done: 3456\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 10: -14.995126196507428\n",
      "[1, 4, 7]\n",
      "Steps done: 3457\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 2: -14.203048156613875\n",
      "[1, 4, 7, 2]\n",
      "Steps done: 3458\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 0: -11.780473498976242\n",
      "[1, 4, 7, 2, 0]\n",
      "Steps done: 3459\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 6: -13.281515779558479\n",
      "[1, 4, 7, 2, 0, 6]\n",
      "Steps done: 3460\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 17: -13.189632863035824\n",
      "[1, 4, 2, 0, 6]\n",
      "Steps done: 3461\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 5: -14.810568521969323\n",
      "[1, 4, 2, 0, 6, 5]\n",
      "Steps done: 3462\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 8: -15.312420603677054\n",
      "[1, 4, 2, 0, 6, 5, 8]\n",
      "Steps done: 3463\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 12: -13.499303926239877\n",
      "[1, 4, 0, 6, 5, 8]\n",
      "Steps done: 3464\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 14: -16.324916058588993\n",
      "[1, 0, 6, 5, 8]\n",
      "Steps done: 3465\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 11: -15.970460851122963\n",
      "[0, 6, 5, 8]\n",
      "Steps done: 3466\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 10: -27.951558577179306\n",
      "[6, 5, 8]\n",
      "Steps done: 3467\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 16: -36.94118930388416\n",
      "[5, 8]\n",
      "Steps done: 3468\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 2: -34.65330060237817\n",
      "[5, 8, 2]\n",
      "Steps done: 3469\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 4: -20.409371176714878\n",
      "[5, 8, 2, 4]\n",
      "Steps done: 3470\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 1: -18.889882318559536\n",
      "[5, 8, 2, 4, 1]\n",
      "Steps done: 3471\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 11: -20.409371176714878\n",
      "[5, 8, 2, 4]\n",
      "Steps done: 3472\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 15: -18.259474187161857\n",
      "[8, 2, 4]\n",
      "Steps done: 3473\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 6: -19.49382859738278\n",
      "[8, 2, 4, 6]\n",
      "Steps done: 3474\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 9: -18.29925719125615\n",
      "[8, 2, 4, 6, 9]\n",
      "Steps done: 3475\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 0: -13.320864177201395\n",
      "[8, 2, 4, 6, 9, 0]\n",
      "Steps done: 3476\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 16: -11.500888422362529\n",
      "[8, 2, 4, 9, 0]\n",
      "Steps done: 3477\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 18: -10.520444067669906\n",
      "[2, 4, 9, 0]\n",
      "Steps done: 3478\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 5: -12.473077944497739\n",
      "[2, 4, 9, 0, 5]\n",
      "Steps done: 3479\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Reward for action 8: -13.754212208308175\n",
      "[2, 4, 9, 0, 5, 8]\n",
      "Steps done: 3480\n",
      "SV: [0.12419394 0.04354708 0.40902242]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -9.754212208308175\n",
      "[2, 4, 9, 0, 5, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 492\n",
      "Steps done: 3481\n",
      "SV: [-1.0782579 -0.5156954  1.6586838]\n",
      "Reward for action 20: -682.6053123869274\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 493\n",
      "Steps done: 3482\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 8: -17.896349058274335\n",
      "[0, 1, 8]\n",
      "Steps done: 3483\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 5: -15.927060759672946\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 3484\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 9: -10.992802319916567\n",
      "[0, 1, 8, 5, 9]\n",
      "Steps done: 3485\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 10: -26.740046061450506\n",
      "[1, 8, 5, 9]\n",
      "Steps done: 3486\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 15: -30.365973161117154\n",
      "[1, 8, 9]\n",
      "Steps done: 3487\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 11: -52.01468848179912\n",
      "[8, 9]\n",
      "Steps done: 3488\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 5: -44.80851558268733\n",
      "[8, 9, 5]\n",
      "Steps done: 3489\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 2: -33.77775576292455\n",
      "[8, 9, 5, 2]\n",
      "Steps done: 3490\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 0: -13.257718081385555\n",
      "[8, 9, 5, 2, 0]\n",
      "Steps done: 3491\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 1: -10.177902984584444\n",
      "[8, 9, 5, 2, 0, 1]\n",
      "Steps done: 3492\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 10: -22.980843387810197\n",
      "[8, 9, 5, 2, 1]\n",
      "Steps done: 3493\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 19: -34.2409644710751\n",
      "[8, 5, 2, 1]\n",
      "Steps done: 3494\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 6: -22.086938187215615\n",
      "[8, 5, 2, 1, 6]\n",
      "Steps done: 3495\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 0: -11.514073369773294\n",
      "[8, 5, 2, 1, 6, 0]\n",
      "Steps done: 3496\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 7: -11.39872604239436\n",
      "[8, 5, 2, 1, 6, 0, 7]\n",
      "Steps done: 3497\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 11: -16.52851039467123\n",
      "[8, 5, 2, 6, 0, 7]\n",
      "Steps done: 3498\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 10: -21.633441904987606\n",
      "[8, 5, 2, 6, 7]\n",
      "Steps done: 3499\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 15: -32.15708426880307\n",
      "[8, 2, 6, 7]\n",
      "Did target update\n",
      "Steps done: 3500\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 17: -77.41882902048823\n",
      "[8, 2, 6]\n",
      "Steps done: 3501\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 3: -34.9411802293036\n",
      "[8, 2, 6, 3]\n",
      "Steps done: 3502\n",
      "SV: [-3.0808465e-04 -1.6513871e-01 -3.6736652e-01]\n",
      "Reward for action 20: -30.941180229303598\n",
      "[8, 2, 6, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 494\n",
      "Steps done: 3503\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 3: -85.15179345251572\n",
      "[0, 1, 3]\n",
      "Steps done: 3504\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 7: -127.32698023184548\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 3505\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 6: -123.89771825015883\n",
      "[0, 1, 3, 7, 6]\n",
      "Steps done: 3506\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 16: -127.32698023184548\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 3507\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 4: -84.74643072380388\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 3508\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 5: -38.940646404948204\n",
      "[0, 1, 3, 7, 4, 5]\n",
      "Steps done: 3509\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 6: -44.31561625661381\n",
      "[0, 1, 3, 7, 4, 5, 6]\n",
      "Steps done: 3510\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 11: -21.2722321441461\n",
      "[0, 3, 7, 4, 5, 6]\n",
      "Steps done: 3511\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 15: -52.14864190227881\n",
      "[0, 3, 7, 4, 6]\n",
      "Steps done: 3512\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 2: -61.00788369054932\n",
      "[0, 3, 7, 4, 6, 2]\n",
      "Steps done: 3513\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 17: -52.42579795731766\n",
      "[0, 3, 4, 6, 2]\n",
      "Steps done: 3514\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 7: -61.00788369054929\n",
      "[0, 3, 4, 6, 2, 7]\n",
      "Steps done: 3515\n",
      "SV: [ 0.52620757  0.06084339 -1.3294622 ]\n",
      "Reward for action 20: -57.00788369054929\n",
      "[0, 3, 4, 6, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 495\n",
      "Steps done: 3516\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 3: -6.5205902129835644\n",
      "[0, 1, 3]\n",
      "Steps done: 3517\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 10: -7.350599347499962\n",
      "[1, 3]\n",
      "Steps done: 3518\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 6: -5.852350388965336\n",
      "[1, 3, 6]\n",
      "Steps done: 3519\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 5: -5.395963412267497\n",
      "[1, 3, 6, 5]\n",
      "Steps done: 3520\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 4: -5.190716092048413\n",
      "[1, 3, 6, 5, 4]\n",
      "Steps done: 3521\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 13: -5.101514488287887\n",
      "[1, 6, 5, 4]\n",
      "Steps done: 3522\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 7: -6.02458417199766\n",
      "[1, 6, 5, 4, 7]\n",
      "Steps done: 3523\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 14: -6.349965031234069\n",
      "[1, 6, 5, 7]\n",
      "Steps done: 3524\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 3: -5.908142570312732\n",
      "[1, 6, 5, 7, 3]\n",
      "Steps done: 3525\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 4: -5.933518195358004\n",
      "[1, 6, 5, 7, 3, 4]\n",
      "Steps done: 3526\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 16: -8.502775184372702\n",
      "[1, 5, 7, 3, 4]\n",
      "Steps done: 3527\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 11: -9.126056754194984\n",
      "[5, 7, 3, 4]\n",
      "Steps done: 3528\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 13: -9.301713703168524\n",
      "[5, 7, 4]\n",
      "Steps done: 3529\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 14: -9.730759681624034\n",
      "[5, 7]\n",
      "Steps done: 3530\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 3: -9.15380670602691\n",
      "[5, 7, 3]\n",
      "Steps done: 3531\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 13: -9.730759681624034\n",
      "[5, 7]\n",
      "Steps done: 3532\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 3: -9.15380670602691\n",
      "[5, 7, 3]\n",
      "Steps done: 3533\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 15: -9.40815074025583\n",
      "[7, 3]\n",
      "Steps done: 3534\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 4: -9.295110835792515\n",
      "[7, 3, 4]\n",
      "Steps done: 3535\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 6: -5.16682169726302\n",
      "[7, 3, 4, 6]\n",
      "Steps done: 3536\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 14: -5.296045948783136\n",
      "[7, 3, 6]\n",
      "Steps done: 3537\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 17: -16.49911822194543\n",
      "[3, 6]\n",
      "Steps done: 3538\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 4: -5.8432133376010835\n",
      "[3, 6, 4]\n",
      "Steps done: 3539\n",
      "SV: [-0.23633051 -0.03668268 -0.17873636]\n",
      "Reward for action 20: -1.8432133376010835\n",
      "[3, 6, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 496\n",
      "Steps done: 3540\n",
      "SV: [ 0.44235143 -0.00725471 -2.346559  ]\n",
      "Reward for action 20: -2.100608080365056\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 497\n",
      "Steps done: 3541\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 5: -166.44107735517224\n",
      "[0, 1, 5]\n",
      "Steps done: 3542\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 4: -77.89498457585272\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 3543\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 7: -86.04413155598945\n",
      "[0, 1, 5, 4, 7]\n",
      "Steps done: 3544\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 11: -64.25001922145154\n",
      "[0, 5, 4, 7]\n",
      "Steps done: 3545\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 2: -62.31245268144392\n",
      "[0, 5, 4, 7, 2]\n",
      "Steps done: 3546\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 9: -198.2501336376145\n",
      "[0, 5, 4, 7, 2, 9]\n",
      "Steps done: 3547\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 19: -62.31245268144392\n",
      "[0, 5, 4, 7, 2]\n",
      "Steps done: 3548\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 6: -55.5609178580051\n",
      "[0, 5, 4, 7, 2, 6]\n",
      "Steps done: 3549\n",
      "SV: [-0.00173308 -0.25517818  0.84923744]\n",
      "Reward for action 20: -51.5609178580051\n",
      "[0, 5, 4, 7, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 498\n",
      "Steps done: 3550\n",
      "SV: [0.63299245 0.37711787 1.8608713 ]\n",
      "Reward for action 20: -4.214175032789216\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 499\n",
      "Steps done: 3551\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 4: -47.41531791522593\n",
      "[0, 1, 4]\n",
      "Steps done: 3552\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 8: -70.44992778911991\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 3553\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 3: -47.24987999516124\n",
      "[0, 1, 4, 8, 3]\n",
      "Steps done: 3554\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 7: -55.48307373279068\n",
      "[0, 1, 4, 8, 3, 7]\n",
      "Steps done: 3555\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 17: -47.24987999516124\n",
      "[0, 1, 4, 8, 3]\n",
      "Steps done: 3556\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 2: -40.422863338241484\n",
      "[0, 1, 4, 8, 3, 2]\n",
      "Steps done: 3557\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 7: -48.60428627687892\n",
      "[0, 1, 4, 8, 3, 2, 7]\n",
      "Steps done: 3558\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 17: -40.422863338241484\n",
      "[0, 1, 4, 8, 3, 2]\n",
      "Steps done: 3559\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 14: -27.556663248299287\n",
      "[0, 1, 8, 3, 2]\n",
      "Steps done: 3560\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 9: -29.79485095198207\n",
      "[0, 1, 8, 3, 2, 9]\n",
      "Steps done: 3561\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 19: -27.556663248299287\n",
      "[0, 1, 8, 3, 2]\n",
      "Steps done: 3562\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 7: -42.784026232506626\n",
      "[0, 1, 8, 3, 2, 7]\n",
      "Steps done: 3563\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 11: -45.50525677380764\n",
      "[0, 8, 3, 2, 7]\n",
      "Steps done: 3564\n",
      "SV: [ 0.5040755 -0.5392663  1.1078064]\n",
      "Reward for action 20: -41.50525677380764\n",
      "[0, 8, 3, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 500\n",
      "Steps done: 3565\n",
      "SV: [-0.16403271  0.28495857  1.2688752 ]\n",
      "Reward for action 2: -169.99894587066154\n",
      "[0, 1, 2]\n",
      "Steps done: 3566\n",
      "SV: [-0.16403271  0.28495857  1.2688752 ]\n",
      "Reward for action 11: -229.37810896128693\n",
      "[0, 2]\n",
      "Steps done: 3567\n",
      "SV: [-0.16403271  0.28495857  1.2688752 ]\n",
      "Reward for action 1: -169.99894587066154\n",
      "[0, 2, 1]\n",
      "Steps done: 3568\n",
      "SV: [-0.16403271  0.28495857  1.2688752 ]\n",
      "Reward for action 20: -165.99894587066154\n",
      "[0, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 501\n",
      "Steps done: 3569\n",
      "SV: [ 0.6571902  -0.29514936  0.871483  ]\n",
      "Reward for action 20: -3.281438050823855\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 502\n",
      "Steps done: 3570\n",
      "SV: [-0.18879496  0.4000357  -0.00071754]\n",
      "Reward for action 20: -36.63616103652022\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 503\n",
      "Steps done: 3571\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 8: -73.19767341848298\n",
      "[0, 1, 8]\n",
      "Steps done: 3572\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 11: -123.21367319029306\n",
      "[0, 8]\n",
      "Steps done: 3573\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 1: -73.19767341848298\n",
      "[0, 8, 1]\n",
      "Steps done: 3574\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 9: -42.430195314631526\n",
      "[0, 8, 1, 9]\n",
      "Steps done: 3575\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 11: -81.2960964682143\n",
      "[0, 8, 9]\n",
      "Steps done: 3576\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 7: -35.329169255648885\n",
      "[0, 8, 9, 7]\n",
      "Steps done: 3577\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 3: -31.994041251620505\n",
      "[0, 8, 9, 7, 3]\n",
      "Steps done: 3578\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 2: -30.140171361847315\n",
      "[0, 8, 9, 7, 3, 2]\n",
      "Steps done: 3579\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 6: -34.12387661590741\n",
      "[0, 8, 9, 7, 3, 2, 6]\n",
      "Steps done: 3580\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 19: -30.699738814197346\n",
      "[0, 8, 7, 3, 2, 6]\n",
      "Steps done: 3581\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 5: -32.65702136882605\n",
      "[0, 8, 7, 3, 2, 6, 5]\n",
      "Steps done: 3582\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 4: -28.408330524553055\n",
      "[0, 8, 7, 3, 2, 6, 5, 4]\n",
      "Steps done: 3583\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 17: -28.522407012350943\n",
      "[0, 8, 3, 2, 6, 5, 4]\n",
      "Steps done: 3584\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 1: -28.265724496893114\n",
      "[0, 8, 3, 2, 6, 5, 4, 1]\n",
      "Steps done: 3585\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 14: -29.54356603020515\n",
      "[0, 8, 3, 2, 6, 5, 1]\n",
      "Steps done: 3586\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 18: -46.13802190820455\n",
      "[0, 3, 2, 6, 5, 1]\n",
      "Steps done: 3587\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 10: -79.9008082146326\n",
      "[3, 2, 6, 5, 1]\n",
      "Steps done: 3588\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 11: -105.2998809511567\n",
      "[3, 2, 6, 5]\n",
      "Steps done: 3589\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 8: -32.72654488445245\n",
      "[3, 2, 6, 5, 8]\n",
      "Steps done: 3590\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 4: -27.6226338677244\n",
      "[3, 2, 6, 5, 8, 4]\n",
      "Steps done: 3591\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 7: -30.341117869449974\n",
      "[3, 2, 6, 5, 8, 4, 7]\n",
      "Steps done: 3592\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 12: -30.992071523311544\n",
      "[3, 6, 5, 8, 4, 7]\n",
      "Steps done: 3593\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 1: -31.38762585207529\n",
      "[3, 6, 5, 8, 4, 7, 1]\n",
      "Steps done: 3594\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 11: -30.992071523311544\n",
      "[3, 6, 5, 8, 4, 7]\n",
      "Steps done: 3595\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 13: -32.90847541024533\n",
      "[6, 5, 8, 4, 7]\n",
      "Steps done: 3596\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Reward for action 3: -30.992071523311544\n",
      "[6, 5, 8, 4, 7, 3]\n",
      "Steps done: 3597\n",
      "SV: [-0.11544283 -0.10901499 -0.63878685]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -26.992071523311544\n",
      "[6, 5, 8, 4, 7, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 504\n",
      "Steps done: 3598\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 3: -40.67951861071913\n",
      "[0, 1, 3]\n",
      "Steps done: 3599\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 13: -218.87822058053135\n",
      "[0, 1]\n",
      "Did target update\n",
      "Steps done: 3600\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 2: -167.77604173682377\n",
      "[0, 1, 2]\n",
      "Steps done: 3601\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 5: -34.185336463705326\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 3602\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 10: -0.5000312080791305\n",
      "[1, 2, 5]\n",
      "Steps done: 3603\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 4: -1.0755002144835744\n",
      "[1, 2, 5, 4]\n",
      "Steps done: 3604\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 11: -0.5545021658190462\n",
      "[2, 5, 4]\n",
      "Steps done: 3605\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 12: -105.30556480516177\n",
      "[5, 4]\n",
      "Steps done: 3606\n",
      "SV: [ 1.2403445 -0.5292673 -0.8277263]\n",
      "Reward for action 20: -101.30556480516177\n",
      "[5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 505\n",
      "Steps done: 3607\n",
      "SV: [-0.05302712  0.2048754   0.5181801 ]\n",
      "Reward for action 20: -24.081703861866128\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 506\n",
      "Steps done: 3608\n",
      "SV: [ 0.2336582  -0.27152044  0.03551529]\n",
      "Reward for action 8: -36.504175121944236\n",
      "[0, 1, 8]\n",
      "Steps done: 3609\n",
      "SV: [ 0.2336582  -0.27152044  0.03551529]\n",
      "Reward for action 9: -23.469414303451398\n",
      "[0, 1, 8, 9]\n",
      "Steps done: 3610\n",
      "SV: [ 0.2336582  -0.27152044  0.03551529]\n",
      "Reward for action 5: -15.41326450888494\n",
      "[0, 1, 8, 9, 5]\n",
      "Steps done: 3611\n",
      "SV: [ 0.2336582  -0.27152044  0.03551529]\n",
      "Reward for action 20: -11.41326450888494\n",
      "[0, 1, 8, 9, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 507\n",
      "Steps done: 3612\n",
      "SV: [-0.9376096  -0.25410357  0.9604482 ]\n",
      "Reward for action 7: -26.461330563219825\n",
      "[0, 1, 7]\n",
      "Steps done: 3613\n",
      "SV: [-0.9376096  -0.25410357  0.9604482 ]\n",
      "Reward for action 10: -2.8592624019700907\n",
      "[1, 7]\n",
      "Steps done: 3614\n",
      "SV: [-0.9376096  -0.25410357  0.9604482 ]\n",
      "Reward for action 20: 1.1407375980299093\n",
      "[1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 508\n",
      "Steps done: 3615\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 4: -45.17612634734829\n",
      "[0, 1, 4]\n",
      "Steps done: 3616\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 10: -226.59435044961572\n",
      "[1, 4]\n",
      "Steps done: 3617\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 2: -75.71246367357469\n",
      "[1, 4, 2]\n",
      "Steps done: 3618\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 11: -50.815737369581164\n",
      "[4, 2]\n",
      "Steps done: 3619\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 0: -43.587164650769786\n",
      "[4, 2, 0]\n",
      "Steps done: 3620\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 12: -31.600796852289214\n",
      "[4, 0]\n",
      "Steps done: 3621\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 3: -39.05915370538166\n",
      "[4, 0, 3]\n",
      "Steps done: 3622\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 2: -38.080597710967446\n",
      "[4, 0, 3, 2]\n",
      "Steps done: 3623\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 13: -43.587164650769786\n",
      "[4, 0, 2]\n",
      "Steps done: 3624\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 3: -38.08059771096744\n",
      "[4, 0, 2, 3]\n",
      "Steps done: 3625\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 13: -43.587164650769786\n",
      "[4, 0, 2]\n",
      "Steps done: 3626\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 1: -41.30049573179453\n",
      "[4, 0, 2, 1]\n",
      "Steps done: 3627\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 10: -75.71246367357469\n",
      "[4, 2, 1]\n",
      "Steps done: 3628\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 11: -50.815737369581164\n",
      "[4, 2]\n",
      "Steps done: 3629\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 1: -75.71246367357469\n",
      "[4, 2, 1]\n",
      "Steps done: 3630\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 12: -226.59435044961572\n",
      "[4, 1]\n",
      "Steps done: 3631\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 2: -75.71246367357469\n",
      "[4, 1, 2]\n",
      "Steps done: 3632\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 11: -50.815737369581164\n",
      "[4, 2]\n",
      "Steps done: 3633\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 1: -75.71246367357469\n",
      "[4, 2, 1]\n",
      "Steps done: 3634\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 0: -41.30049573179452\n",
      "[4, 2, 1, 0]\n",
      "Steps done: 3635\n",
      "SV: [ 0.15903653 -0.22190225  0.03283717]\n",
      "Reward for action 20: -37.30049573179452\n",
      "[4, 2, 1, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 509\n",
      "Steps done: 3636\n",
      "SV: [ 0.02715228 -0.00502385 -0.3530395 ]\n",
      "Reward for action 4: -0.4885973107657355\n",
      "[0, 1, 4]\n",
      "Steps done: 3637\n",
      "SV: [ 0.02715228 -0.00502385 -0.3530395 ]\n",
      "Reward for action 20: 3.5114026892342647\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 510\n",
      "Steps done: 3638\n",
      "SV: [-0.53376746  0.1530579   0.02043362]\n",
      "Reward for action 3: -16.926264842683747\n",
      "[0, 1, 3]\n",
      "Steps done: 3639\n",
      "SV: [-0.53376746  0.1530579   0.02043362]\n",
      "Reward for action 20: -12.926264842683747\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 511\n",
      "Steps done: 3640\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 5: -13.490579588863465\n",
      "[0, 1, 5]\n",
      "Steps done: 3641\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 2: -13.31711775308039\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 3642\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 10: -15.122227103543413\n",
      "[1, 5, 2]\n",
      "Steps done: 3643\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 4: -11.527494413160825\n",
      "[1, 5, 2, 4]\n",
      "Steps done: 3644\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 15: -12.964858756200194\n",
      "[1, 2, 4]\n",
      "Steps done: 3645\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 11: -31.55808892956962\n",
      "[2, 4]\n",
      "Steps done: 3646\n",
      "SV: [-0.04038646  0.12870769  0.3924519 ]\n",
      "Reward for action 20: -27.55808892956962\n",
      "[2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 512\n",
      "Steps done: 3647\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 2: -40.33250783706802\n",
      "[0, 1, 2]\n",
      "Steps done: 3648\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 3: -46.52395768699296\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 3649\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 8: -37.11333736739917\n",
      "[0, 1, 2, 3, 8]\n",
      "Steps done: 3650\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 7: -34.62769835630866\n",
      "[0, 1, 2, 3, 8, 7]\n",
      "Steps done: 3651\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 9: -32.17829082973038\n",
      "[0, 1, 2, 3, 8, 7, 9]\n",
      "Steps done: 3652\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 11: -23.620477640957773\n",
      "[0, 2, 3, 8, 7, 9]\n",
      "Steps done: 3653\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 10: -24.525300567948467\n",
      "[2, 3, 8, 7, 9]\n",
      "Steps done: 3654\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 0: -23.620477640957773\n",
      "[2, 3, 8, 7, 9, 0]\n",
      "Steps done: 3655\n",
      "SV: [ 0.24684463  0.06560905 -0.6642083 ]\n",
      "Reward for action 20: -19.620477640957773\n",
      "[2, 3, 8, 7, 9, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 513\n",
      "Steps done: 3656\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 5: -98.45473418281557\n",
      "[0, 1, 5]\n",
      "Steps done: 3657\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 4: -0.7911950605792569\n",
      "[0, 1, 5, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3658\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 10: -6.318545034620618\n",
      "[1, 5, 4]\n",
      "Steps done: 3659\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 0: -0.7911950605792583\n",
      "[1, 5, 4, 0]\n",
      "Steps done: 3660\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 3: -39.14658703604451\n",
      "[1, 5, 4, 0, 3]\n",
      "Steps done: 3661\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 13: -0.7911950605792583\n",
      "[1, 5, 4, 0]\n",
      "Steps done: 3662\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 11: -5.763015565949514\n",
      "[5, 4, 0]\n",
      "Steps done: 3663\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 2: -11.933000399994256\n",
      "[5, 4, 0, 2]\n",
      "Steps done: 3664\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 12: -5.763015565949514\n",
      "[5, 4, 0]\n",
      "Steps done: 3665\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 10: -30.492747232838404\n",
      "[5, 4]\n",
      "Steps done: 3666\n",
      "SV: [ 0.51307577 -1.1920222   1.4735241 ]\n",
      "Reward for action 20: -26.492747232838404\n",
      "[5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 514\n",
      "Steps done: 3667\n",
      "SV: [-1.2246522   0.29245862 -2.664092  ]\n",
      "Reward for action 20: -117.40332268587065\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 515\n",
      "Steps done: 3668\n",
      "SV: [-0.07119541 -0.21131377 -0.583353  ]\n",
      "Reward for action 4: -29.191598724180825\n",
      "[0, 1, 4]\n",
      "Steps done: 3669\n",
      "SV: [-0.07119541 -0.21131377 -0.583353  ]\n",
      "Reward for action 10: -28.06975866745459\n",
      "[1, 4]\n",
      "Steps done: 3670\n",
      "SV: [-0.07119541 -0.21131377 -0.583353  ]\n",
      "Reward for action 3: -27.157229119338176\n",
      "[1, 4, 3]\n",
      "Steps done: 3671\n",
      "SV: [-0.07119541 -0.21131377 -0.583353  ]\n",
      "Reward for action 20: -23.157229119338176\n",
      "[1, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 516\n",
      "Steps done: 3672\n",
      "SV: [ 0.4747513  2.0265207 -5.035301 ]\n",
      "Reward for action 20: -671.5710134843921\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 517\n",
      "Steps done: 3673\n",
      "SV: [-1.023012    0.01338681  0.82985175]\n",
      "Reward for action 2: -23.544859718265776\n",
      "[0, 1, 2]\n",
      "Steps done: 3674\n",
      "SV: [-1.023012    0.01338681  0.82985175]\n",
      "Reward for action 11: -5.8522703579428\n",
      "[0, 2]\n",
      "Steps done: 3675\n",
      "SV: [-1.023012    0.01338681  0.82985175]\n",
      "Reward for action 20: -1.8522703579427997\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 518\n",
      "Steps done: 3676\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 3: -59.176929879872375\n",
      "[0, 1, 3]\n",
      "Steps done: 3677\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 7: -56.35442270898981\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 3678\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 2: -57.21531209291564\n",
      "[0, 1, 3, 7, 2]\n",
      "Steps done: 3679\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 17: -58.38335621866187\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 3680\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 8: -58.68050118097092\n",
      "[0, 1, 3, 2, 8]\n",
      "Steps done: 3681\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 12: -55.796600874793256\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 3682\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 9: -56.23420297878962\n",
      "[0, 1, 3, 8, 9]\n",
      "Steps done: 3683\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 18: -56.88659327400442\n",
      "[0, 1, 3, 9]\n",
      "Steps done: 3684\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 2: -57.89056772878965\n",
      "[0, 1, 3, 9, 2]\n",
      "Steps done: 3685\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 19: -58.38335621866187\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 3686\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 4: -57.611135773250275\n",
      "[0, 1, 3, 2, 4]\n",
      "Steps done: 3687\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 8: -57.4606789738697\n",
      "[0, 1, 3, 2, 4, 8]\n",
      "Steps done: 3688\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 11: -67.00991114977327\n",
      "[0, 3, 2, 4, 8]\n",
      "Steps done: 3689\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 13: -58.21838135799497\n",
      "[0, 2, 4, 8]\n",
      "Steps done: 3690\n",
      "SV: [ 0.3889983  -0.31322932 -0.682393  ]\n",
      "Reward for action 20: -54.21838135799497\n",
      "[0, 2, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 519\n",
      "Steps done: 3691\n",
      "SV: [ 0.21106927 -0.39147484  0.51961106]\n",
      "Reward for action 5: -66.44328739346598\n",
      "[0, 1, 5]\n",
      "Steps done: 3692\n",
      "SV: [ 0.21106927 -0.39147484  0.51961106]\n",
      "Reward for action 3: -29.951204589498225\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 3693\n",
      "SV: [ 0.21106927 -0.39147484  0.51961106]\n",
      "Reward for action 4: -37.61458620299291\n",
      "[0, 1, 5, 3, 4]\n",
      "Steps done: 3694\n",
      "SV: [ 0.21106927 -0.39147484  0.51961106]\n",
      "Reward for action 10: -48.66946517744208\n",
      "[1, 5, 3, 4]\n",
      "Steps done: 3695\n",
      "SV: [ 0.21106927 -0.39147484  0.51961106]\n",
      "Reward for action 20: -44.66946517744208\n",
      "[1, 5, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 520\n",
      "Steps done: 3696\n",
      "SV: [-1.0094334  -1.6191195  -0.77686214]\n",
      "Reward for action 20: -2.041721225277252\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 521\n",
      "Steps done: 3697\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 4: -0.4169936178682395\n",
      "[0, 1, 4]\n",
      "Steps done: 3698\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 6: -0.6269244250037564\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 3699\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 14: -0.9496073636673348\n",
      "[0, 1, 6]\n",
      "Did target update\n",
      "Steps done: 3700\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 11: -0.3309581987698713\n",
      "[0, 6]\n",
      "Steps done: 3701\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 1: -0.9496073636673348\n",
      "[0, 6, 1]\n",
      "Steps done: 3702\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 11: -0.3309581987698713\n",
      "[0, 6]\n",
      "Steps done: 3703\n",
      "SV: [ 0.16489117 -0.07601148 -0.2353405 ]\n",
      "Reward for action 20: 3.6690418012301285\n",
      "[0, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 522\n",
      "Steps done: 3704\n",
      "SV: [ 1.0055944 -1.6166943 -2.5973227]\n",
      "Reward for action 2: -370.8321292895048\n",
      "[0, 1, 2]\n",
      "Steps done: 3705\n",
      "SV: [ 1.0055944 -1.6166943 -2.5973227]\n",
      "Reward for action 11: -428.6913012391725\n",
      "[0, 2]\n",
      "Steps done: 3706\n",
      "SV: [ 1.0055944 -1.6166943 -2.5973227]\n",
      "Reward for action 3: -418.70466055968495\n",
      "[0, 2, 3]\n",
      "Steps done: 3707\n",
      "SV: [ 1.0055944 -1.6166943 -2.5973227]\n",
      "Reward for action 20: -414.70466055968495\n",
      "[0, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 523\n",
      "Steps done: 3708\n",
      "SV: [ 0.2597557  -0.15183814 -1.587924  ]\n",
      "Reward for action 20: -216.94657561377073\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 524\n",
      "Steps done: 3709\n",
      "SV: [-0.5635425   0.88411075  3.986988  ]\n",
      "Reward for action 3: -585.5573439259879\n",
      "[0, 1, 3]\n",
      "Steps done: 3710\n",
      "SV: [-0.5635425   0.88411075  3.986988  ]\n",
      "Reward for action 13: -399.7507695644767\n",
      "[0, 1]\n",
      "Steps done: 3711\n",
      "SV: [-0.5635425   0.88411075  3.986988  ]\n",
      "Reward for action 3: -585.5573439259879\n",
      "[0, 1, 3]\n",
      "Steps done: 3712\n",
      "SV: [-0.5635425   0.88411075  3.986988  ]\n",
      "Reward for action 11: -691.0459915175643\n",
      "[0, 3]\n",
      "Steps done: 3713\n",
      "SV: [-0.5635425   0.88411075  3.986988  ]\n",
      "Reward for action 20: -687.0459915175643\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 525\n",
      "Steps done: 3714\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 5: -533.7339005103926\n",
      "[0, 1, 5]\n",
      "Steps done: 3715\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 7: -654.0590465386452\n",
      "[0, 1, 5, 7]\n",
      "Steps done: 3716\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 11: -785.0832679634755\n",
      "[0, 5, 7]\n",
      "Steps done: 3717\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 3: -484.5040617013232\n",
      "[0, 5, 7, 3]\n",
      "Steps done: 3718\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 2: -450.2957061446036\n",
      "[0, 5, 7, 3, 2]\n",
      "Steps done: 3719\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 13: -652.3531678081812\n",
      "[0, 5, 7, 2]\n",
      "Steps done: 3720\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 6: -445.1158289916579\n",
      "[0, 5, 7, 2, 6]\n",
      "Steps done: 3721\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 16: -652.3531678081812\n",
      "[0, 5, 7, 2]\n",
      "Steps done: 3722\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 8: -543.4011950735633\n",
      "[0, 5, 7, 2, 8]\n",
      "Steps done: 3723\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 17: -401.6557535394618\n",
      "[0, 5, 2, 8]\n",
      "Steps done: 3724\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 6: -261.81197158458986\n",
      "[0, 5, 2, 8, 6]\n",
      "Steps done: 3725\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 3: -203.98833316918706\n",
      "[0, 5, 2, 8, 6, 3]\n",
      "Steps done: 3726\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 12: -185.8694514511562\n",
      "[0, 5, 8, 6, 3]\n",
      "Steps done: 3727\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 13: -250.22326863893312\n",
      "[0, 5, 8, 6]\n",
      "Steps done: 3728\n",
      "SV: [ 0.03938491  0.00648998 -1.3136637 ]\n",
      "Reward for action 20: -246.22326863893312\n",
      "[0, 5, 8, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 526\n",
      "Steps done: 3729\n",
      "SV: [-0.8616894 -1.0274233 -1.2852849]\n",
      "Reward for action 2: -36.727225381860606\n",
      "[0, 1, 2]\n",
      "Steps done: 3730\n",
      "SV: [-0.8616894 -1.0274233 -1.2852849]\n",
      "Reward for action 10: -200.33695352893886\n",
      "[1, 2]\n",
      "Steps done: 3731\n",
      "SV: [-0.8616894 -1.0274233 -1.2852849]\n",
      "Reward for action 0: -36.727225381860606\n",
      "[1, 2, 0]\n",
      "Steps done: 3732\n",
      "SV: [-0.8616894 -1.0274233 -1.2852849]\n",
      "Reward for action 20: -32.727225381860606\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 527\n",
      "Steps done: 3733\n",
      "SV: [-0.42813182 -0.04965101 -0.30398747]\n",
      "Reward for action 5: -17.697629627235383\n",
      "[0, 1, 5]\n",
      "Steps done: 3734\n",
      "SV: [-0.42813182 -0.04965101 -0.30398747]\n",
      "Reward for action 15: -25.04352289928282\n",
      "[0, 1]\n",
      "Steps done: 3735\n",
      "SV: [-0.42813182 -0.04965101 -0.30398747]\n",
      "Reward for action 20: -21.04352289928282\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 528\n",
      "Steps done: 3736\n",
      "SV: [ 0.12831941 -0.25748143 -1.290097  ]\n",
      "Reward for action 20: -134.43675819657832\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 529\n",
      "Steps done: 3737\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 6: -293.6147171329687\n",
      "[0, 1, 6]\n",
      "Steps done: 3738\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 2: -125.60497086072633\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 3739\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 4: -147.9537345167774\n",
      "[0, 1, 6, 2, 4]\n",
      "Steps done: 3740\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 5: -149.250769826113\n",
      "[0, 1, 6, 2, 4, 5]\n",
      "Steps done: 3741\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 12: -235.36870955334803\n",
      "[0, 1, 6, 4, 5]\n",
      "Steps done: 3742\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 11: -192.90361564365847\n",
      "[0, 6, 4, 5]\n",
      "Steps done: 3743\n",
      "SV: [0.06410909 0.59996563 0.61547345]\n",
      "Reward for action 20: -188.90361564365847\n",
      "[0, 6, 4, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 530\n",
      "Steps done: 3744\n",
      "SV: [-0.14201637 -0.20013474 -0.7737544 ]\n",
      "Reward for action 3: -827.2757802173871\n",
      "[0, 1, 3]\n",
      "Steps done: 3745\n",
      "SV: [-0.14201637 -0.20013474 -0.7737544 ]\n",
      "Reward for action 6: -104.82257600873054\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 3746\n",
      "SV: [-0.14201637 -0.20013474 -0.7737544 ]\n",
      "Reward for action 13: -1.9637548672232141\n",
      "[0, 1, 6]\n",
      "Steps done: 3747\n",
      "SV: [-0.14201637 -0.20013474 -0.7737544 ]\n",
      "Reward for action 2: -17.50269121531416\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 3748\n",
      "SV: [-0.14201637 -0.20013474 -0.7737544 ]\n",
      "Reward for action 20: -13.502691215314162\n",
      "[0, 1, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 531\n",
      "Steps done: 3749\n",
      "SV: [-0.42144376 -0.16916887 -0.3486818 ]\n",
      "Reward for action 6: -68.92195733976743\n",
      "[0, 1, 6]\n",
      "Steps done: 3750\n",
      "SV: [-0.42144376 -0.16916887 -0.3486818 ]\n",
      "Reward for action 20: -64.92195733976743\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 532\n",
      "Steps done: 3751\n",
      "SV: [-0.4311063  -0.36454433  0.62404865]\n",
      "Reward for action 20: -48.27797240240985\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 533\n",
      "Steps done: 3752\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 2: -35.54774348368332\n",
      "[0, 1, 2]\n",
      "Steps done: 3753\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 4: -14.438117883820539\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 3754\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 10: -7.596486667404206\n",
      "[1, 2, 4]\n",
      "Steps done: 3755\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 12: -50.96354112875311\n",
      "[1, 4]\n",
      "Steps done: 3756\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 2: -7.596486667404209\n",
      "[1, 4, 2]\n",
      "Steps done: 3757\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 14: -120.95142203776142\n",
      "[1, 2]\n",
      "Steps done: 3758\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 3: -9.054007716756821\n",
      "[1, 2, 3]\n",
      "Steps done: 3759\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 13: -120.95142203776142\n",
      "[1, 2]\n",
      "Steps done: 3760\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 3: -9.054007716756821\n",
      "[1, 2, 3]\n",
      "Steps done: 3761\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 13: -120.95142203776142\n",
      "[1, 2]\n",
      "Steps done: 3762\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 4: -7.596486667404206\n",
      "[1, 2, 4]\n",
      "Steps done: 3763\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 0: -14.438117883820532\n",
      "[1, 2, 4, 0]\n",
      "Steps done: 3764\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 14: -35.54774348368331\n",
      "[1, 2, 0]\n",
      "Steps done: 3765\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 11: -13.398354360280686\n",
      "[2, 0]\n",
      "Steps done: 3766\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 5: -19.499922183431178\n",
      "[2, 0, 5]\n",
      "Steps done: 3767\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 4: -10.933236305352212\n",
      "[2, 0, 5, 4]\n",
      "Steps done: 3768\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 3: -7.864230545877389\n",
      "[2, 0, 5, 4, 3]\n",
      "Steps done: 3769\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 12: -16.12706252640383\n",
      "[0, 5, 4, 3]\n",
      "Steps done: 3770\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 2: -7.8642305458773905\n",
      "[0, 5, 4, 3, 2]\n",
      "Steps done: 3771\n",
      "SV: [ 0.63572484  0.43035704 -0.31169954]\n",
      "Reward for action 20: -3.8642305458773905\n",
      "[0, 5, 4, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 534\n",
      "Steps done: 3772\n",
      "SV: [ 0.06704333 -0.04898277 -0.31646076]\n",
      "Reward for action 20: -5.740457048579762\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 535\n",
      "Steps done: 3773\n",
      "SV: [ 1.1786999   0.26077995 -4.035174  ]\n",
      "Reward for action 20: -686.6583859836942\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 536\n",
      "Steps done: 3774\n",
      "SV: [ 0.06795178 -0.07616625  0.65287644]\n",
      "Reward for action 2: -22.180590317832493\n",
      "[0, 1, 2]\n",
      "Steps done: 3775\n",
      "SV: [ 0.06795178 -0.07616625  0.65287644]\n",
      "Reward for action 5: -51.16666042631335\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 3776\n",
      "SV: [ 0.06795178 -0.07616625  0.65287644]\n",
      "Reward for action 15: -22.180590317832493\n",
      "[0, 1, 2]\n",
      "Steps done: 3777\n",
      "SV: [ 0.06795178 -0.07616625  0.65287644]\n",
      "Reward for action 10: -26.61236626504058\n",
      "[1, 2]\n",
      "Steps done: 3778\n",
      "SV: [ 0.06795178 -0.07616625  0.65287644]\n",
      "Reward for action 20: -22.61236626504058\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 537\n",
      "Steps done: 3779\n",
      "SV: [-0.14495851  0.18433732  0.18571874]\n",
      "Reward for action 6: -36.14228490987602\n",
      "[0, 1, 6]\n",
      "Steps done: 3780\n",
      "SV: [-0.14495851  0.18433732  0.18571874]\n",
      "Reward for action 3: -47.425211436505194\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 3781\n",
      "SV: [-0.14495851  0.18433732  0.18571874]\n",
      "Reward for action 13: -36.14228490987602\n",
      "[0, 1, 6]\n",
      "Steps done: 3782\n",
      "SV: [-0.14495851  0.18433732  0.18571874]\n",
      "Reward for action 20: -32.14228490987602\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 538\n",
      "Steps done: 3783\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 6: -286.9885891815908\n",
      "[0, 1, 6]\n",
      "Steps done: 3784\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 10: -335.70174647733785\n",
      "[1, 6]\n",
      "Steps done: 3785\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 2: -346.6195903612996\n",
      "[1, 6, 2]\n",
      "Steps done: 3786\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 0: -241.93801019300827\n",
      "[1, 6, 2, 0]\n",
      "Steps done: 3787\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 16: -29.779221423342893\n",
      "[1, 2, 0]\n",
      "Steps done: 3788\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 10: -66.33283602923164\n",
      "[1, 2]\n",
      "Steps done: 3789\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 4: -86.20053208663965\n",
      "[1, 2, 4]\n",
      "Steps done: 3790\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 5: -84.39775437199252\n",
      "[1, 2, 4, 5]\n",
      "Steps done: 3791\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 15: -86.20053208663965\n",
      "[1, 2, 4]\n",
      "Steps done: 3792\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 6: -307.9026153430391\n",
      "[1, 2, 4, 6]\n",
      "Steps done: 3793\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 16: -86.20053208663965\n",
      "[1, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3794\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 11: -275.3200871188373\n",
      "[2, 4]\n",
      "Steps done: 3795\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 0: -70.53586502797644\n",
      "[2, 4, 0]\n",
      "Steps done: 3796\n",
      "SV: [ 0.08365057 -0.00747798 -0.8364133 ]\n",
      "Reward for action 20: -66.53586502797644\n",
      "[2, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 539\n",
      "Steps done: 3797\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 8: -72.82664874793349\n",
      "[0, 1, 8]\n",
      "Steps done: 3798\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 11: -68.38146655308944\n",
      "[0, 8]\n",
      "Steps done: 3799\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 5: -15.8884915508921\n",
      "[0, 8, 5]\n",
      "Did target update\n",
      "Steps done: 3800\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 18: -36.842952762905725\n",
      "[0, 5]\n",
      "Steps done: 3801\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 9: -7.48323735498529\n",
      "[0, 5, 9]\n",
      "Steps done: 3802\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 7: -7.091593152584644\n",
      "[0, 5, 9, 7]\n",
      "Steps done: 3803\n",
      "SV: [0.07878581 0.14828327 0.1977037 ]\n",
      "Reward for action 20: -3.091593152584644\n",
      "[0, 5, 9, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 540\n",
      "Steps done: 3804\n",
      "SV: [-0.1139704   0.2098753   0.00072711]\n",
      "Reward for action 3: -13.930476729647992\n",
      "[0, 1, 3]\n",
      "Steps done: 3805\n",
      "SV: [-0.1139704   0.2098753   0.00072711]\n",
      "Reward for action 10: -6.810429570819468\n",
      "[1, 3]\n",
      "Steps done: 3806\n",
      "SV: [-0.1139704   0.2098753   0.00072711]\n",
      "Reward for action 20: -2.810429570819468\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 541\n",
      "Steps done: 3807\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 4: -6.951309675177088\n",
      "[0, 1, 4]\n",
      "Steps done: 3808\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 10: -41.06612165294893\n",
      "[1, 4]\n",
      "Steps done: 3809\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 5: -5.267237554283411\n",
      "[1, 4, 5]\n",
      "Steps done: 3810\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 11: -5.341942344345279\n",
      "[4, 5]\n",
      "Steps done: 3811\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 6: -4.007051293534145\n",
      "[4, 5, 6]\n",
      "Steps done: 3812\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 16: -5.341942344345279\n",
      "[4, 5]\n",
      "Steps done: 3813\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 6: -4.007051293534145\n",
      "[4, 5, 6]\n",
      "Steps done: 3814\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 16: -5.341942344345279\n",
      "[4, 5]\n",
      "Steps done: 3815\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 3: -6.186642019936945\n",
      "[4, 5, 3]\n",
      "Steps done: 3816\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 1: -9.354877593533631\n",
      "[4, 5, 3, 1]\n",
      "Steps done: 3817\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 6: -10.729229340252942\n",
      "[4, 5, 3, 1, 6]\n",
      "Steps done: 3818\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 16: -9.354877593533631\n",
      "[4, 5, 3, 1]\n",
      "Steps done: 3819\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 15: -40.000661301372155\n",
      "[4, 3, 1]\n",
      "Steps done: 3820\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 5: -9.354877593533631\n",
      "[4, 3, 1, 5]\n",
      "Steps done: 3821\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 2: -24.69801950566345\n",
      "[4, 3, 1, 5, 2]\n",
      "Steps done: 3822\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 13: -26.02285234068731\n",
      "[4, 1, 5, 2]\n",
      "Steps done: 3823\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 0: -3.744364922261371\n",
      "[4, 1, 5, 2, 0]\n",
      "Steps done: 3824\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 3: -4.682803242526921\n",
      "[4, 1, 5, 2, 0, 3]\n",
      "Steps done: 3825\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 11: -3.975093666989547\n",
      "[4, 5, 2, 0, 3]\n",
      "Steps done: 3826\n",
      "SV: [-0.28135034 -0.63923854  0.03089544]\n",
      "Reward for action 20: 0.024906333010453086\n",
      "[4, 5, 2, 0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 542\n",
      "Steps done: 3827\n",
      "SV: [0.00436498 0.03103978 0.43451473]\n",
      "Reward for action 5: -136.74056081629692\n",
      "[0, 1, 5]\n",
      "Steps done: 3828\n",
      "SV: [0.00436498 0.03103978 0.43451473]\n",
      "Reward for action 20: -132.74056081629692\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 543\n",
      "Steps done: 3829\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 8: -246.5028031334071\n",
      "[0, 1, 8]\n",
      "Steps done: 3830\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 11: -827.3658164200632\n",
      "[0, 8]\n",
      "Steps done: 3831\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 6: -6.459694667568032\n",
      "[0, 8, 6]\n",
      "Steps done: 3832\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 5: -179.72297943532064\n",
      "[0, 8, 6, 5]\n",
      "Steps done: 3833\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 15: -6.459694667568032\n",
      "[0, 8, 6]\n",
      "Steps done: 3834\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 10: -7.197287322262127\n",
      "[8, 6]\n",
      "Steps done: 3835\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 2: -20.166558381658124\n",
      "[8, 6, 2]\n",
      "Steps done: 3836\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 9: -21.591432603420735\n",
      "[8, 6, 2, 9]\n",
      "Steps done: 3837\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 19: -20.166558381658124\n",
      "[8, 6, 2]\n",
      "Steps done: 3838\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 18: -175.33509906615026\n",
      "[6, 2]\n",
      "Steps done: 3839\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 8: -20.166558381658124\n",
      "[6, 2, 8]\n",
      "Steps done: 3840\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 0: -19.434007853121557\n",
      "[6, 2, 8, 0]\n",
      "Steps done: 3841\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 9: -8.308260477232963\n",
      "[6, 2, 8, 0, 9]\n",
      "Steps done: 3842\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 18: -67.43761704762778\n",
      "[6, 2, 0, 9]\n",
      "Steps done: 3843\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 8: -8.308260477232961\n",
      "[6, 2, 0, 9, 8]\n",
      "Steps done: 3844\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 3: -61.43437139530501\n",
      "[6, 2, 0, 9, 8, 3]\n",
      "Steps done: 3845\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 16: -29.8382396836669\n",
      "[2, 0, 9, 8, 3]\n",
      "Steps done: 3846\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 1: -11.331888747990774\n",
      "[2, 0, 9, 8, 3, 1]\n",
      "Steps done: 3847\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 11: -29.8382396836669\n",
      "[2, 0, 9, 8, 3]\n",
      "Steps done: 3848\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 13: -70.19672141461128\n",
      "[2, 0, 9, 8]\n",
      "Steps done: 3849\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 19: -150.05281169606593\n",
      "[2, 0, 8]\n",
      "Steps done: 3850\n",
      "SV: [-0.10367812  0.2976656   0.25440904]\n",
      "Reward for action 20: -146.05281169606593\n",
      "[2, 0, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 544\n",
      "Steps done: 3851\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 2: -63.23251798601608\n",
      "[0, 1, 2]\n",
      "Steps done: 3852\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 3: -44.593589587539256\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 3853\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 13: -63.23251798601608\n",
      "[0, 1, 2]\n",
      "Steps done: 3854\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 12: -50.102047264179\n",
      "[0, 1]\n",
      "Steps done: 3855\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 3: -39.409928277844\n",
      "[0, 1, 3]\n",
      "Steps done: 3856\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 13: -50.102047264179\n",
      "[0, 1]\n",
      "Steps done: 3857\n",
      "SV: [0.5016877  0.59531206 0.46299472]\n",
      "Reward for action 20: -46.102047264179\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 545\n",
      "Steps done: 3858\n",
      "SV: [-0.09513487  0.35749242 -1.1740859 ]\n",
      "Reward for action 20: -181.6230837853127\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 546\n",
      "Steps done: 3859\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 2: -76.23530078994548\n",
      "[0, 1, 2]\n",
      "Steps done: 3860\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 4: -16.636778164219397\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 3861\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 12: -21.455007191905203\n",
      "[0, 1, 4]\n",
      "Steps done: 3862\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 6: -6.658105454197942\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 3863\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 2: -4.837263832559102\n",
      "[0, 1, 4, 6, 2]\n",
      "Steps done: 3864\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 14: -16.381878508717044\n",
      "[0, 1, 6, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3865\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 12: -6.5349513541756785\n",
      "[0, 1, 6]\n",
      "Steps done: 3866\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 5: -8.77630114247118\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 3867\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 11: -3.7865521971598044\n",
      "[0, 6, 5]\n",
      "Steps done: 3868\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 10: -2.0656062001402127\n",
      "[6, 5]\n",
      "Steps done: 3869\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 2: -10.205926316158761\n",
      "[6, 5, 2]\n",
      "Steps done: 3870\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 1: -15.079171488330417\n",
      "[6, 5, 2, 1]\n",
      "Steps done: 3871\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 15: -2.1321090964036045\n",
      "[6, 2, 1]\n",
      "Steps done: 3872\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 5: -15.079171488330418\n",
      "[6, 2, 1, 5]\n",
      "Steps done: 3873\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 11: -10.205926316158761\n",
      "[6, 2, 5]\n",
      "Steps done: 3874\n",
      "SV: [ 0.36142382 -0.19913207 -0.26641676]\n",
      "Reward for action 20: -6.205926316158761\n",
      "[6, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 547\n",
      "Steps done: 3875\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 6: -0.4196593572785952\n",
      "[0, 1, 6]\n",
      "Steps done: 3876\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 16: -0.843037323768199\n",
      "[0, 1]\n",
      "Steps done: 3877\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 4: -0.00268195535928789\n",
      "[0, 1, 4]\n",
      "Steps done: 3878\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 11: -4.799616888626845\n",
      "[0, 4]\n",
      "Steps done: 3879\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 2: -0.41163529418205286\n",
      "[0, 4, 2]\n",
      "Steps done: 3880\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 10: -3.113819259605573\n",
      "[4, 2]\n",
      "Steps done: 3881\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 1: -2.152775481248144\n",
      "[4, 2, 1]\n",
      "Steps done: 3882\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 6: -1.2495505690644744\n",
      "[4, 2, 1, 6]\n",
      "Steps done: 3883\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 3: -0.16790225888906077\n",
      "[4, 2, 1, 6, 3]\n",
      "Steps done: 3884\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 0: -0.06537529842267006\n",
      "[4, 2, 1, 6, 3, 0]\n",
      "Steps done: 3885\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 11: -0.06631168471884742\n",
      "[4, 2, 6, 3, 0]\n",
      "Steps done: 3886\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 7: -0.009869104672001782\n",
      "[4, 2, 6, 3, 0, 7]\n",
      "Steps done: 3887\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 10: -0.037887265027414026\n",
      "[4, 2, 6, 3, 7]\n",
      "Steps done: 3888\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 12: -0.6232343625727857\n",
      "[4, 6, 3, 7]\n",
      "Steps done: 3889\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 14: -0.04352515413976799\n",
      "[6, 3, 7]\n",
      "Steps done: 3890\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 13: -0.10790841331611059\n",
      "[6, 7]\n",
      "Steps done: 3891\n",
      "SV: [ 0.07318624  0.10345594 -0.06591877]\n",
      "Reward for action 20: 3.892091586683889\n",
      "[6, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 548\n",
      "Steps done: 3892\n",
      "SV: [-0.46310768 -0.49177346  0.8114657 ]\n",
      "Reward for action 2: -39.24005425723516\n",
      "[0, 1, 2]\n",
      "Steps done: 3893\n",
      "SV: [-0.46310768 -0.49177346  0.8114657 ]\n",
      "Reward for action 10: -79.0363346023453\n",
      "[1, 2]\n",
      "Steps done: 3894\n",
      "SV: [-0.46310768 -0.49177346  0.8114657 ]\n",
      "Reward for action 20: -75.0363346023453\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 549\n",
      "Steps done: 3895\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 6: -675.4963461660403\n",
      "[0, 1, 6]\n",
      "Steps done: 3896\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 10: -833.656793665032\n",
      "[1, 6]\n",
      "Steps done: 3897\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 8: -127.82393471402827\n",
      "[1, 6, 8]\n",
      "Steps done: 3898\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 11: -989.7219907019484\n",
      "[6, 8]\n",
      "Steps done: 3899\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 2: -122.15035636168733\n",
      "[6, 8, 2]\n",
      "Did target update\n",
      "Steps done: 3900\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 1: -132.50077720625575\n",
      "[6, 8, 2, 1]\n",
      "Steps done: 3901\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 16: -1028.5818710871179\n",
      "[8, 2, 1]\n",
      "Steps done: 3902\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 6: -132.50077720625575\n",
      "[8, 2, 1, 6]\n",
      "Steps done: 3903\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 16: -1028.5818710871179\n",
      "[8, 2, 1]\n",
      "Steps done: 3904\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 6: -132.50077720625575\n",
      "[8, 2, 1, 6]\n",
      "Steps done: 3905\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 5: -18.160449265449877\n",
      "[8, 2, 1, 6, 5]\n",
      "Steps done: 3906\n",
      "SV: [0.85175943 0.307235   2.6349347 ]\n",
      "Reward for action 20: -14.160449265449877\n",
      "[8, 2, 1, 6, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 550\n",
      "Steps done: 3907\n",
      "SV: [-0.35577208  0.84037983  0.2913916 ]\n",
      "Reward for action 2: -35.824691121767394\n",
      "[0, 1, 2]\n",
      "Steps done: 3908\n",
      "SV: [-0.35577208  0.84037983  0.2913916 ]\n",
      "Reward for action 20: -31.824691121767394\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 551\n",
      "Steps done: 3909\n",
      "SV: [ 0.11237427 -0.8359286  -1.885224  ]\n",
      "Reward for action 20: -535.5879770728966\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 552\n",
      "Steps done: 3910\n",
      "SV: [ 0.19298361 -0.07345059  1.0749418 ]\n",
      "Reward for action 20: -432.6611997840969\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 553\n",
      "Steps done: 3911\n",
      "SV: [ 0.06957664 -0.05335564  0.26757735]\n",
      "Reward for action 20: -232.50047250688874\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 554\n",
      "Steps done: 3912\n",
      "SV: [0.12504485 0.66151124 0.64499056]\n",
      "Reward for action 20: -23.795290078894872\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 555\n",
      "Steps done: 3913\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 7: -33.56191164754423\n",
      "[0, 1, 7]\n",
      "Steps done: 3914\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 5: -374.57813438298126\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 3915\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 15: -33.56191164754423\n",
      "[0, 1, 7]\n",
      "Steps done: 3916\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 8: -10.987343833083104\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 3917\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 2: -8.116354398293431\n",
      "[0, 1, 7, 8, 2]\n",
      "Steps done: 3918\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 3: -6.965341039233064\n",
      "[0, 1, 7, 8, 2, 3]\n",
      "Steps done: 3919\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 17: -6.271509229992765\n",
      "[0, 1, 8, 2, 3]\n",
      "Steps done: 3920\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 18: -41.6700076262037\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 3921\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 13: -94.04791363585962\n",
      "[0, 1, 2]\n",
      "Steps done: 3922\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 11: -24.4064134498082\n",
      "[0, 2]\n",
      "Steps done: 3923\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 8: -36.1098679478862\n",
      "[0, 2, 8]\n",
      "Steps done: 3924\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 4: -33.93277575690028\n",
      "[0, 2, 8, 4]\n",
      "Steps done: 3925\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 6: -20.256955267246227\n",
      "[0, 2, 8, 4, 6]\n",
      "Steps done: 3926\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 5: -177.79285169668051\n",
      "[0, 2, 8, 4, 6, 5]\n",
      "Steps done: 3927\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 7: -164.29406023762934\n",
      "[0, 2, 8, 4, 6, 5, 7]\n",
      "Steps done: 3928\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 10: -205.51050896618352\n",
      "[2, 8, 4, 6, 5, 7]\n",
      "Steps done: 3929\n",
      "SV: [-0.49488884  0.15253815  0.8294404 ]\n",
      "Reward for action 20: -201.51050896618352\n",
      "[2, 8, 4, 6, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 556\n",
      "Steps done: 3930\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 3: -0.37634041166293586\n",
      "[0, 1, 3]\n",
      "Steps done: 3931\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 11: -0.33061733035421303\n",
      "[0, 3]\n",
      "Steps done: 3932\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 2: -0.6865821452833824\n",
      "[0, 3, 2]\n",
      "Steps done: 3933\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 4: -0.38501116184830075\n",
      "[0, 3, 2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 3934\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 12: -0.2791207416234486\n",
      "[0, 3, 4]\n",
      "Steps done: 3935\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 5: -0.2732824249964927\n",
      "[0, 3, 4, 5]\n",
      "Steps done: 3936\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 15: -0.2791207416234486\n",
      "[0, 3, 4]\n",
      "Steps done: 3937\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 14: -0.33061733035421303\n",
      "[0, 3]\n",
      "Steps done: 3938\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 4: -0.2791207416234486\n",
      "[0, 3, 4]\n",
      "Steps done: 3939\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 13: -0.3506203770038355\n",
      "[0, 4]\n",
      "Steps done: 3940\n",
      "SV: [0.00925983 0.05062687 0.03222613]\n",
      "Reward for action 20: 3.6493796229961646\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 557\n",
      "Steps done: 3941\n",
      "SV: [ 0.15196513 -0.14397703 -0.40303072]\n",
      "Reward for action 5: -4.239935715622556\n",
      "[0, 1, 5]\n",
      "Steps done: 3942\n",
      "SV: [ 0.15196513 -0.14397703 -0.40303072]\n",
      "Reward for action 20: -0.2399357156225559\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 558\n",
      "Steps done: 3943\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 8: -58.5761495572547\n",
      "[0, 1, 8]\n",
      "Steps done: 3944\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 5: -62.36322136321498\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 3945\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 6: -59.02947120941568\n",
      "[0, 1, 8, 5, 6]\n",
      "Steps done: 3946\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 7: -60.66773314297363\n",
      "[0, 1, 8, 5, 6, 7]\n",
      "Steps done: 3947\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 3: -59.976394004185366\n",
      "[0, 1, 8, 5, 6, 7, 3]\n",
      "Steps done: 3948\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 16: -61.87061511798231\n",
      "[0, 1, 8, 5, 7, 3]\n",
      "Steps done: 3949\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 4: -60.05464322917973\n",
      "[0, 1, 8, 5, 7, 3, 4]\n",
      "Steps done: 3950\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 11: -64.78263553838053\n",
      "[0, 8, 5, 7, 3, 4]\n",
      "Steps done: 3951\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 15: -66.76156149327937\n",
      "[0, 8, 7, 3, 4]\n",
      "Steps done: 3952\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 9: -66.26331998991833\n",
      "[0, 8, 7, 3, 4, 9]\n",
      "Steps done: 3953\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 1: -61.21835230499813\n",
      "[0, 8, 7, 3, 4, 9, 1]\n",
      "Steps done: 3954\n",
      "SV: [ 0.04560553  0.06800955 -0.86303854]\n",
      "Reward for action 20: -57.21835230499813\n",
      "[0, 8, 7, 3, 4, 9, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 559\n",
      "Steps done: 3955\n",
      "SV: [ 0.10596245 -0.05518742 -0.3222018 ]\n",
      "Reward for action 8: -212.75791071915123\n",
      "[0, 1, 8]\n",
      "Steps done: 3956\n",
      "SV: [ 0.10596245 -0.05518742 -0.3222018 ]\n",
      "Reward for action 20: -208.75791071915123\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 560\n",
      "Steps done: 3957\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 3: -172.79200417309067\n",
      "[0, 1, 3]\n",
      "Steps done: 3958\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 5: -98.08169807065491\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 3959\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 10: -15.55381752956136\n",
      "[1, 3, 5]\n",
      "Steps done: 3960\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 9: -14.590972990068416\n",
      "[1, 3, 5, 9]\n",
      "Steps done: 3961\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 2: -57.43379832045736\n",
      "[1, 3, 5, 9, 2]\n",
      "Steps done: 3962\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 19: -70.44914676592028\n",
      "[1, 3, 5, 2]\n",
      "Steps done: 3963\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 7: -13.358643038147838\n",
      "[1, 3, 5, 2, 7]\n",
      "Steps done: 3964\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 0: -34.16400825510263\n",
      "[1, 3, 5, 2, 7, 0]\n",
      "Steps done: 3965\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 6: -45.19600840262825\n",
      "[1, 3, 5, 2, 7, 0, 6]\n",
      "Steps done: 3966\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 16: -34.16400825510263\n",
      "[1, 3, 5, 2, 7, 0]\n",
      "Steps done: 3967\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 6: -45.19600840262825\n",
      "[1, 3, 5, 2, 7, 0, 6]\n",
      "Steps done: 3968\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 16: -34.16400825510263\n",
      "[1, 3, 5, 2, 7, 0]\n",
      "Steps done: 3969\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 9: -20.941753275042412\n",
      "[1, 3, 5, 2, 7, 0, 9]\n",
      "Steps done: 3970\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 19: -34.16400825510263\n",
      "[1, 3, 5, 2, 7, 0]\n",
      "Steps done: 3971\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 17: -11.879854600584181\n",
      "[1, 3, 5, 2, 0]\n",
      "Steps done: 3972\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 7: -34.16400825510263\n",
      "[1, 3, 5, 2, 0, 7]\n",
      "Steps done: 3973\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 11: -25.980737291189648\n",
      "[3, 5, 2, 0, 7]\n",
      "Steps done: 3974\n",
      "SV: [-0.96154004  1.672003   -4.7076244 ]\n",
      "Reward for action 20: -21.980737291189648\n",
      "[3, 5, 2, 0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 561\n",
      "Steps done: 3975\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 8: -72.08092530483577\n",
      "[0, 1, 8]\n",
      "Steps done: 3976\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 10: -39.81471551298879\n",
      "[1, 8]\n",
      "Steps done: 3977\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 4: -42.720040402088834\n",
      "[1, 8, 4]\n",
      "Steps done: 3978\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 3: -70.36349870065027\n",
      "[1, 8, 4, 3]\n",
      "Steps done: 3979\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 6: -20.19838442898811\n",
      "[1, 8, 4, 3, 6]\n",
      "Steps done: 3980\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 18: -13.329948508109291\n",
      "[1, 4, 3, 6]\n",
      "Steps done: 3981\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 8: -20.198384428988103\n",
      "[1, 4, 3, 6, 8]\n",
      "Steps done: 3982\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 16: -70.36349870065024\n",
      "[1, 4, 3, 8]\n",
      "Steps done: 3983\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 2: -66.52965732187133\n",
      "[1, 4, 3, 8, 2]\n",
      "Steps done: 3984\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 12: -70.36349870065024\n",
      "[1, 4, 3, 8]\n",
      "Steps done: 3985\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 7: -95.9159540726595\n",
      "[1, 4, 3, 8, 7]\n",
      "Steps done: 3986\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 6: -54.04913593857083\n",
      "[1, 4, 3, 8, 7, 6]\n",
      "Steps done: 3987\n",
      "SV: [ 0.12960023 -0.01294148 -0.76725394]\n",
      "Reward for action 20: -50.04913593857083\n",
      "[1, 4, 3, 8, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 562\n",
      "Steps done: 3988\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 5: -85.74621180979248\n",
      "[0, 1, 5]\n",
      "Steps done: 3989\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 4: -107.91839927958313\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 3990\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 6: -70.13138125773847\n",
      "[0, 1, 5, 4, 6]\n",
      "Steps done: 3991\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 10: -41.70009531423906\n",
      "[1, 5, 4, 6]\n",
      "Steps done: 3992\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 14: -279.1639003331044\n",
      "[1, 5, 6]\n",
      "Steps done: 3993\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 15: -780.1357496008218\n",
      "[1, 6]\n",
      "Steps done: 3994\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 8: -110.42638092679405\n",
      "[1, 6, 8]\n",
      "Steps done: 3995\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 16: -868.840669733888\n",
      "[1, 8]\n",
      "Steps done: 3996\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 7: -361.61261634877513\n",
      "[1, 8, 7]\n",
      "Steps done: 3997\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 6: -211.56300984466225\n",
      "[1, 8, 7, 6]\n",
      "Steps done: 3998\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 9: -161.06641887280597\n",
      "[1, 8, 7, 6, 9]\n",
      "Steps done: 3999\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 3: -104.39652254304376\n",
      "[1, 8, 7, 6, 9, 3]\n",
      "Did target update\n",
      "Steps done: 4000\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 16: -198.12071693534375\n",
      "[1, 8, 7, 9, 3]\n",
      "Steps done: 4001\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 17: -86.38071133282011\n",
      "[1, 8, 9, 3]\n",
      "Steps done: 4002\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 13: -269.00313029930874\n",
      "[1, 8, 9]\n",
      "Steps done: 4003\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 6: -46.970026372167965\n",
      "[1, 8, 9, 6]\n",
      "Steps done: 4004\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 3: -20.913866833756572\n",
      "[1, 8, 9, 6, 3]\n",
      "Steps done: 4005\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 19: -51.06523526894188\n",
      "[1, 8, 6, 3]\n",
      "Steps done: 4006\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 11: -267.9059798171812\n",
      "[8, 6, 3]\n",
      "Steps done: 4007\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 1: -51.06523526894189\n",
      "[8, 6, 3, 1]\n",
      "Steps done: 4008\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 9: -20.913866833756572\n",
      "[8, 6, 3, 1, 9]\n",
      "Steps done: 4009\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 16: -86.38071133282014\n",
      "[8, 3, 1, 9]\n",
      "Steps done: 4010\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 5: -28.261919569154124\n",
      "[8, 3, 1, 9, 5]\n",
      "Steps done: 4011\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 11: -72.12123521414712\n",
      "[8, 3, 9, 5]\n",
      "Steps done: 4012\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 1: -28.26191956915411\n",
      "[8, 3, 9, 5, 1]\n",
      "Steps done: 4013\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Reward for action 13: -84.75095236098491\n",
      "[8, 9, 5, 1]\n",
      "Steps done: 4014\n",
      "SV: [ 0.01124426  0.02498833 -1.3073666 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -80.75095236098491\n",
      "[8, 9, 5, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 563\n",
      "Steps done: 4015\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 3: -25.37556911345454\n",
      "[0, 1, 3]\n",
      "Steps done: 4016\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 13: -44.06546172719361\n",
      "[0, 1]\n",
      "Steps done: 4017\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 2: -82.94620383474003\n",
      "[0, 1, 2]\n",
      "Steps done: 4018\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 12: -44.06546172719361\n",
      "[0, 1]\n",
      "Steps done: 4019\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 4: -366.07243378082774\n",
      "[0, 1, 4]\n",
      "Steps done: 4020\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 14: -44.06546172719361\n",
      "[0, 1]\n",
      "Steps done: 4021\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 2: -82.94620383474003\n",
      "[0, 1, 2]\n",
      "Steps done: 4022\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 12: -44.06546172719361\n",
      "[0, 1]\n",
      "Steps done: 4023\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 3: -25.37556911345454\n",
      "[0, 1, 3]\n",
      "Steps done: 4024\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 10: -28.243141709762153\n",
      "[1, 3]\n",
      "Steps done: 4025\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 2: -5.603966440908038\n",
      "[1, 3, 2]\n",
      "Steps done: 4026\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 13: -31.587560205086447\n",
      "[1, 2]\n",
      "Steps done: 4027\n",
      "SV: [ 0.8732377  0.8339142 -1.7518218]\n",
      "Reward for action 20: -27.587560205086447\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 564\n",
      "Steps done: 4028\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 5: -178.17610612847156\n",
      "[0, 1, 5]\n",
      "Steps done: 4029\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 11: -162.61675772276902\n",
      "[0, 5]\n",
      "Steps done: 4030\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 6: -58.73022882461035\n",
      "[0, 5, 6]\n",
      "Steps done: 4031\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 16: -162.61675772276902\n",
      "[0, 5]\n",
      "Steps done: 4032\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 6: -58.73022882461035\n",
      "[0, 5, 6]\n",
      "Steps done: 4033\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 10: -24.68952020285167\n",
      "[5, 6]\n",
      "Steps done: 4034\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 0: -58.730228824610336\n",
      "[5, 6, 0]\n",
      "Steps done: 4035\n",
      "SV: [-0.3643814 -0.6884541 -1.051962 ]\n",
      "Reward for action 20: -54.730228824610336\n",
      "[5, 6, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 565\n",
      "Steps done: 4036\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 6: -25.363594919299196\n",
      "[0, 1, 6]\n",
      "Steps done: 4037\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 2: -27.00307827679444\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 4038\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 11: -25.72203620280682\n",
      "[0, 6, 2]\n",
      "Steps done: 4039\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 10: -25.391348954135527\n",
      "[6, 2]\n",
      "Steps done: 4040\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 3: -26.32430445755186\n",
      "[6, 2, 3]\n",
      "Steps done: 4041\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 7: -26.696372918170148\n",
      "[6, 2, 3, 7]\n",
      "Steps done: 4042\n",
      "SV: [ 0.20713878 -0.01159265 -0.5237264 ]\n",
      "Reward for action 20: -22.696372918170148\n",
      "[6, 2, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 566\n",
      "Steps done: 4043\n",
      "SV: [0.20475502 0.9923415  0.27722344]\n",
      "Reward for action 20: -139.6503941188276\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 567\n",
      "Steps done: 4044\n",
      "SV: [0.12085818 0.70947176 1.5652109 ]\n",
      "Reward for action 20: -483.6814111251679\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 568\n",
      "Steps done: 4045\n",
      "SV: [0.73260695 0.19784005 0.52542627]\n",
      "Reward for action 4: -15.230281936168359\n",
      "[0, 1, 4]\n",
      "Steps done: 4046\n",
      "SV: [0.73260695 0.19784005 0.52542627]\n",
      "Reward for action 10: -14.153011947825432\n",
      "[1, 4]\n",
      "Steps done: 4047\n",
      "SV: [0.73260695 0.19784005 0.52542627]\n",
      "Reward for action 0: -15.230281936168359\n",
      "[1, 4, 0]\n",
      "Steps done: 4048\n",
      "SV: [0.73260695 0.19784005 0.52542627]\n",
      "Reward for action 20: -11.230281936168359\n",
      "[1, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 569\n",
      "Steps done: 4049\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 2: -109.45316866806324\n",
      "[0, 1, 2]\n",
      "Steps done: 4050\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 7: -63.885412453282214\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 4051\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 8: -48.40634961599847\n",
      "[0, 1, 2, 7, 8]\n",
      "Steps done: 4052\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 5: -18.343864335858896\n",
      "[0, 1, 2, 7, 8, 5]\n",
      "Steps done: 4053\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 4: -12.643096069488026\n",
      "[0, 1, 2, 7, 8, 5, 4]\n",
      "Steps done: 4054\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 15: -26.594296269021683\n",
      "[0, 1, 2, 7, 8, 4]\n",
      "Steps done: 4055\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 5: -12.643096069488026\n",
      "[0, 1, 2, 7, 8, 4, 5]\n",
      "Steps done: 4056\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 3: -12.087474107750257\n",
      "[0, 1, 2, 7, 8, 4, 5, 3]\n",
      "Steps done: 4057\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 6: -17.164078688631523\n",
      "[0, 1, 2, 7, 8, 4, 5, 3, 6]\n",
      "Steps done: 4058\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 18: -14.895600752843412\n",
      "[0, 1, 2, 7, 4, 5, 3, 6]\n",
      "Steps done: 4059\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 14: -18.795247444088044\n",
      "[0, 1, 2, 7, 5, 3, 6]\n",
      "Steps done: 4060\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 15: -37.96489866924683\n",
      "[0, 1, 2, 7, 3, 6]\n",
      "Steps done: 4061\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 13: -56.064933740726445\n",
      "[0, 1, 2, 7, 6]\n",
      "Steps done: 4062\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 8: -45.56637211829197\n",
      "[0, 1, 2, 7, 6, 8]\n",
      "Steps done: 4063\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 3: -34.26484246437528\n",
      "[0, 1, 2, 7, 6, 8, 3]\n",
      "Steps done: 4064\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 17: -34.731980142231755\n",
      "[0, 1, 2, 6, 8, 3]\n",
      "Steps done: 4065\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 11: -36.04708678983604\n",
      "[0, 2, 6, 8, 3]\n",
      "Steps done: 4066\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 7: -34.73765048588617\n",
      "[0, 2, 6, 8, 3, 7]\n",
      "Steps done: 4067\n",
      "SV: [-0.03505152  0.0338306   0.82047695]\n",
      "Reward for action 20: -30.73765048588617\n",
      "[0, 2, 6, 8, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 570\n",
      "Steps done: 4068\n",
      "SV: [-0.3483264  2.197213   4.2046027]\n",
      "Reward for action 20: -121.96961452895317\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 571\n",
      "Steps done: 4069\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 3: -1.8420745260513776\n",
      "[0, 1, 3]\n",
      "Steps done: 4070\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 8: -2.3422000400126954\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 4071\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 13: -6.585016235815486\n",
      "[0, 1, 8]\n",
      "Steps done: 4072\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 6: -9.685241021468158\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 4073\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 4: -7.528387270555049\n",
      "[0, 1, 8, 6, 4]\n",
      "Steps done: 4074\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 9: -3.686769674848879\n",
      "[0, 1, 8, 6, 4, 9]\n",
      "Steps done: 4075\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 5: -4.857092558444963\n",
      "[0, 1, 8, 6, 4, 9, 5]\n",
      "Steps done: 4076\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 15: -3.686769674848879\n",
      "[0, 1, 8, 6, 4, 9]\n",
      "Steps done: 4077\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 11: -4.974962567241142\n",
      "[0, 8, 6, 4, 9]\n",
      "Steps done: 4078\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 16: -3.3687166498023804\n",
      "[0, 8, 4, 9]\n",
      "Steps done: 4079\n",
      "SV: [-0.10033417 -0.05946456  0.41083828]\n",
      "Reward for action 20: 0.6312833501976196\n",
      "[0, 8, 4, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 572\n",
      "Steps done: 4080\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 4: -120.40866804586695\n",
      "[0, 1, 4]\n",
      "Steps done: 4081\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 14: -190.6504594734444\n",
      "[0, 1]\n",
      "Steps done: 4082\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 2: -95.96517506268876\n",
      "[0, 1, 2]\n",
      "Steps done: 4083\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 5: -134.47986679360778\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 4084\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 11: -117.46647689434405\n",
      "[0, 2, 5]\n",
      "Steps done: 4085\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 10: -97.43142001647065\n",
      "[2, 5]\n",
      "Steps done: 4086\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 3: -65.43007009535157\n",
      "[2, 5, 3]\n",
      "Steps done: 4087\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 4: -59.61161613467033\n",
      "[2, 5, 3, 4]\n",
      "Steps done: 4088\n",
      "SV: [-0.01178394  0.2190213   1.344307  ]\n",
      "Reward for action 20: -55.61161613467033\n",
      "[2, 5, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 573\n",
      "Steps done: 4089\n",
      "SV: [ 0.1087395   0.32016262 -0.71085876]\n",
      "Reward for action 5: -27.76665365648921\n",
      "[0, 1, 5]\n",
      "Steps done: 4090\n",
      "SV: [ 0.1087395   0.32016262 -0.71085876]\n",
      "Reward for action 3: -19.50396662260963\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 4091\n",
      "SV: [ 0.1087395   0.32016262 -0.71085876]\n",
      "Reward for action 11: -74.7977918490791\n",
      "[0, 5, 3]\n",
      "Steps done: 4092\n",
      "SV: [ 0.1087395   0.32016262 -0.71085876]\n",
      "Reward for action 20: -70.7977918490791\n",
      "[0, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 574\n",
      "Steps done: 4093\n",
      "SV: [0.01527888 0.01669124 0.22052018]\n",
      "Reward for action 3: -4.133239179463795\n",
      "[0, 1, 3]\n",
      "Steps done: 4094\n",
      "SV: [0.01527888 0.01669124 0.22052018]\n",
      "Reward for action 11: -4.128869566609797\n",
      "[0, 3]\n",
      "Steps done: 4095\n",
      "SV: [0.01527888 0.01669124 0.22052018]\n",
      "Reward for action 1: -4.133239179463793\n",
      "[0, 3, 1]\n",
      "Steps done: 4096\n",
      "SV: [0.01527888 0.01669124 0.22052018]\n",
      "Reward for action 11: -4.128869566609797\n",
      "[0, 3]\n",
      "Steps done: 4097\n",
      "SV: [0.01527888 0.01669124 0.22052018]\n",
      "Reward for action 20: -0.1288695666097972\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 575\n",
      "Steps done: 4098\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 6: -34.0976000898927\n",
      "[0, 1, 6]\n",
      "Steps done: 4099\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 11: -163.4083292096776\n",
      "[0, 6]\n",
      "Did target update\n",
      "Steps done: 4100\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 7: -48.07596769094983\n",
      "[0, 6, 7]\n",
      "Steps done: 4101\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 8: -18.19264310816616\n",
      "[0, 6, 7, 8]\n",
      "Steps done: 4102\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 9: -17.82081509901036\n",
      "[0, 6, 7, 8, 9]\n",
      "Steps done: 4103\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 18: -30.841113989503125\n",
      "[0, 6, 7, 9]\n",
      "Steps done: 4104\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 19: -48.07596769094983\n",
      "[0, 6, 7]\n",
      "Steps done: 4105\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 5: -29.5373494732659\n",
      "[0, 6, 7, 5]\n",
      "Steps done: 4106\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 1: -19.91168318714676\n",
      "[0, 6, 7, 5, 1]\n",
      "Steps done: 4107\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 4: -17.909700834308662\n",
      "[0, 6, 7, 5, 1, 4]\n",
      "Steps done: 4108\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 11: -22.211868846563245\n",
      "[0, 6, 7, 5, 4]\n",
      "Steps done: 4109\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 9: -19.31664219295719\n",
      "[0, 6, 7, 5, 4, 9]\n",
      "Steps done: 4110\n",
      "SV: [-0.12598817  0.08537909 -0.5136131 ]\n",
      "Reward for action 20: -15.31664219295719\n",
      "[0, 6, 7, 5, 4, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 576\n",
      "Steps done: 4111\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 6: -179.03374216814703\n",
      "[0, 1, 6]\n",
      "Steps done: 4112\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 16: -569.2900709477053\n",
      "[0, 1]\n",
      "Steps done: 4113\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 6: -179.03374216814703\n",
      "[0, 1, 6]\n",
      "Steps done: 4114\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 4: -126.5846439286179\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 4115\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 3: -31.030864765587538\n",
      "[0, 1, 6, 4, 3]\n",
      "Steps done: 4116\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 11: -11.219672946461937\n",
      "[0, 6, 4, 3]\n",
      "Steps done: 4117\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 1: -31.030864765587538\n",
      "[0, 6, 4, 3, 1]\n",
      "Steps done: 4118\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 7: -14.698310359787243\n",
      "[0, 6, 4, 3, 1, 7]\n",
      "Steps done: 4119\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 13: -44.19667136077396\n",
      "[0, 6, 4, 1, 7]\n",
      "Steps done: 4120\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 5: -53.337021044056044\n",
      "[0, 6, 4, 1, 7, 5]\n",
      "Steps done: 4121\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 2: -43.72833925532423\n",
      "[0, 6, 4, 1, 7, 5, 2]\n",
      "Steps done: 4122\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 10: -21.865027123427534\n",
      "[6, 4, 1, 7, 5, 2]\n",
      "Steps done: 4123\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 9: -12.71312282999588\n",
      "[6, 4, 1, 7, 5, 2, 9]\n",
      "Steps done: 4124\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 19: -21.865027123427534\n",
      "[6, 4, 1, 7, 5, 2]\n",
      "Steps done: 4125\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 14: -20.766544055089536\n",
      "[6, 1, 7, 5, 2]\n",
      "Steps done: 4126\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 9: -15.759142088709034\n",
      "[6, 1, 7, 5, 2, 9]\n",
      "Steps done: 4127\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 17: -14.89610226197328\n",
      "[6, 1, 5, 2, 9]\n",
      "Steps done: 4128\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 16: -65.69452210337445\n",
      "[1, 5, 2, 9]\n",
      "Steps done: 4129\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 0: -15.845813300933289\n",
      "[1, 5, 2, 9, 0]\n",
      "Steps done: 4130\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 15: -15.679849333582645\n",
      "[1, 2, 9, 0]\n",
      "Steps done: 4131\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 7: -14.539134219940824\n",
      "[1, 2, 9, 0, 7]\n",
      "Steps done: 4132\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 6: -12.187679051159844\n",
      "[1, 2, 9, 0, 7, 6]\n",
      "Steps done: 4133\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 4: -12.294629812489234\n",
      "[1, 2, 9, 0, 7, 6, 4]\n",
      "Steps done: 4134\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 17: -19.244298956736003\n",
      "[1, 2, 9, 0, 6, 4]\n",
      "Steps done: 4135\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 16: -14.883401304187306\n",
      "[1, 2, 9, 0, 4]\n",
      "Steps done: 4136\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Reward for action 5: -13.219625516901552\n",
      "[1, 2, 9, 0, 4, 5]\n",
      "Steps done: 4137\n",
      "SV: [0.19143483 0.13053763 1.2863364 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -9.219625516901552\n",
      "[1, 2, 9, 0, 4, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 577\n",
      "Steps done: 4138\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 3: -108.55643445394959\n",
      "[0, 1, 3]\n",
      "Steps done: 4139\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 10: -618.9186876609713\n",
      "[1, 3]\n",
      "Steps done: 4140\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 8: -804.1285246885818\n",
      "[1, 3, 8]\n",
      "Steps done: 4141\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 6: -837.1921641262447\n",
      "[1, 3, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4142\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 16: -804.1285246885818\n",
      "[1, 3, 8]\n",
      "Steps done: 4143\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 6: -837.1921641262447\n",
      "[1, 3, 8, 6]\n",
      "Steps done: 4144\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 16: -804.1285246885818\n",
      "[1, 3, 8]\n",
      "Steps done: 4145\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 4: -141.59759684885833\n",
      "[1, 3, 8, 4]\n",
      "Steps done: 4146\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 6: -206.4631284492724\n",
      "[1, 3, 8, 4, 6]\n",
      "Steps done: 4147\n",
      "SV: [ 0.00618657  0.19166958 -1.1192465 ]\n",
      "Reward for action 20: -202.4631284492724\n",
      "[1, 3, 8, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 578\n",
      "Steps done: 4148\n",
      "SV: [0.6018749  0.34096482 0.5884411 ]\n",
      "Reward for action 2: -91.48050157091578\n",
      "[0, 1, 2]\n",
      "Steps done: 4149\n",
      "SV: [0.6018749  0.34096482 0.5884411 ]\n",
      "Reward for action 3: -41.85856571127874\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 4150\n",
      "SV: [0.6018749  0.34096482 0.5884411 ]\n",
      "Reward for action 13: -91.48050157091578\n",
      "[0, 1, 2]\n",
      "Steps done: 4151\n",
      "SV: [0.6018749  0.34096482 0.5884411 ]\n",
      "Reward for action 12: -174.85948152928233\n",
      "[0, 1]\n",
      "Steps done: 4152\n",
      "SV: [0.6018749  0.34096482 0.5884411 ]\n",
      "Reward for action 3: -35.15368825681195\n",
      "[0, 1, 3]\n",
      "Steps done: 4153\n",
      "SV: [0.6018749  0.34096482 0.5884411 ]\n",
      "Reward for action 20: -31.153688256811947\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 579\n",
      "Steps done: 4154\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 7: -12.06309099136709\n",
      "[0, 1, 7]\n",
      "Steps done: 4155\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 4: -14.648786840120689\n",
      "[0, 1, 7, 4]\n",
      "Steps done: 4156\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 10: -18.668502068284052\n",
      "[1, 7, 4]\n",
      "Steps done: 4157\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 17: -18.859365812720327\n",
      "[1, 4]\n",
      "Steps done: 4158\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 7: -18.66850206828406\n",
      "[1, 4, 7]\n",
      "Steps done: 4159\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 11: -17.93653341494574\n",
      "[4, 7]\n",
      "Steps done: 4160\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 6: -18.655707676059514\n",
      "[4, 7, 6]\n",
      "Steps done: 4161\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 14: -18.4022345322355\n",
      "[7, 6]\n",
      "Steps done: 4162\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 9: -16.698076791090877\n",
      "[7, 6, 9]\n",
      "Steps done: 4163\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 4: -17.389176104429787\n",
      "[7, 6, 9, 4]\n",
      "Steps done: 4164\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 2: -17.39230156657319\n",
      "[7, 6, 9, 4, 2]\n",
      "Steps done: 4165\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 14: -16.846724157105193\n",
      "[7, 6, 9, 2]\n",
      "Steps done: 4166\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 16: -16.510639154520412\n",
      "[7, 9, 2]\n",
      "Steps done: 4167\n",
      "SV: [ 0.0136243  -0.01896905 -0.483916  ]\n",
      "Reward for action 20: -12.510639154520412\n",
      "[7, 9, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 580\n",
      "Steps done: 4168\n",
      "SV: [-0.50655764  0.8496044   0.10983349]\n",
      "Reward for action 3: -61.02773439010198\n",
      "[0, 1, 3]\n",
      "Steps done: 4169\n",
      "SV: [-0.50655764  0.8496044   0.10983349]\n",
      "Reward for action 20: -57.02773439010198\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 581\n",
      "Steps done: 4170\n",
      "SV: [ 0.24076784 -0.6073425  -0.60906255]\n",
      "Reward for action 4: -19.169746644985437\n",
      "[0, 1, 4]\n",
      "Steps done: 4171\n",
      "SV: [ 0.24076784 -0.6073425  -0.60906255]\n",
      "Reward for action 14: -250.29918213632556\n",
      "[0, 1]\n",
      "Steps done: 4172\n",
      "SV: [ 0.24076784 -0.6073425  -0.60906255]\n",
      "Reward for action 2: -7.302356567838758\n",
      "[0, 1, 2]\n",
      "Steps done: 4173\n",
      "SV: [ 0.24076784 -0.6073425  -0.60906255]\n",
      "Reward for action 12: -250.29918213632556\n",
      "[0, 1]\n",
      "Steps done: 4174\n",
      "SV: [ 0.24076784 -0.6073425  -0.60906255]\n",
      "Reward for action 20: -246.29918213632556\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 582\n",
      "Steps done: 4175\n",
      "SV: [-0.0637601   0.03080282 -0.43162972]\n",
      "Reward for action 3: -148.23679881089888\n",
      "[0, 1, 3]\n",
      "Steps done: 4176\n",
      "SV: [-0.0637601   0.03080282 -0.43162972]\n",
      "Reward for action 4: -91.29661217581831\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 4177\n",
      "SV: [-0.0637601   0.03080282 -0.43162972]\n",
      "Reward for action 7: -60.505316844315004\n",
      "[0, 1, 3, 4, 7]\n",
      "Steps done: 4178\n",
      "SV: [-0.0637601   0.03080282 -0.43162972]\n",
      "Reward for action 20: -56.505316844315004\n",
      "[0, 1, 3, 4, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 583\n",
      "Steps done: 4179\n",
      "SV: [ 0.18576464  0.04882162 -1.5925254 ]\n",
      "Reward for action 2: -194.02141999024408\n",
      "[0, 1, 2]\n",
      "Steps done: 4180\n",
      "SV: [ 0.18576464  0.04882162 -1.5925254 ]\n",
      "Reward for action 5: -215.88866657931968\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 4181\n",
      "SV: [ 0.18576464  0.04882162 -1.5925254 ]\n",
      "Reward for action 11: -221.14139150711654\n",
      "[0, 2, 5]\n",
      "Steps done: 4182\n",
      "SV: [ 0.18576464  0.04882162 -1.5925254 ]\n",
      "Reward for action 1: -215.88866657931968\n",
      "[0, 2, 5, 1]\n",
      "Steps done: 4183\n",
      "SV: [ 0.18576464  0.04882162 -1.5925254 ]\n",
      "Reward for action 4: -213.79858144324143\n",
      "[0, 2, 5, 1, 4]\n",
      "Steps done: 4184\n",
      "SV: [ 0.18576464  0.04882162 -1.5925254 ]\n",
      "Reward for action 20: -209.79858144324143\n",
      "[0, 2, 5, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 585\n",
      "Steps done: 4185\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 7: -36.108842168732586\n",
      "[0, 1, 7]\n",
      "Steps done: 4186\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 8: -36.340341301904644\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 4187\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 11: -36.27241972852488\n",
      "[0, 7, 8]\n",
      "Steps done: 4188\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 17: -36.23780668516984\n",
      "[0, 8]\n",
      "Steps done: 4189\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 1: -37.52484819056263\n",
      "[0, 8, 1]\n",
      "Steps done: 4190\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 7: -36.340341301904644\n",
      "[0, 8, 1, 7]\n",
      "Steps done: 4191\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 5: -37.30491345759603\n",
      "[0, 8, 1, 7, 5]\n",
      "Steps done: 4192\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 2: -37.9834981755815\n",
      "[0, 8, 1, 7, 5, 2]\n",
      "Steps done: 4193\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 11: -35.50836051995235\n",
      "[0, 8, 7, 5, 2]\n",
      "Steps done: 4194\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 3: -35.36456155428563\n",
      "[0, 8, 7, 5, 2, 3]\n",
      "Steps done: 4195\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 9: -35.32919057247051\n",
      "[0, 8, 7, 5, 2, 3, 9]\n",
      "Steps done: 4196\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 19: -35.36456155428563\n",
      "[0, 8, 7, 5, 2, 3]\n",
      "Steps done: 4197\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 4: -35.565986950377656\n",
      "[0, 8, 7, 5, 2, 3, 4]\n",
      "Steps done: 4198\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 10: -36.119587882349876\n",
      "[8, 7, 5, 2, 3, 4]\n",
      "Steps done: 4199\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 13: -35.969562674216974\n",
      "[8, 7, 5, 2, 4]\n",
      "Did target update\n",
      "Steps done: 4200\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 12: -35.85031854842309\n",
      "[8, 7, 5, 4]\n",
      "Steps done: 4201\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 0: -35.65668133588306\n",
      "[8, 7, 5, 4, 0]\n",
      "Steps done: 4202\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 15: -36.11116828433367\n",
      "[8, 7, 4, 0]\n",
      "Steps done: 4203\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 6: -34.73568200092774\n",
      "[8, 7, 4, 0, 6]\n",
      "Steps done: 4204\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 14: -35.03261996072828\n",
      "[8, 7, 0, 6]\n",
      "Steps done: 4205\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 16: -36.27241972852488\n",
      "[8, 7, 0]\n",
      "Steps done: 4206\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 6: -35.03261996072828\n",
      "[8, 7, 0, 6]\n",
      "Steps done: 4207\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 1: -35.57991105548648\n",
      "[8, 7, 0, 6, 1]\n",
      "Steps done: 4208\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 3: -35.12670936700522\n",
      "[8, 7, 0, 6, 1, 3]\n",
      "Steps done: 4209\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 10: -35.132347251092156\n",
      "[8, 7, 6, 1, 3]\n",
      "Steps done: 4210\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Reward for action 2: -36.01043845774166\n",
      "[8, 7, 6, 1, 3, 2]\n",
      "Steps done: 4211\n",
      "SV: [ 0.18991123 -0.13422082  0.628406  ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -32.01043845774166\n",
      "[8, 7, 6, 1, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 586\n",
      "Steps done: 4212\n",
      "SV: [ 0.13681106  0.18418674 -0.37729475]\n",
      "Reward for action 20: -12.133066969814582\n",
      "[0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 587\n",
      "Steps done: 4213\n",
      "SV: [-0.9031103 -1.0009009 -2.2289317]\n",
      "Reward for action 2: -34.862856644091565\n",
      "[0, 1, 2]\n",
      "Steps done: 4214\n",
      "SV: [-0.9031103 -1.0009009 -2.2289317]\n",
      "Reward for action 3: -65.06212985235675\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 4215\n",
      "SV: [-0.9031103 -1.0009009 -2.2289317]\n",
      "Reward for action 20: -61.06212985235675\n",
      "[0, 1, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 588\n",
      "Steps done: 4216\n",
      "SV: [-0.02442632 -0.06450357 -0.64297444]\n",
      "Reward for action 20: -77.50524739162482\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 589\n",
      "Steps done: 4217\n",
      "SV: [0.41853723 0.260755   1.3504785 ]\n",
      "Reward for action 4: -377.9893195565251\n",
      "[0, 1, 4]\n",
      "Steps done: 4218\n",
      "SV: [0.41853723 0.260755   1.3504785 ]\n",
      "Reward for action 7: -458.83499979336244\n",
      "[0, 1, 4, 7]\n",
      "Steps done: 4219\n",
      "SV: [0.41853723 0.260755   1.3504785 ]\n",
      "Reward for action 14: -381.2498020130439\n",
      "[0, 1, 7]\n",
      "Steps done: 4220\n",
      "SV: [0.41853723 0.260755   1.3504785 ]\n",
      "Reward for action 20: -377.2498020130439\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 590\n",
      "Steps done: 4221\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 8: -20.175802891571667\n",
      "[0, 1, 8]\n",
      "Steps done: 4222\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 6: -21.40364495026751\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 4223\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 16: -20.175802891571667\n",
      "[0, 1, 8]\n",
      "Steps done: 4224\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 7: -19.665070155079096\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 4225\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 17: -20.175802891571667\n",
      "[0, 1, 8]\n",
      "Steps done: 4226\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 7: -19.665070155079096\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 4227\n",
      "SV: [ 0.00965062  0.0153893  -0.5078959 ]\n",
      "Reward for action 20: -15.665070155079096\n",
      "[0, 1, 8, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 591\n",
      "Steps done: 4228\n",
      "SV: [-0.43660146  1.4970244  -0.9581041 ]\n",
      "Reward for action 20: -43.378070643335164\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 592\n",
      "Steps done: 4229\n",
      "SV: [ 0.45428082  0.4097376  -1.0152487 ]\n",
      "Reward for action 4: -0.3048872790500851\n",
      "[0, 1, 4]\n",
      "Steps done: 4230\n",
      "SV: [ 0.45428082  0.4097376  -1.0152487 ]\n",
      "Reward for action 14: -80.83415879224148\n",
      "[0, 1]\n",
      "Steps done: 4231\n",
      "SV: [ 0.45428082  0.4097376  -1.0152487 ]\n",
      "Reward for action 2: -3.2728162858881156\n",
      "[0, 1, 2]\n",
      "Steps done: 4232\n",
      "SV: [ 0.45428082  0.4097376  -1.0152487 ]\n",
      "Reward for action 20: 0.7271837141118844\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 593\n",
      "Steps done: 4233\n",
      "SV: [0.49130136 0.0545686  0.21088122]\n",
      "Reward for action 4: -18.831892439411295\n",
      "[0, 1, 4]\n",
      "Steps done: 4234\n",
      "SV: [0.49130136 0.0545686  0.21088122]\n",
      "Reward for action 10: -2.09101568065469\n",
      "[1, 4]\n",
      "Steps done: 4235\n",
      "SV: [0.49130136 0.0545686  0.21088122]\n",
      "Reward for action 2: -10.726657183298121\n",
      "[1, 4, 2]\n",
      "Steps done: 4236\n",
      "SV: [0.49130136 0.0545686  0.21088122]\n",
      "Reward for action 20: -6.726657183298121\n",
      "[1, 4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 594\n",
      "Steps done: 4237\n",
      "SV: [ 0.04388895 -0.4309678   0.0733678 ]\n",
      "Reward for action 4: -27.595720657685373\n",
      "[0, 1, 4]\n",
      "Steps done: 4238\n",
      "SV: [ 0.04388895 -0.4309678   0.0733678 ]\n",
      "Reward for action 11: -35.58262980886775\n",
      "[0, 4]\n",
      "Steps done: 4239\n",
      "SV: [ 0.04388895 -0.4309678   0.0733678 ]\n",
      "Reward for action 5: -20.0384504549521\n",
      "[0, 4, 5]\n",
      "Steps done: 4240\n",
      "SV: [ 0.04388895 -0.4309678   0.0733678 ]\n",
      "Reward for action 14: -52.43931132776146\n",
      "[0, 5]\n",
      "Steps done: 4241\n",
      "SV: [ 0.04388895 -0.4309678   0.0733678 ]\n",
      "Reward for action 20: -48.43931132776146\n",
      "[0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 595\n",
      "Steps done: 4242\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 4: -160.060147870101\n",
      "[0, 1, 4]\n",
      "Steps done: 4243\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 3: -142.91494519186696\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 4244\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 14: -140.9702682445615\n",
      "[0, 1, 3]\n",
      "Steps done: 4245\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 11: -133.25382761911604\n",
      "[0, 3]\n",
      "Steps done: 4246\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 1: -140.9702682445615\n",
      "[0, 3, 1]\n",
      "Steps done: 4247\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 10: -133.4220621224136\n",
      "[3, 1]\n",
      "Steps done: 4248\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 4: -136.38395728063819\n",
      "[3, 1, 4]\n",
      "Steps done: 4249\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 0: -142.91494519186696\n",
      "[3, 1, 4, 0]\n",
      "Steps done: 4250\n",
      "SV: [-1.7985528   0.5757996   0.60192126]\n",
      "Reward for action 20: -138.91494519186696\n",
      "[3, 1, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 596\n",
      "Steps done: 4251\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 8: -106.47054703804737\n",
      "[0, 1, 8]\n",
      "Steps done: 4252\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 3: -42.63599154945277\n",
      "[0, 1, 8, 3]\n",
      "Steps done: 4253\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 2: -26.035748731967395\n",
      "[0, 1, 8, 3, 2]\n",
      "Steps done: 4254\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 13: -46.36311159162487\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 4255\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 11: -61.90974813276337\n",
      "[0, 8, 2]\n",
      "Steps done: 4256\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 12: -204.20947199449282\n",
      "[0, 8]\n",
      "Steps done: 4257\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 5: -33.45944473295491\n",
      "[0, 8, 5]\n",
      "Steps done: 4258\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 3: -22.66227106263124\n",
      "[0, 8, 5, 3]\n",
      "Steps done: 4259\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 15: -50.16620401905513\n",
      "[0, 8, 3]\n",
      "Steps done: 4260\n",
      "SV: [ 0.09996287 -0.13597915  0.08294141]\n",
      "Reward for action 20: -46.16620401905513\n",
      "[0, 8, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 597\n",
      "Steps done: 4261\n",
      "SV: [0.7878104  0.22844334 0.78792566]\n",
      "Reward for action 20: -3.088937339750739\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 598\n",
      "Steps done: 4262\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 4: -0.5115066024272313\n",
      "[0, 1, 4]\n",
      "Steps done: 4263\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 3: -0.3426057065567746\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 4264\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 2: -0.6878982270429218\n",
      "[0, 1, 4, 3, 2]\n",
      "Steps done: 4265\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 11: -2.2373727007486934\n",
      "[0, 4, 3, 2]\n",
      "Steps done: 4266\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 14: -0.14149806402300494\n",
      "[0, 3, 2]\n",
      "Steps done: 4267\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 12: -0.07921953462079132\n",
      "[0, 3]\n",
      "Steps done: 4268\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 4: -1.5143120665608518\n",
      "[0, 3, 4]\n",
      "Steps done: 4269\n",
      "SV: [ 0.03753951 -0.02195819  0.03261318]\n",
      "Reward for action 20: 2.485687933439148\n",
      "[0, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 599\n",
      "Steps done: 4270\n",
      "SV: [-0.38468817  0.32981244 -0.98575014]\n",
      "Reward for action 20: -131.00182240223447\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 600\n",
      "Steps done: 4271\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 2: -0.38053201760950256\n",
      "[0, 1, 2]\n",
      "Steps done: 4272\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 4: -1.7889621636846815\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 4273\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 9: -0.6749485236316903\n",
      "[0, 1, 2, 4, 9]\n",
      "Steps done: 4274\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 19: -1.7889621636846815\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 4275\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 7: -0.9181885386140459\n",
      "[0, 1, 2, 4, 7]\n",
      "Steps done: 4276\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 6: -7.02208991850473\n",
      "[0, 1, 2, 4, 7, 6]\n",
      "Steps done: 4277\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 12: -0.5941196941207898\n",
      "[0, 1, 4, 7, 6]\n",
      "Steps done: 4278\n",
      "SV: [0.06875577 0.0750307  0.05535115]\n",
      "Reward for action 20: 3.40588030587921\n",
      "[0, 1, 4, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 601\n",
      "Steps done: 4279\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 8: -29.18789736633095\n",
      "[0, 1, 8]\n",
      "Steps done: 4280\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 9: -36.15432483840207\n",
      "[0, 1, 8, 9]\n",
      "Steps done: 4281\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 4: -32.409622557667035\n",
      "[0, 1, 8, 9, 4]\n",
      "Steps done: 4282\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 10: -34.08869979224518\n",
      "[1, 8, 9, 4]\n",
      "Steps done: 4283\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 2: -33.60080207561602\n",
      "[1, 8, 9, 4, 2]\n",
      "Steps done: 4284\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 7: -32.166831268839864\n",
      "[1, 8, 9, 4, 2, 7]\n",
      "Steps done: 4285\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 14: -34.569064045114935\n",
      "[1, 8, 9, 2, 7]\n",
      "Steps done: 4286\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 11: -32.94942578969599\n",
      "[8, 9, 2, 7]\n",
      "Steps done: 4287\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 6: -35.805126075319734\n",
      "[8, 9, 2, 7, 6]\n",
      "Steps done: 4288\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 16: -32.94942578969599\n",
      "[8, 9, 2, 7]\n",
      "Steps done: 4289\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 1: -34.56906404511493\n",
      "[8, 9, 2, 7, 1]\n",
      "Steps done: 4290\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 12: -37.97319529383814\n",
      "[8, 9, 7, 1]\n",
      "Steps done: 4291\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 2: -34.569064045114914\n",
      "[8, 9, 7, 1, 2]\n",
      "Steps done: 4292\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 12: -37.97319529383814\n",
      "[8, 9, 7, 1]\n",
      "Steps done: 4293\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 11: -35.46769178592393\n",
      "[8, 9, 7]\n",
      "Steps done: 4294\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 1: -37.97319529383814\n",
      "[8, 9, 7, 1]\n",
      "Steps done: 4295\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 11: -35.46769178592393\n",
      "[8, 9, 7]\n",
      "Steps done: 4296\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 5: -40.71552179047542\n",
      "[8, 9, 7, 5]\n",
      "Steps done: 4297\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 1: -41.57566372891785\n",
      "[8, 9, 7, 5, 1]\n",
      "Steps done: 4298\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 11: -40.71552179047542\n",
      "[8, 9, 7, 5]\n",
      "Steps done: 4299\n",
      "SV: [ 0.08275062  0.26455104 -0.5414133 ]\n",
      "Reward for action 20: -36.71552179047542\n",
      "[8, 9, 7, 5]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 603\n",
      "Steps done: 4300\n",
      "SV: [ 0.47786218 -0.61828506  0.5990812 ]\n",
      "Reward for action 2: -14.087219023139113\n",
      "[0, 1, 2]\n",
      "Steps done: 4301\n",
      "SV: [ 0.47786218 -0.61828506  0.5990812 ]\n",
      "Reward for action 4: -11.13972368302944\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 4302\n",
      "SV: [ 0.47786218 -0.61828506  0.5990812 ]\n",
      "Reward for action 14: -14.087219023139113\n",
      "[0, 1, 2]\n",
      "Steps done: 4303\n",
      "SV: [ 0.47786218 -0.61828506  0.5990812 ]\n",
      "Reward for action 11: -132.74829825759807\n",
      "[0, 2]\n",
      "Steps done: 4304\n",
      "SV: [ 0.47786218 -0.61828506  0.5990812 ]\n",
      "Reward for action 20: -128.74829825759807\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 604\n",
      "Steps done: 4305\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 5: -80.67441561441547\n",
      "[0, 1, 5]\n",
      "Steps done: 4306\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 7: -44.132305939927164\n",
      "[0, 1, 5, 7]\n",
      "Steps done: 4307\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 2: -8.9491218773034\n",
      "[0, 1, 5, 7, 2]\n",
      "Steps done: 4308\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 9: -18.498223161451865\n",
      "[0, 1, 5, 7, 2, 9]\n",
      "Steps done: 4309\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 6: -20.758788015310714\n",
      "[0, 1, 5, 7, 2, 9, 6]\n",
      "Steps done: 4310\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 16: -18.498223161451865\n",
      "[0, 1, 5, 7, 2, 9]\n",
      "Steps done: 4311\n",
      "SV: [-0.34745356 -0.3029198  -0.558124  ]\n",
      "Reward for action 20: -14.498223161451865\n",
      "[0, 1, 5, 7, 2, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 605\n",
      "Steps done: 4312\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 7: -33.541735764296625\n",
      "[0, 1, 7]\n",
      "Steps done: 4313\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 3: -30.97063143770292\n",
      "[0, 1, 7, 3]\n",
      "Steps done: 4314\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 13: -33.541735764296625\n",
      "[0, 1, 7]\n",
      "Steps done: 4315\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 10: -33.612304941783904\n",
      "[1, 7]\n",
      "Steps done: 4316\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 2: -41.57196261765223\n",
      "[1, 7, 2]\n",
      "Steps done: 4317\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 8: -37.725510868764864\n",
      "[1, 7, 2, 8]\n",
      "Steps done: 4318\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 17: -39.61070831972995\n",
      "[1, 2, 8]\n",
      "Steps done: 4319\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 9: -38.8943659300239\n",
      "[1, 2, 8, 9]\n",
      "Steps done: 4320\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 11: -37.3630369214013\n",
      "[2, 8, 9]\n",
      "Steps done: 4321\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 7: -37.455481983202766\n",
      "[2, 8, 9, 7]\n",
      "Steps done: 4322\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 12: -36.5925907601238\n",
      "[8, 9, 7]\n",
      "Steps done: 4323\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 1: -36.019242487880376\n",
      "[8, 9, 7, 1]\n",
      "Steps done: 4324\n",
      "SV: [-0.2834363 -0.4287111 -0.3305135]\n",
      "Reward for action 20: -32.019242487880376\n",
      "[8, 9, 7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 606\n",
      "Steps done: 4325\n",
      "SV: [-0.2274922  0.4029956 -0.6089656]\n",
      "Reward for action 4: -36.486838015223455\n",
      "[0, 1, 4]\n",
      "Steps done: 4326\n",
      "SV: [-0.2274922  0.4029956 -0.6089656]\n",
      "Reward for action 3: -32.527304307031926\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 4327\n",
      "SV: [-0.2274922  0.4029956 -0.6089656]\n",
      "Reward for action 20: -28.527304307031926\n",
      "[0, 1, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 607\n",
      "Steps done: 4328\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 7: -125.33747169448603\n",
      "[0, 1, 7]\n",
      "Steps done: 4329\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 5: -91.78593582010271\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 4330\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 8: -97.74384113467579\n",
      "[0, 1, 7, 5, 8]\n",
      "Steps done: 4331\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 15: -87.4363024426841\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 4332\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 2: -91.15741624773791\n",
      "[0, 1, 7, 8, 2]\n",
      "Steps done: 4333\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 11: -89.31411085480457\n",
      "[0, 7, 8, 2]\n",
      "Steps done: 4334\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 6: -108.67513038953398\n",
      "[0, 7, 8, 2, 6]\n",
      "Steps done: 4335\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 16: -89.31411085480457\n",
      "[0, 7, 8, 2]\n",
      "Steps done: 4336\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 3: -93.12342670629961\n",
      "[0, 7, 8, 2, 3]\n",
      "Steps done: 4337\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 13: -89.31411085480457\n",
      "[0, 7, 8, 2]\n",
      "Steps done: 4338\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 3: -93.12342670629961\n",
      "[0, 7, 8, 2, 3]\n",
      "Steps done: 4339\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 13: -89.31411085480457\n",
      "[0, 7, 8, 2]\n",
      "Steps done: 4340\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 17: -88.78312780141343\n",
      "[0, 8, 2]\n",
      "Steps done: 4341\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 5: -109.44476704482828\n",
      "[0, 8, 2, 5]\n",
      "Steps done: 4342\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 12: -132.03427230853134\n",
      "[0, 8, 5]\n",
      "Steps done: 4343\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 15: -79.5653877082637\n",
      "[0, 8]\n",
      "Steps done: 4344\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 5: -132.03427230853134\n",
      "[0, 8, 5]\n",
      "Steps done: 4345\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 15: -79.5653877082637\n",
      "[0, 8]\n",
      "Steps done: 4346\n",
      "SV: [-0.0230915  0.5136575 -0.9937959]\n",
      "Reward for action 20: -75.5653877082637\n",
      "[0, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 608\n",
      "Steps done: 4347\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 2: -8.682684967004693\n",
      "[0, 1, 2]\n",
      "Steps done: 4348\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 10: -19.33873545152328\n",
      "[1, 2]\n",
      "Steps done: 4349\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 0: -8.682684967004692\n",
      "[1, 2, 0]\n",
      "Steps done: 4350\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 5: -6.8472464691580095\n",
      "[1, 2, 0, 5]\n",
      "Steps done: 4351\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 15: -8.682684967004692\n",
      "[1, 2, 0]\n",
      "Steps done: 4352\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 10: -19.33873545152328\n",
      "[1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4353\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 4: -22.08272665591377\n",
      "[1, 2, 4]\n",
      "Steps done: 4354\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 0: -11.89063860002344\n",
      "[1, 2, 4, 0]\n",
      "Steps done: 4355\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 14: -8.682684967004692\n",
      "[1, 2, 0]\n",
      "Steps done: 4356\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 5: -6.8472464691580095\n",
      "[1, 2, 0, 5]\n",
      "Steps done: 4357\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 11: -6.399439788806952\n",
      "[2, 0, 5]\n",
      "Steps done: 4358\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 4: -7.536039366754004\n",
      "[2, 0, 5, 4]\n",
      "Steps done: 4359\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 10: -9.666356088253332\n",
      "[2, 5, 4]\n",
      "Steps done: 4360\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 3: -7.800251234842737\n",
      "[2, 5, 4, 3]\n",
      "Steps done: 4361\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 0: -6.837472463103721\n",
      "[2, 5, 4, 3, 0]\n",
      "Steps done: 4362\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 10: -7.800251234842737\n",
      "[2, 5, 4, 3]\n",
      "Steps done: 4363\n",
      "SV: [0.18379109 0.13385333 0.20845436]\n",
      "Reward for action 20: -3.8002512348427366\n",
      "[2, 5, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 609\n",
      "Steps done: 4364\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 3: -4.784197255302845\n",
      "[0, 1, 3]\n",
      "Steps done: 4365\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 13: -2.1151857224641186\n",
      "[0, 1]\n",
      "Steps done: 4366\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 5: -2.3751159456411384\n",
      "[0, 1, 5]\n",
      "Steps done: 4367\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 7: -2.1642941441668446\n",
      "[0, 1, 5, 7]\n",
      "Steps done: 4368\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 8: -4.760598084517198\n",
      "[0, 1, 5, 7, 8]\n",
      "Steps done: 4369\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 17: -6.079173855587166\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 4370\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 10: -6.68118685026133\n",
      "[1, 5, 8]\n",
      "Steps done: 4371\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 11: -51.64443491521284\n",
      "[5, 8]\n",
      "Steps done: 4372\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 0: -19.952124226955064\n",
      "[5, 8, 0]\n",
      "Steps done: 4373\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 2: -12.214387816851374\n",
      "[5, 8, 0, 2]\n",
      "Steps done: 4374\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 15: -8.314524474049687\n",
      "[8, 0, 2]\n",
      "Steps done: 4375\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 6: -5.358855627537003\n",
      "[8, 0, 2, 6]\n",
      "Steps done: 4376\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 16: -8.314524474049687\n",
      "[8, 0, 2]\n",
      "Steps done: 4377\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 6: -5.358855627537003\n",
      "[8, 0, 2, 6]\n",
      "Steps done: 4378\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 3: -2.2384132676691566\n",
      "[8, 0, 2, 6, 3]\n",
      "Steps done: 4379\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 16: -2.13486595265753\n",
      "[8, 0, 2, 3]\n",
      "Steps done: 4380\n",
      "SV: [0.13411316 0.05548387 0.07439049]\n",
      "Reward for action 20: 1.8651340473424698\n",
      "[8, 0, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 610\n",
      "Steps done: 4381\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 4: -40.64645835017756\n",
      "[0, 1, 4]\n",
      "Steps done: 4382\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 8: -30.691898966416936\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 4383\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 10: -29.776414945819432\n",
      "[1, 4, 8]\n",
      "Steps done: 4384\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 5: -35.85650239448583\n",
      "[1, 4, 8, 5]\n",
      "Steps done: 4385\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 0: -34.17262178106846\n",
      "[1, 4, 8, 5, 0]\n",
      "Steps done: 4386\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 7: -33.52504942215558\n",
      "[1, 4, 8, 5, 0, 7]\n",
      "Steps done: 4387\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 14: -29.12581353552558\n",
      "[1, 8, 5, 0, 7]\n",
      "Steps done: 4388\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 17: -27.679964168386515\n",
      "[1, 8, 5, 0]\n",
      "Steps done: 4389\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 6: -30.566215796655573\n",
      "[1, 8, 5, 0, 6]\n",
      "Steps done: 4390\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 7: -30.4854630742336\n",
      "[1, 8, 5, 0, 6, 7]\n",
      "Steps done: 4391\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 18: -34.98588907943996\n",
      "[1, 5, 0, 6, 7]\n",
      "Steps done: 4392\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 3: -30.545860825525995\n",
      "[1, 5, 0, 6, 7, 3]\n",
      "Steps done: 4393\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 11: -26.580611425856464\n",
      "[5, 0, 6, 7, 3]\n",
      "Steps done: 4394\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 4: -30.95870069775516\n",
      "[5, 0, 6, 7, 3, 4]\n",
      "Steps done: 4395\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 14: -26.580611425856464\n",
      "[5, 0, 6, 7, 3]\n",
      "Steps done: 4396\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 15: -23.767999784344646\n",
      "[0, 6, 7, 3]\n",
      "Steps done: 4397\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 5: -26.580611425856457\n",
      "[0, 6, 7, 3, 5]\n",
      "Steps done: 4398\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 4: -30.95870069775516\n",
      "[0, 6, 7, 3, 5, 4]\n",
      "Steps done: 4399\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 1: -33.428536653499464\n",
      "[0, 6, 7, 3, 5, 4, 1]\n",
      "Did target update\n",
      "Steps done: 4400\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 14: -30.545860825526017\n",
      "[0, 6, 7, 3, 5, 1]\n",
      "Steps done: 4401\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 4: -33.42853665349947\n",
      "[0, 6, 7, 3, 5, 1, 4]\n",
      "Steps done: 4402\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 17: -35.38585202632287\n",
      "[0, 6, 3, 5, 1, 4]\n",
      "Steps done: 4403\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 13: -41.52511305797314\n",
      "[0, 6, 5, 1, 4]\n",
      "Steps done: 4404\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 7: -38.716452674388066\n",
      "[0, 6, 5, 1, 4, 7]\n",
      "Steps done: 4405\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 16: -41.71325558126353\n",
      "[0, 5, 1, 4, 7]\n",
      "Steps done: 4406\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Reward for action 11: -44.509695438824615\n",
      "[0, 5, 4, 7]\n",
      "Steps done: 4407\n",
      "SV: [ 0.16817336  0.1452729  -0.7030997 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -40.509695438824615\n",
      "[0, 5, 4, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 611\n",
      "Steps done: 4408\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 5: -19.483288219503947\n",
      "[0, 1, 5]\n",
      "Steps done: 4409\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 15: -94.53965088717963\n",
      "[0, 1]\n",
      "Steps done: 4410\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 4: -49.45706975498221\n",
      "[0, 1, 4]\n",
      "Steps done: 4411\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 10: -237.312864408046\n",
      "[1, 4]\n",
      "Steps done: 4412\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 0: -49.45706975498221\n",
      "[1, 4, 0]\n",
      "Steps done: 4413\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 5: -71.05485599535605\n",
      "[1, 4, 0, 5]\n",
      "Steps done: 4414\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 11: -144.72399781511606\n",
      "[4, 0, 5]\n",
      "Steps done: 4415\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 14: -106.07189986076484\n",
      "[0, 5]\n",
      "Steps done: 4416\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 6: -13.272415986415616\n",
      "[0, 5, 6]\n",
      "Steps done: 4417\n",
      "SV: [ 2.6275547  -1.4483155   0.38684103]\n",
      "Reward for action 20: -9.272415986415616\n",
      "[0, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 612\n",
      "Steps done: 4418\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 8: -21.198533856181026\n",
      "[0, 1, 8]\n",
      "Steps done: 4419\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 2: -22.277194296208258\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 4420\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 3: -19.448590781685485\n",
      "[0, 1, 8, 2, 3]\n",
      "Steps done: 4421\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 18: -20.791092086991604\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 4422\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 13: -27.932110457569717\n",
      "[0, 1, 2]\n",
      "Steps done: 4423\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 10: -40.62394790202955\n",
      "[1, 2]\n",
      "Steps done: 4424\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 9: -27.835085966953603\n",
      "[1, 2, 9]\n",
      "Steps done: 4425\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 6: -21.15466501062529\n",
      "[1, 2, 9, 6]\n",
      "Steps done: 4426\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 12: -19.51478145773717\n",
      "[1, 9, 6]\n",
      "Steps done: 4427\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 19: -14.756427284236263\n",
      "[1, 6]\n",
      "Steps done: 4428\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 5: -16.07284860780471\n",
      "[1, 6, 5]\n",
      "Steps done: 4429\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 2: -18.029641493707743\n",
      "[1, 6, 5, 2]\n",
      "Steps done: 4430\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 9: -20.03724435471431\n",
      "[1, 6, 5, 2, 9]\n",
      "Steps done: 4431\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 12: -19.277143909105508\n",
      "[1, 6, 5, 9]\n",
      "Steps done: 4432\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 8: -19.704671243678032\n",
      "[1, 6, 5, 9, 8]\n",
      "Steps done: 4433\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 7: -19.774017577007605\n",
      "[1, 6, 5, 9, 8, 7]\n",
      "Steps done: 4434\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 18: -20.103097446201396\n",
      "[1, 6, 5, 9, 7]\n",
      "Steps done: 4435\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 8: -19.774017577007605\n",
      "[1, 6, 5, 9, 7, 8]\n",
      "Steps done: 4436\n",
      "SV: [0.09275763 0.12801671 0.511661  ]\n",
      "Reward for action 20: -15.774017577007605\n",
      "[1, 6, 5, 9, 7, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 613\n",
      "Steps done: 4437\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 3: -47.1293227851781\n",
      "[0, 1, 3]\n",
      "Steps done: 4438\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 13: -71.82792827453471\n",
      "[0, 1]\n",
      "Steps done: 4439\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 3: -47.1293227851781\n",
      "[0, 1, 3]\n",
      "Steps done: 4440\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 11: -164.3908848945131\n",
      "[0, 3]\n",
      "Steps done: 4441\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 4: -67.80538068203015\n",
      "[0, 3, 4]\n",
      "Steps done: 4442\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 14: -164.3908848945131\n",
      "[0, 3]\n",
      "Steps done: 4443\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 2: -35.30624881587911\n",
      "[0, 3, 2]\n",
      "Steps done: 4444\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 13: -142.04173041355102\n",
      "[0, 2]\n",
      "Steps done: 4445\n",
      "SV: [ 0.55768603  0.07972156 -1.1797425 ]\n",
      "Reward for action 20: -138.04173041355102\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 614\n",
      "Steps done: 4446\n",
      "SV: [-0.45122352 -0.62690127 -0.4525791 ]\n",
      "Reward for action 6: -70.10148675044927\n",
      "[0, 1, 6]\n",
      "Steps done: 4447\n",
      "SV: [-0.45122352 -0.62690127 -0.4525791 ]\n",
      "Reward for action 10: -77.79339787802974\n",
      "[1, 6]\n",
      "Steps done: 4448\n",
      "SV: [-0.45122352 -0.62690127 -0.4525791 ]\n",
      "Reward for action 20: -73.79339787802974\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 615\n",
      "Steps done: 4449\n",
      "SV: [-0.13743374  1.0634023   1.7193421 ]\n",
      "Reward for action 20: -356.5994446593958\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 616\n",
      "Steps done: 4450\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 4: -13.127465513698663\n",
      "[0, 1, 4]\n",
      "Steps done: 4451\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 7: -3.677209791347502\n",
      "[0, 1, 4, 7]\n",
      "Steps done: 4452\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 6: -6.010698690836191\n",
      "[0, 1, 4, 7, 6]\n",
      "Steps done: 4453\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 8: -1.8719049825499312\n",
      "[0, 1, 4, 7, 6, 8]\n",
      "Steps done: 4454\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 16: -2.5154177657250214\n",
      "[0, 1, 4, 7, 8]\n",
      "Steps done: 4455\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 3: -0.9714291620745845\n",
      "[0, 1, 4, 7, 8, 3]\n",
      "Steps done: 4456\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 11: -1.9996297606607525\n",
      "[0, 4, 7, 8, 3]\n",
      "Steps done: 4457\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 6: -1.7210820193258836\n",
      "[0, 4, 7, 8, 3, 6]\n",
      "Steps done: 4458\n",
      "SV: [ 0.6402791  -0.8240432   0.06680128]\n",
      "Reward for action 20: 2.2789179806741164\n",
      "[0, 4, 7, 8, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 617\n",
      "Steps done: 4459\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 5: -11.846072217712793\n",
      "[0, 1, 5]\n",
      "Steps done: 4460\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 6: -12.882006050661778\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 4461\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 10: -12.604841004406033\n",
      "[1, 5, 6]\n",
      "Steps done: 4462\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 0: -12.882006050661783\n",
      "[1, 5, 6, 0]\n",
      "Steps done: 4463\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 16: -11.846072217712795\n",
      "[1, 5, 0]\n",
      "Steps done: 4464\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 7: -12.59729011697815\n",
      "[1, 5, 0, 7]\n",
      "Steps done: 4465\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 2: -13.01237164157214\n",
      "[1, 5, 0, 7, 2]\n",
      "Steps done: 4466\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 17: -12.666818454360316\n",
      "[1, 5, 0, 2]\n",
      "Steps done: 4467\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 4: -12.574081607569566\n",
      "[1, 5, 0, 2, 4]\n",
      "Steps done: 4468\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 12: -12.200560418320856\n",
      "[1, 5, 0, 4]\n",
      "Steps done: 4469\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 10: -11.3255707271059\n",
      "[1, 5, 4]\n",
      "Steps done: 4470\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 14: -10.251527385078802\n",
      "[1, 5]\n",
      "Steps done: 4471\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 3: -12.139158467682774\n",
      "[1, 5, 3]\n",
      "Steps done: 4472\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 0: -12.462838310945275\n",
      "[1, 5, 3, 0]\n",
      "Steps done: 4473\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 11: -13.109817475689027\n",
      "[5, 3, 0]\n",
      "Steps done: 4474\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 10: -13.898168548337356\n",
      "[5, 3]\n",
      "Steps done: 4475\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 0: -13.109817475689027\n",
      "[5, 3, 0]\n",
      "Steps done: 4476\n",
      "SV: [ 0.00750192  0.02521363 -0.41173306]\n",
      "Reward for action 20: -9.109817475689027\n",
      "[5, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 618\n",
      "Steps done: 4477\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 4: -0.15363727008364936\n",
      "[0, 1, 4]\n",
      "Steps done: 4478\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 6: -247.49960694502244\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 4479\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 10: -448.44090356917656\n",
      "[1, 4, 6]\n",
      "Steps done: 4480\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 14: -1045.8889146262813\n",
      "[1, 6]\n",
      "Steps done: 4481\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 4: -448.44090356917656\n",
      "[1, 6, 4]\n",
      "Steps done: 4482\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 16: -0.22475867291440954\n",
      "[1, 4]\n",
      "Steps done: 4483\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 3: -0.36967807218060483\n",
      "[1, 4, 3]\n",
      "Steps done: 4484\n",
      "SV: [ 0.01638327  0.01541376 -0.02939351]\n",
      "Reward for action 20: 3.630321927819395\n",
      "[1, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 619\n",
      "Steps done: 4485\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 7: -1.8727851337443324\n",
      "[0, 1, 7]\n",
      "Steps done: 4486\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 8: -53.7309936796796\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 4487\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 5: -73.1327700144179\n",
      "[0, 1, 7, 8, 5]\n",
      "Steps done: 4488\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 11: -138.64531795306968\n",
      "[0, 7, 8, 5]\n",
      "Steps done: 4489\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 4: -66.2604848163923\n",
      "[0, 7, 8, 5, 4]\n",
      "Steps done: 4490\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 2: -54.7519131370938\n",
      "[0, 7, 8, 5, 4, 2]\n",
      "Steps done: 4491\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 12: -66.2604848163923\n",
      "[0, 7, 8, 5, 4]\n",
      "Steps done: 4492\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 2: -54.7519131370938\n",
      "[0, 7, 8, 5, 4, 2]\n",
      "Steps done: 4493\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 10: -79.47843354372614\n",
      "[7, 8, 5, 4, 2]\n",
      "Steps done: 4494\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 15: -61.76816396423211\n",
      "[7, 8, 4, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4495\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 12: -52.698277830317906\n",
      "[7, 8, 4]\n",
      "Steps done: 4496\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 2: -61.76816396423211\n",
      "[7, 8, 4, 2]\n",
      "Steps done: 4497\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 12: -52.698277830317906\n",
      "[7, 8, 4]\n",
      "Steps done: 4498\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 2: -61.76816396423211\n",
      "[7, 8, 4, 2]\n",
      "Steps done: 4499\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 12: -52.698277830317906\n",
      "[7, 8, 4]\n",
      "Did target update\n",
      "Steps done: 4500\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 0: -40.7439578767869\n",
      "[7, 8, 4, 0]\n",
      "Steps done: 4501\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 17: -102.98436339111677\n",
      "[8, 4, 0]\n",
      "Steps done: 4502\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 7: -40.7439578767869\n",
      "[8, 4, 0, 7]\n",
      "Steps done: 4503\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 1: -33.96263953588503\n",
      "[8, 4, 0, 7, 1]\n",
      "Steps done: 4504\n",
      "SV: [ 0.160484    0.04278533 -1.104081  ]\n",
      "Reward for action 20: -29.96263953588503\n",
      "[8, 4, 0, 7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 620\n",
      "Steps done: 4505\n",
      "SV: [ 2.794033  -0.9820142  3.9169583]\n",
      "Reward for action 20: -52.49552121112097\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 621\n",
      "Steps done: 4506\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 2: -3.9941033897741267\n",
      "[0, 1, 2]\n",
      "Steps done: 4507\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 12: -3.965941042570695\n",
      "[0, 1]\n",
      "Steps done: 4508\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 2: -3.9941033897741267\n",
      "[0, 1, 2]\n",
      "Steps done: 4509\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 5: -3.987567252631698\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 4510\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 15: -3.9941033897741267\n",
      "[0, 1, 2]\n",
      "Steps done: 4511\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 10: -4.193646909007101\n",
      "[1, 2]\n",
      "Steps done: 4512\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 0: -3.9941033897741267\n",
      "[1, 2, 0]\n",
      "Steps done: 4513\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 4: -3.977126440970537\n",
      "[1, 2, 0, 4]\n",
      "Steps done: 4514\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 11: -3.902864782299787\n",
      "[2, 0, 4]\n",
      "Steps done: 4515\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 6: -6.3539237311632775\n",
      "[2, 0, 4, 6]\n",
      "Steps done: 4516\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 10: -6.965946604490815\n",
      "[2, 4, 6]\n",
      "Steps done: 4517\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 14: -9.229710318716963\n",
      "[2, 6]\n",
      "Steps done: 4518\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 3: -4.59659721505465\n",
      "[2, 6, 3]\n",
      "Steps done: 4519\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 13: -9.229710318716963\n",
      "[2, 6]\n",
      "Steps done: 4520\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 1: -4.591416973274589\n",
      "[2, 6, 1]\n",
      "Steps done: 4521\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 3: -4.43640834343506\n",
      "[2, 6, 1, 3]\n",
      "Steps done: 4522\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 16: -4.419884758576564\n",
      "[2, 1, 3]\n",
      "Steps done: 4523\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 6: -4.43640834343506\n",
      "[2, 1, 3, 6]\n",
      "Steps done: 4524\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 13: -4.591416973274587\n",
      "[2, 1, 6]\n",
      "Steps done: 4525\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 11: -9.229710318716963\n",
      "[2, 6]\n",
      "Steps done: 4526\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 1: -4.591416973274589\n",
      "[2, 6, 1]\n",
      "Steps done: 4527\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 4: -4.684893287841755\n",
      "[2, 6, 1, 4]\n",
      "Steps done: 4528\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 12: -4.907076228587251\n",
      "[6, 1, 4]\n",
      "Steps done: 4529\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 0: -5.312749323707347\n",
      "[6, 1, 4, 0]\n",
      "Steps done: 4530\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 5: -5.3728049034207\n",
      "[6, 1, 4, 0, 5]\n",
      "Steps done: 4531\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Reward for action 11: -8.153574127766468\n",
      "[6, 4, 0, 5]\n",
      "Steps done: 4532\n",
      "SV: [-5.6282200e-02  1.5749583e-04 -2.1649010e-01]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -4.153574127766468\n",
      "[6, 4, 0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 622\n",
      "Steps done: 4533\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 3: -128.2436144092028\n",
      "[0, 1, 3]\n",
      "Steps done: 4534\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 11: -248.09547194538754\n",
      "[0, 3]\n",
      "Steps done: 4535\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 6: -223.24418797309787\n",
      "[0, 3, 6]\n",
      "Steps done: 4536\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 2: -47.85402806496366\n",
      "[0, 3, 6, 2]\n",
      "Steps done: 4537\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 12: -223.24418797309787\n",
      "[0, 3, 6]\n",
      "Steps done: 4538\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 4: -61.44034771990397\n",
      "[0, 3, 6, 4]\n",
      "Steps done: 4539\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 13: -33.985576221205704\n",
      "[0, 6, 4]\n",
      "Steps done: 4540\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 10: -10.700951540739526\n",
      "[6, 4]\n",
      "Steps done: 4541\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 0: -33.9855762212057\n",
      "[6, 4, 0]\n",
      "Steps done: 4542\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 1: -19.779273161458228\n",
      "[6, 4, 0, 1]\n",
      "Steps done: 4543\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 16: -3.636807976084763\n",
      "[4, 0, 1]\n",
      "Steps done: 4544\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 5: -2.2446536123629612\n",
      "[4, 0, 1, 5]\n",
      "Steps done: 4545\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 11: -0.6098348101055431\n",
      "[4, 0, 5]\n",
      "Steps done: 4546\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 10: -2.325238500820718\n",
      "[4, 5]\n",
      "Steps done: 4547\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 1: -10.494719840200682\n",
      "[4, 5, 1]\n",
      "Steps done: 4548\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 6: -15.816746434270701\n",
      "[4, 5, 1, 6]\n",
      "Steps done: 4549\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 11: -16.816667458215328\n",
      "[4, 5, 6]\n",
      "Steps done: 4550\n",
      "SV: [ 0.24149545 -0.07055689 -0.272077  ]\n",
      "Reward for action 20: -12.816667458215328\n",
      "[4, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 623\n",
      "Steps done: 4551\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 8: -1.0141554781246975\n",
      "[0, 1, 8]\n",
      "Steps done: 4552\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 5: -1.0736268582328765\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 4553\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 6: -0.9347752274391189\n",
      "[0, 1, 8, 5, 6]\n",
      "Steps done: 4554\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 16: -1.0736268582328765\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 4555\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 10: -1.2646435194078842\n",
      "[1, 8, 5]\n",
      "Steps done: 4556\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 0: -1.0736268582328765\n",
      "[1, 8, 5, 0]\n",
      "Steps done: 4557\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 3: -0.9508071233657804\n",
      "[1, 8, 5, 0, 3]\n",
      "Steps done: 4558\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 2: -0.8838949066749877\n",
      "[1, 8, 5, 0, 3, 2]\n",
      "Steps done: 4559\n",
      "SV: [ 0.01651338  0.00638902 -0.099958  ]\n",
      "Reward for action 20: 3.1161050933250123\n",
      "[1, 8, 5, 0, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 624\n",
      "Steps done: 4560\n",
      "SV: [-1.734286  -2.5081651  3.0636547]\n",
      "Reward for action 2: -231.66883509781246\n",
      "[0, 1, 2]\n",
      "Steps done: 4561\n",
      "SV: [-1.734286  -2.5081651  3.0636547]\n",
      "Reward for action 12: -239.22624467442242\n",
      "[0, 1]\n",
      "Steps done: 4562\n",
      "SV: [-1.734286  -2.5081651  3.0636547]\n",
      "Reward for action 2: -231.66883509781246\n",
      "[0, 1, 2]\n",
      "Steps done: 4563\n",
      "SV: [-1.734286  -2.5081651  3.0636547]\n",
      "Reward for action 20: -227.66883509781246\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 625\n",
      "Steps done: 4564\n",
      "SV: [-0.13609089  0.23421393  0.5568061 ]\n",
      "Reward for action 2: -32.18532234330618\n",
      "[0, 1, 2]\n",
      "Steps done: 4565\n",
      "SV: [-0.13609089  0.23421393  0.5568061 ]\n",
      "Reward for action 10: -39.71077050299981\n",
      "[1, 2]\n",
      "Steps done: 4566\n",
      "SV: [-0.13609089  0.23421393  0.5568061 ]\n",
      "Reward for action 3: -18.264693348637906\n",
      "[1, 2, 3]\n",
      "Steps done: 4567\n",
      "SV: [-0.13609089  0.23421393  0.5568061 ]\n",
      "Reward for action 12: -21.204394827546704\n",
      "[1, 3]\n",
      "Steps done: 4568\n",
      "SV: [-0.13609089  0.23421393  0.5568061 ]\n",
      "Reward for action 20: -17.204394827546704\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 626\n",
      "Steps done: 4569\n",
      "SV: [ 0.71536434  0.46158203 -1.7460018 ]\n",
      "Reward for action 2: -310.6946826920663\n",
      "[0, 1, 2]\n",
      "Steps done: 4570\n",
      "SV: [ 0.71536434  0.46158203 -1.7460018 ]\n",
      "Reward for action 12: -317.1599538204548\n",
      "[0, 1]\n",
      "Steps done: 4571\n",
      "SV: [ 0.71536434  0.46158203 -1.7460018 ]\n",
      "Reward for action 20: -313.1599538204548\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 627\n",
      "Steps done: 4572\n",
      "SV: [ 0.24438734  0.22211662 -1.2702106 ]\n",
      "Reward for action 5: -71.39211950765815\n",
      "[0, 1, 5]\n",
      "Steps done: 4573\n",
      "SV: [ 0.24438734  0.22211662 -1.2702106 ]\n",
      "Reward for action 11: -40.51952071343347\n",
      "[0, 5]\n",
      "Steps done: 4574\n",
      "SV: [ 0.24438734  0.22211662 -1.2702106 ]\n",
      "Reward for action 2: -35.297625563342166\n",
      "[0, 5, 2]\n",
      "Steps done: 4575\n",
      "SV: [ 0.24438734  0.22211662 -1.2702106 ]\n",
      "Reward for action 10: -7.13163477274983\n",
      "[5, 2]\n",
      "Steps done: 4576\n",
      "SV: [ 0.24438734  0.22211662 -1.2702106 ]\n",
      "Reward for action 20: -3.1316347727498304\n",
      "[5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 628\n",
      "Steps done: 4577\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 2: -10.680703860227206\n",
      "[0, 1, 2]\n",
      "Steps done: 4578\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 3: -54.41324298620037\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 4579\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 12: -104.44269562859046\n",
      "[0, 1, 3]\n",
      "Steps done: 4580\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 10: -98.65094136488358\n",
      "[1, 3]\n",
      "Steps done: 4581\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 2: -9.580148277545819\n",
      "[1, 3, 2]\n",
      "Steps done: 4582\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 12: -98.65094136488358\n",
      "[1, 3]\n",
      "Steps done: 4583\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 2: -9.580148277545819\n",
      "[1, 3, 2]\n",
      "Steps done: 4584\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 13: -234.10646642504156\n",
      "[1, 2]\n",
      "Steps done: 4585\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 3: -9.580148277545819\n",
      "[1, 2, 3]\n",
      "Steps done: 4586\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 11: -208.34290279241287\n",
      "[2, 3]\n",
      "Steps done: 4587\n",
      "SV: [-0.1520037   0.71194315  0.9070023 ]\n",
      "Reward for action 20: -204.34290279241287\n",
      "[2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 629\n",
      "Steps done: 4588\n",
      "SV: [ 0.7714813   0.06612586 -0.44246945]\n",
      "Reward for action 20: -85.99175136246842\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 630\n",
      "Steps done: 4589\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 8: -103.27678539519533\n",
      "[0, 1, 8]\n",
      "Steps done: 4590\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 11: -220.5710591212817\n",
      "[0, 8]\n",
      "Steps done: 4591\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 6: -75.81513379751624\n",
      "[0, 8, 6]\n",
      "Steps done: 4592\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 10: -47.11110304855778\n",
      "[8, 6]\n",
      "Steps done: 4593\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 7: -85.66923962871616\n",
      "[8, 6, 7]\n",
      "Steps done: 4594\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 16: -213.95225334611905\n",
      "[8, 7]\n",
      "Steps done: 4595\n",
      "SV: [0.11396767 0.27113456 0.45791164]\n",
      "Reward for action 20: -209.95225334611905\n",
      "[8, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 631\n",
      "Steps done: 4596\n",
      "SV: [ 0.00276253  0.03318447 -1.3072807 ]\n",
      "Reward for action 2: -147.850874652048\n",
      "[0, 1, 2]\n",
      "Steps done: 4597\n",
      "SV: [ 0.00276253  0.03318447 -1.3072807 ]\n",
      "Reward for action 5: -81.04191705634432\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 4598\n",
      "SV: [ 0.00276253  0.03318447 -1.3072807 ]\n",
      "Reward for action 20: -77.04191705634432\n",
      "[0, 1, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 632\n",
      "Steps done: 4599\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 5: -487.0882919881815\n",
      "[0, 1, 5]\n",
      "Did target update\n",
      "Steps done: 4600\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 2: -591.4958461977083\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 4601\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 4: -235.88179954834283\n",
      "[0, 1, 5, 2, 4]\n",
      "Steps done: 4602\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 3: -32.00077473306042\n",
      "[0, 1, 5, 2, 4, 3]\n",
      "Steps done: 4603\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 8: -105.02670095212609\n",
      "[0, 1, 5, 2, 4, 3, 8]\n",
      "Steps done: 4604\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 6: -185.32542007515664\n",
      "[0, 1, 5, 2, 4, 3, 8, 6]\n",
      "Steps done: 4605\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 7: -225.87877488183972\n",
      "[0, 1, 5, 2, 4, 3, 8, 6, 7]\n",
      "Steps done: 4606\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 14: -320.61641671835923\n",
      "[0, 1, 5, 2, 3, 8, 6, 7]\n",
      "Steps done: 4607\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 13: -557.033947993875\n",
      "[0, 1, 5, 2, 8, 6, 7]\n",
      "Steps done: 4608\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 15: -609.9299174054385\n",
      "[0, 1, 2, 8, 6, 7]\n",
      "Steps done: 4609\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 17: -610.6703604422313\n",
      "[0, 1, 2, 8, 6]\n",
      "Steps done: 4610\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 12: -550.8222424872429\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 4611\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 7: -561.1795628018033\n",
      "[0, 1, 8, 6, 7]\n",
      "Steps done: 4612\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 18: -527.3699564809347\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 4613\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 8: -561.179562801803\n",
      "[0, 1, 6, 7, 8]\n",
      "Steps done: 4614\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 17: -550.8222424872429\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 4615\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 3: -233.91000723502043\n",
      "[0, 1, 6, 8, 3]\n",
      "Steps done: 4616\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 7: -300.4728345720876\n",
      "[0, 1, 6, 8, 3, 7]\n",
      "Steps done: 4617\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 2: -293.71770030991604\n",
      "[0, 1, 6, 8, 3, 7, 2]\n",
      "Steps done: 4618\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 13: -609.9299174054386\n",
      "[0, 1, 6, 8, 7, 2]\n",
      "Steps done: 4619\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 12: -561.1795628018035\n",
      "[0, 1, 6, 8, 7]\n",
      "Steps done: 4620\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 16: -494.61223356499113\n",
      "[0, 1, 8, 7]\n",
      "Steps done: 4621\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 2: -581.4155206895089\n",
      "[0, 1, 8, 7, 2]\n",
      "Steps done: 4622\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 17: -573.3617977063076\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 4623\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 3: -102.27109813955846\n",
      "[0, 1, 8, 2, 3]\n",
      "Steps done: 4624\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Reward for action 11: -55.66493068088088\n",
      "[0, 8, 2, 3]\n",
      "Steps done: 4625\n",
      "SV: [-0.5804084 -0.5496319  3.5584106]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -51.66493068088088\n",
      "[0, 8, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 633\n",
      "Steps done: 4626\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 7: -161.65599387757908\n",
      "[0, 1, 7]\n",
      "Steps done: 4627\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 6: -110.58188462785915\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 4628\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 17: -38.20270359026716\n",
      "[0, 1, 6]\n",
      "Steps done: 4629\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 7: -110.58188462785908\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 4630\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 4: -114.84148864717267\n",
      "[0, 1, 6, 7, 4]\n",
      "Steps done: 4631\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 8: -58.770334334935825\n",
      "[0, 1, 6, 7, 4, 8]\n",
      "Steps done: 4632\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 11: -26.279589069525397\n",
      "[0, 6, 7, 4, 8]\n",
      "Steps done: 4633\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 3: -21.00468487160417\n",
      "[0, 6, 7, 4, 8, 3]\n",
      "Steps done: 4634\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 13: -26.279589069525397\n",
      "[0, 6, 7, 4, 8]\n",
      "Steps done: 4635\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 18: -58.996723563624165\n",
      "[0, 6, 7, 4]\n",
      "Steps done: 4636\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 8: -26.279589069525397\n",
      "[0, 6, 7, 4, 8]\n",
      "Steps done: 4637\n",
      "SV: [ 0.30800262 -0.12051814  0.14539532]\n",
      "Reward for action 20: -22.279589069525397\n",
      "[0, 6, 7, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 634\n",
      "Steps done: 4638\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 4: -471.6280042083628\n",
      "[0, 1, 4]\n",
      "Steps done: 4639\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 11: -97.20556020652506\n",
      "[0, 4]\n",
      "Steps done: 4640\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 3: -45.8943617775435\n",
      "[0, 4, 3]\n",
      "Steps done: 4641\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 10: -38.92846266043326\n",
      "[4, 3]\n",
      "Steps done: 4642\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 1: -40.09637824135605\n",
      "[4, 3, 1]\n",
      "Steps done: 4643\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 11: -38.92846266043326\n",
      "[4, 3]\n",
      "Steps done: 4644\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 1: -40.09637824135605\n",
      "[4, 3, 1]\n",
      "Steps done: 4645\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 0: -111.35203229447043\n",
      "[4, 3, 1, 0]\n",
      "Steps done: 4646\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 10: -40.09637824135605\n",
      "[4, 3, 1]\n",
      "Steps done: 4647\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 13: -627.7961291129357\n",
      "[4, 1]\n",
      "Steps done: 4648\n",
      "SV: [-0.2529908   0.23212321 -1.7403351 ]\n",
      "Reward for action 20: -623.7961291129357\n",
      "[4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 635\n",
      "Steps done: 4649\n",
      "SV: [-0.08094595  0.03492404 -0.5629852 ]\n",
      "Reward for action 3: -30.189918359378503\n",
      "[0, 1, 3]\n",
      "Steps done: 4650\n",
      "SV: [-0.08094595  0.03492404 -0.5629852 ]\n",
      "Reward for action 2: -28.27851184436767\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 4651\n",
      "SV: [-0.08094595  0.03492404 -0.5629852 ]\n",
      "Reward for action 20: -24.27851184436767\n",
      "[0, 1, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 636\n",
      "Steps done: 4652\n",
      "SV: [-0.5011615  -0.4584236  -0.15181693]\n",
      "Reward for action 20: 0.9498918741950826\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 637\n",
      "Steps done: 4653\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 7: -31.721775696350747\n",
      "[0, 1, 7]\n",
      "Steps done: 4654\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 17: -31.287361629333155\n",
      "[0, 1]\n",
      "Steps done: 4655\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 6: -29.340718649429103\n",
      "[0, 1, 6]\n",
      "Steps done: 4656\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 10: -28.039421553128133\n",
      "[1, 6]\n",
      "Steps done: 4657\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 0: -29.340718649429096\n",
      "[1, 6, 0]\n",
      "Steps done: 4658\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 16: -31.287361629333155\n",
      "[1, 0]\n",
      "Steps done: 4659\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 6: -29.340718649429096\n",
      "[1, 0, 6]\n",
      "Steps done: 4660\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 4: -29.98272857900877\n",
      "[1, 0, 6, 4]\n",
      "Steps done: 4661\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 10: -30.299767947919364\n",
      "[1, 6, 4]\n",
      "Steps done: 4662\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 2: -30.792125650997136\n",
      "[1, 6, 4, 2]\n",
      "Steps done: 4663\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 12: -30.299767947919364\n",
      "[1, 6, 4]\n",
      "Steps done: 4664\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 0: -29.982728579008782\n",
      "[1, 6, 4, 0]\n",
      "Steps done: 4665\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 7: -30.292293017892256\n",
      "[1, 6, 4, 0, 7]\n",
      "Steps done: 4666\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 17: -29.982728579008782\n",
      "[1, 6, 4, 0]\n",
      "Steps done: 4667\n",
      "SV: [ 0.11620692 -0.09102961 -0.54956895]\n",
      "Reward for action 20: -25.982728579008782\n",
      "[1, 6, 4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 638\n",
      "Steps done: 4668\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 4: -37.43699216316885\n",
      "[0, 1, 4]\n",
      "Steps done: 4669\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 5: -12.191813302467136\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 4670\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 11: -92.80492994068896\n",
      "[0, 4, 5]\n",
      "Steps done: 4671\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 10: -200.24948445843802\n",
      "[4, 5]\n",
      "Steps done: 4672\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 6: -717.8451035289228\n",
      "[4, 5, 6]\n",
      "Steps done: 4673\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 15: -1018.0249453436552\n",
      "[4, 6]\n",
      "Steps done: 4674\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 5: -717.8451035289226\n",
      "[4, 6, 5]\n",
      "Steps done: 4675\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 14: -1182.2294179655776\n",
      "[6, 5]\n",
      "Steps done: 4676\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 1: -307.99540773490213\n",
      "[6, 5, 1]\n",
      "Steps done: 4677\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 16: -167.52741762594283\n",
      "[5, 1]\n",
      "Steps done: 4678\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 6: -307.99540773490213\n",
      "[5, 1, 6]\n",
      "Steps done: 4679\n",
      "SV: [-0.8492136  -0.40505654  1.8979826 ]\n",
      "Reward for action 20: -303.99540773490213\n",
      "[5, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 640\n",
      "Steps done: 4680\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 4: -13.899417585074747\n",
      "[0, 1, 4]\n",
      "Steps done: 4681\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 8: -14.459915410240573\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 4682\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 7: -40.822183362824525\n",
      "[0, 1, 4, 8, 7]\n",
      "Steps done: 4683\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 5: -22.399245519383268\n",
      "[0, 1, 4, 8, 7, 5]\n",
      "Steps done: 4684\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 3: -32.95636963485231\n",
      "[0, 1, 4, 8, 7, 5, 3]\n",
      "Steps done: 4685\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 11: -50.6513195660963\n",
      "[0, 4, 8, 7, 5, 3]\n",
      "Steps done: 4686\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 1: -32.9563696348523\n",
      "[0, 4, 8, 7, 5, 3, 1]\n",
      "Steps done: 4687\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 11: -50.6513195660963\n",
      "[0, 4, 8, 7, 5, 3]\n",
      "Steps done: 4688\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 2: -43.269456640102995\n",
      "[0, 4, 8, 7, 5, 3, 2]\n",
      "Steps done: 4689\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 6: -36.80778631718211\n",
      "[0, 4, 8, 7, 5, 3, 2, 6]\n",
      "Steps done: 4690\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 16: -43.269456640102995\n",
      "[0, 4, 8, 7, 5, 3, 2]\n",
      "Steps done: 4691\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 17: -29.727403628234697\n",
      "[0, 4, 8, 5, 3, 2]\n",
      "Steps done: 4692\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 7: -43.26945664010302\n",
      "[0, 4, 8, 5, 3, 2, 7]\n",
      "Steps done: 4693\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 18: -55.387389729557825\n",
      "[0, 4, 5, 3, 2, 7]\n",
      "Steps done: 4694\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 9: -60.39214627444789\n",
      "[0, 4, 5, 3, 2, 7, 9]\n",
      "Steps done: 4695\n",
      "SV: [ 0.06156988 -0.06630738 -0.32038337]\n",
      "Reward for action 20: -56.39214627444789\n",
      "[0, 4, 5, 3, 2, 7, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 641\n",
      "Steps done: 4696\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 2: -80.17638245245438\n",
      "[0, 1, 2]\n",
      "Steps done: 4697\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 12: -78.64217061060803\n",
      "[0, 1]\n",
      "Steps done: 4698\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 9: -53.45222413081375\n",
      "[0, 1, 9]\n",
      "Steps done: 4699\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 11: -162.45943433597463\n",
      "[0, 9]\n",
      "Did target update\n",
      "Steps done: 4700\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 6: -21.895405178100432\n",
      "[0, 9, 6]\n",
      "Steps done: 4701\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 8: -44.32303006100115\n",
      "[0, 9, 6, 8]\n",
      "Steps done: 4702\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 18: -21.895405178100432\n",
      "[0, 9, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4703\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 8: -44.32303006100115\n",
      "[0, 9, 6, 8]\n",
      "Steps done: 4704\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 2: -39.00591655597393\n",
      "[0, 9, 6, 8, 2]\n",
      "Steps done: 4705\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 1: -40.42692076989413\n",
      "[0, 9, 6, 8, 2, 1]\n",
      "Steps done: 4706\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 4: -43.24126116532047\n",
      "[0, 9, 6, 8, 2, 1, 4]\n",
      "Steps done: 4707\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 16: -35.373966770073636\n",
      "[0, 9, 8, 2, 1, 4]\n",
      "Steps done: 4708\n",
      "SV: [ 0.01669991  0.05812592 -1.0413649 ]\n",
      "Reward for action 20: -31.373966770073636\n",
      "[0, 9, 8, 2, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 642\n",
      "Steps done: 4709\n",
      "SV: [-0.12253436 -0.73633695  0.9462248 ]\n",
      "Reward for action 20: -129.1797107647397\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 643\n",
      "Steps done: 4710\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 2: -86.57421731270323\n",
      "[0, 1, 2]\n",
      "Steps done: 4711\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 10: -63.54664942526549\n",
      "[1, 2]\n",
      "Steps done: 4712\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 5: -268.7421033595281\n",
      "[1, 2, 5]\n",
      "Steps done: 4713\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 15: -63.54664942526549\n",
      "[1, 2]\n",
      "Steps done: 4714\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 0: -86.57421731270323\n",
      "[1, 2, 0]\n",
      "Steps done: 4715\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 7: -321.6346786578803\n",
      "[1, 2, 0, 7]\n",
      "Steps done: 4716\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 17: -86.57421731270323\n",
      "[1, 2, 0]\n",
      "Steps done: 4717\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 6: -27.298877093907798\n",
      "[1, 2, 0, 6]\n",
      "Steps done: 4718\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 4: -40.036692415358345\n",
      "[1, 2, 0, 6, 4]\n",
      "Steps done: 4719\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 7: -130.38899173040636\n",
      "[1, 2, 0, 6, 4, 7]\n",
      "Steps done: 4720\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 17: -40.036692415358345\n",
      "[1, 2, 0, 6, 4]\n",
      "Steps done: 4721\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 10: -12.38402485464521\n",
      "[1, 2, 6, 4]\n",
      "Steps done: 4722\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 3: -5.672560124226508\n",
      "[1, 2, 6, 4, 3]\n",
      "Steps done: 4723\n",
      "SV: [-0.21330483 -0.7608114   2.9008458 ]\n",
      "Reward for action 20: -1.672560124226508\n",
      "[1, 2, 6, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 644\n",
      "Steps done: 4724\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 3: -175.1012308582941\n",
      "[0, 1, 3]\n",
      "Steps done: 4725\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 7: -146.6749957451968\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 4726\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 4: -117.8925546878798\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 4727\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 6: -101.08032292046641\n",
      "[0, 1, 3, 7, 4, 6]\n",
      "Steps done: 4728\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 13: -84.27980518890529\n",
      "[0, 1, 7, 4, 6]\n",
      "Steps done: 4729\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 11: -67.1111281563195\n",
      "[0, 7, 4, 6]\n",
      "Steps done: 4730\n",
      "SV: [ 0.13532718 -0.13403367 -0.87801135]\n",
      "Reward for action 20: -63.111128156319495\n",
      "[0, 7, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 645\n",
      "Steps done: 4731\n",
      "SV: [ 0.07199603 -0.19793637 -0.8007102 ]\n",
      "Reward for action 3: -53.13216029510009\n",
      "[0, 1, 3]\n",
      "Steps done: 4732\n",
      "SV: [ 0.07199603 -0.19793637 -0.8007102 ]\n",
      "Reward for action 7: -47.5153341334577\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 4733\n",
      "SV: [ 0.07199603 -0.19793637 -0.8007102 ]\n",
      "Reward for action 9: -47.15929330163378\n",
      "[0, 1, 3, 7, 9]\n",
      "Steps done: 4734\n",
      "SV: [ 0.07199603 -0.19793637 -0.8007102 ]\n",
      "Reward for action 13: -46.33238969402084\n",
      "[0, 1, 7, 9]\n",
      "Steps done: 4735\n",
      "SV: [ 0.07199603 -0.19793637 -0.8007102 ]\n",
      "Reward for action 20: -42.33238969402084\n",
      "[0, 1, 7, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 646\n",
      "Steps done: 4736\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 7: -171.83206694768634\n",
      "[0, 1, 7]\n",
      "Steps done: 4737\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 11: -450.81869873413086\n",
      "[0, 7]\n",
      "Steps done: 4738\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 4: -222.22497799206042\n",
      "[0, 7, 4]\n",
      "Steps done: 4739\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 14: -450.81869873413086\n",
      "[0, 7]\n",
      "Steps done: 4740\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 4: -222.22497799206042\n",
      "[0, 7, 4]\n",
      "Steps done: 4741\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 14: -450.81869873413086\n",
      "[0, 7]\n",
      "Steps done: 4742\n",
      "SV: [ 0.01513371 -0.27039373  1.0582336 ]\n",
      "Reward for action 20: -446.81869873413086\n",
      "[0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 647\n",
      "Steps done: 4743\n",
      "SV: [-0.18236409 -0.05738255 -1.0256323 ]\n",
      "Reward for action 20: -117.81959505285906\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 648\n",
      "Steps done: 4744\n",
      "SV: [ 0.04992517 -0.04585312 -0.24559079]\n",
      "Reward for action 5: -4.979597411003274\n",
      "[0, 1, 5]\n",
      "Steps done: 4745\n",
      "SV: [ 0.04992517 -0.04585312 -0.24559079]\n",
      "Reward for action 2: -5.018473187248207\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 4746\n",
      "SV: [ 0.04992517 -0.04585312 -0.24559079]\n",
      "Reward for action 11: -5.033905607774332\n",
      "[0, 5, 2]\n",
      "Steps done: 4747\n",
      "SV: [ 0.04992517 -0.04585312 -0.24559079]\n",
      "Reward for action 4: -5.052374836469526\n",
      "[0, 5, 2, 4]\n",
      "Steps done: 4748\n",
      "SV: [ 0.04992517 -0.04585312 -0.24559079]\n",
      "Reward for action 10: -4.898443601034707\n",
      "[5, 2, 4]\n",
      "Steps done: 4749\n",
      "SV: [ 0.04992517 -0.04585312 -0.24559079]\n",
      "Reward for action 20: -0.8984436010347068\n",
      "[5, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 649\n",
      "Steps done: 4750\n",
      "SV: [0.1200593  0.03815746 0.17328408]\n",
      "Reward for action 8: -4.409492097344877\n",
      "[0, 1, 8]\n",
      "Steps done: 4751\n",
      "SV: [0.1200593  0.03815746 0.17328408]\n",
      "Reward for action 11: -4.348729918051777\n",
      "[0, 8]\n",
      "Steps done: 4752\n",
      "SV: [0.1200593  0.03815746 0.17328408]\n",
      "Reward for action 6: -3.455914144694061\n",
      "[0, 8, 6]\n",
      "Steps done: 4753\n",
      "SV: [0.1200593  0.03815746 0.17328408]\n",
      "Reward for action 2: -3.77171280149149\n",
      "[0, 8, 6, 2]\n",
      "Steps done: 4754\n",
      "SV: [0.1200593  0.03815746 0.17328408]\n",
      "Reward for action 20: 0.22828719850850998\n",
      "[0, 8, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 650\n",
      "Steps done: 4755\n",
      "SV: [-1.5241816   0.57446355 -4.526775  ]\n",
      "Reward for action 4: -432.3097529657452\n",
      "[0, 1, 4]\n",
      "Steps done: 4756\n",
      "SV: [-1.5241816   0.57446355 -4.526775  ]\n",
      "Reward for action 20: -428.3097529657452\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 651\n",
      "Steps done: 4757\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 5: -86.55717805483411\n",
      "[0, 1, 5]\n",
      "Steps done: 4758\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 15: -64.96897776189343\n",
      "[0, 1]\n",
      "Steps done: 4759\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 2: -65.88316627923233\n",
      "[0, 1, 2]\n",
      "Steps done: 4760\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 4: -65.33784186926609\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 4761\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 7: -65.37012682143079\n",
      "[0, 1, 2, 4, 7]\n",
      "Steps done: 4762\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 5: -60.92395777448761\n",
      "[0, 1, 2, 4, 7, 5]\n",
      "Steps done: 4763\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 10: -57.12008511968301\n",
      "[1, 2, 4, 7, 5]\n",
      "Steps done: 4764\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 11: -57.44705533140912\n",
      "[2, 4, 7, 5]\n",
      "Steps done: 4765\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 14: -59.407252750771185\n",
      "[2, 7, 5]\n",
      "Steps done: 4766\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 17: -62.52576706038964\n",
      "[2, 5]\n",
      "Steps done: 4767\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 7: -59.40725275077118\n",
      "[2, 5, 7]\n",
      "Steps done: 4768\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 0: -65.38704039752147\n",
      "[2, 5, 7, 0]\n",
      "Steps done: 4769\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 3: -63.40067012929769\n",
      "[2, 5, 7, 0, 3]\n",
      "Steps done: 4770\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 4: -60.46388204877218\n",
      "[2, 5, 7, 0, 3, 4]\n",
      "Steps done: 4771\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 14: -63.40067012929769\n",
      "[2, 5, 7, 0, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4772\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 13: -65.38704039752147\n",
      "[2, 5, 7, 0]\n",
      "Steps done: 4773\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 17: -74.53744078113783\n",
      "[2, 5, 0]\n",
      "Steps done: 4774\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 15: -64.1299288356414\n",
      "[2, 0]\n",
      "Steps done: 4775\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 5: -74.53744078113786\n",
      "[2, 0, 5]\n",
      "Steps done: 4776\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 3: -65.8340592310786\n",
      "[2, 0, 5, 3]\n",
      "Steps done: 4777\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 7: -63.40067012929769\n",
      "[2, 0, 5, 3, 7]\n",
      "Steps done: 4778\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 13: -65.3870403975215\n",
      "[2, 0, 5, 7]\n",
      "Steps done: 4779\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 17: -74.53744078113786\n",
      "[2, 0, 5]\n",
      "Steps done: 4780\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 7: -65.3870403975215\n",
      "[2, 0, 5, 7]\n",
      "Steps done: 4781\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 17: -74.53744078113786\n",
      "[2, 0, 5]\n",
      "Steps done: 4782\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Reward for action 3: -65.8340592310786\n",
      "[2, 0, 5, 3]\n",
      "Steps done: 4783\n",
      "SV: [0.00873581 0.05553539 0.90112823]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -61.8340592310786\n",
      "[2, 0, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 652\n",
      "Steps done: 4784\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 3: -48.51097767555618\n",
      "[0, 1, 3]\n",
      "Steps done: 4785\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 13: -46.75053444105049\n",
      "[0, 1]\n",
      "Steps done: 4786\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 5: -36.74758643643886\n",
      "[0, 1, 5]\n",
      "Steps done: 4787\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 2: -37.347140835196505\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 4788\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 12: -36.74758643643886\n",
      "[0, 1, 5]\n",
      "Steps done: 4789\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 4: -43.62730762868014\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 4790\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 2: -46.6742796593735\n",
      "[0, 1, 5, 4, 2]\n",
      "Steps done: 4791\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 10: -48.19082046573618\n",
      "[1, 5, 4, 2]\n",
      "Steps done: 4792\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 15: -154.19516039773714\n",
      "[1, 4, 2]\n",
      "Steps done: 4793\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 0: -92.54644672287125\n",
      "[1, 4, 2, 0]\n",
      "Steps done: 4794\n",
      "SV: [ 0.0852239  -0.7591304  -0.14097144]\n",
      "Reward for action 20: -88.54644672287125\n",
      "[1, 4, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 653\n",
      "Steps done: 4795\n",
      "SV: [-0.2712976  -1.418987    0.32314527]\n",
      "Reward for action 2: -27.861521616667822\n",
      "[0, 1, 2]\n",
      "Steps done: 4796\n",
      "SV: [-0.2712976  -1.418987    0.32314527]\n",
      "Reward for action 12: -102.04464021559923\n",
      "[0, 1]\n",
      "Steps done: 4797\n",
      "SV: [-0.2712976  -1.418987    0.32314527]\n",
      "Reward for action 20: -98.04464021559923\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 654\n",
      "Steps done: 4798\n",
      "SV: [-0.17847762  0.3957398   0.274716  ]\n",
      "Reward for action 5: -126.2800907206599\n",
      "[0, 1, 5]\n",
      "Steps done: 4799\n",
      "SV: [-0.17847762  0.3957398   0.274716  ]\n",
      "Reward for action 15: -17.638828752838027\n",
      "[0, 1]\n",
      "Did target update\n",
      "Steps done: 4800\n",
      "SV: [-0.17847762  0.3957398   0.274716  ]\n",
      "Reward for action 20: -13.638828752838027\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 655\n",
      "Steps done: 4801\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 5: -9.294605959257932\n",
      "[0, 1, 5]\n",
      "Steps done: 4802\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 11: -120.05108338071423\n",
      "[0, 5]\n",
      "Steps done: 4803\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 9: -108.63075910802522\n",
      "[0, 5, 9]\n",
      "Steps done: 4804\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 19: -120.05108338071423\n",
      "[0, 5]\n",
      "Steps done: 4805\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 9: -108.63075910802522\n",
      "[0, 5, 9]\n",
      "Steps done: 4806\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 3: -37.05133339625556\n",
      "[0, 5, 9, 3]\n",
      "Steps done: 4807\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 13: -108.63075910802522\n",
      "[0, 5, 9]\n",
      "Steps done: 4808\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 15: -103.35906578817365\n",
      "[0, 9]\n",
      "Steps done: 4809\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 2: -100.60257891554507\n",
      "[0, 9, 2]\n",
      "Steps done: 4810\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 3: -25.11928622280841\n",
      "[0, 9, 2, 3]\n",
      "Steps done: 4811\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 7: -31.590800356469774\n",
      "[0, 9, 2, 3, 7]\n",
      "Steps done: 4812\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 19: -12.977926594760808\n",
      "[0, 2, 3, 7]\n",
      "Steps done: 4813\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 9: -31.59080035646978\n",
      "[0, 2, 3, 7, 9]\n",
      "Steps done: 4814\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 6: -40.3551975240443\n",
      "[0, 2, 3, 7, 9, 6]\n",
      "Steps done: 4815\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 17: -35.16517809250642\n",
      "[0, 2, 3, 9, 6]\n",
      "Steps done: 4816\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 7: -40.35519752404431\n",
      "[0, 2, 3, 9, 6, 7]\n",
      "Steps done: 4817\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 19: -25.3665880735874\n",
      "[0, 2, 3, 6, 7]\n",
      "Steps done: 4818\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 10: -8.238669793705085\n",
      "[2, 3, 6, 7]\n",
      "Steps done: 4819\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 17: -2.318086636977518\n",
      "[2, 3, 6]\n",
      "Steps done: 4820\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 5: -4.5823010346424615\n",
      "[2, 3, 6, 5]\n",
      "Steps done: 4821\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 12: -4.35891582265228\n",
      "[3, 6, 5]\n",
      "Steps done: 4822\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 1: -10.84054592403102\n",
      "[3, 6, 5, 1]\n",
      "Steps done: 4823\n",
      "SV: [ 0.8839709  -0.9114112  -0.15039149]\n",
      "Reward for action 20: -6.840545924031019\n",
      "[3, 6, 5, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 656\n",
      "Steps done: 4824\n",
      "SV: [0.96615773 0.6262612  3.627856  ]\n",
      "Reward for action 3: -290.5272567903648\n",
      "[0, 1, 3]\n",
      "Steps done: 4825\n",
      "SV: [0.96615773 0.6262612  3.627856  ]\n",
      "Reward for action 5: -303.1106370126453\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 4826\n",
      "SV: [0.96615773 0.6262612  3.627856  ]\n",
      "Reward for action 20: -299.1106370126453\n",
      "[0, 1, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 657\n",
      "Steps done: 4827\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 8: -70.59128011356901\n",
      "[0, 1, 8]\n",
      "Steps done: 4828\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 11: -137.18637295076815\n",
      "[0, 8]\n",
      "Steps done: 4829\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 2: -69.46782317897777\n",
      "[0, 8, 2]\n",
      "Steps done: 4830\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 1: -31.685562794538953\n",
      "[0, 8, 2, 1]\n",
      "Steps done: 4831\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 11: -69.46782317897777\n",
      "[0, 8, 2]\n",
      "Steps done: 4832\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 18: -0.3422683676956594\n",
      "[0, 2]\n",
      "Steps done: 4833\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 3: -0.19693572206866583\n",
      "[0, 2, 3]\n",
      "Steps done: 4834\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 13: -0.3422683676956594\n",
      "[0, 2]\n",
      "Steps done: 4835\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 3: -0.19693572206866583\n",
      "[0, 2, 3]\n",
      "Steps done: 4836\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 13: -0.3422683676956594\n",
      "[0, 2]\n",
      "Steps done: 4837\n",
      "SV: [-0.18156491  0.05735913 -0.01685465]\n",
      "Reward for action 20: 3.6577316323043405\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 658\n",
      "Steps done: 4838\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 8: -18.93088624783937\n",
      "[0, 1, 8]\n",
      "Steps done: 4839\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 5: -3.406025494589658\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 4840\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 18: -5.120305668383324\n",
      "[0, 1, 5]\n",
      "Steps done: 4841\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 8: -3.4060254945896586\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 4842\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 18: -5.120305668383324\n",
      "[0, 1, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4843\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 2: -1.984963822740041\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 4844\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 7: -6.474331665405359\n",
      "[0, 1, 5, 2, 7]\n",
      "Steps done: 4845\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 8: -1.4082425228180975\n",
      "[0, 1, 5, 2, 7, 8]\n",
      "Steps done: 4846\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 15: -21.852992827990363\n",
      "[0, 1, 2, 7, 8]\n",
      "Steps done: 4847\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 12: -1.586392523356969\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 4848\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 5: -11.35738180792668\n",
      "[0, 1, 7, 8, 5]\n",
      "Steps done: 4849\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 11: -89.5959515924434\n",
      "[0, 7, 8, 5]\n",
      "Steps done: 4850\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 4: -12.08466056692903\n",
      "[0, 7, 8, 5, 4]\n",
      "Steps done: 4851\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 2: -1.2034041130299782\n",
      "[0, 7, 8, 5, 4, 2]\n",
      "Steps done: 4852\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 3: -0.9392667959092214\n",
      "[0, 7, 8, 5, 4, 2, 3]\n",
      "Steps done: 4853\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 6: -0.9632845711692604\n",
      "[0, 7, 8, 5, 4, 2, 3, 6]\n",
      "Steps done: 4854\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 18: -0.7798943934431566\n",
      "[0, 7, 5, 4, 2, 3, 6]\n",
      "Steps done: 4855\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 17: -1.2154696693459277\n",
      "[0, 5, 4, 2, 3, 6]\n",
      "Steps done: 4856\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 16: -2.3239120479406936\n",
      "[0, 5, 4, 2, 3]\n",
      "Steps done: 4857\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 8: -1.2707504990616154\n",
      "[0, 5, 4, 2, 3, 8]\n",
      "Steps done: 4858\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 18: -2.3239120479406936\n",
      "[0, 5, 4, 2, 3]\n",
      "Steps done: 4859\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 6: -1.2154696693459277\n",
      "[0, 5, 4, 2, 3, 6]\n",
      "Steps done: 4860\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 13: -2.1485383570734378\n",
      "[0, 5, 4, 2, 6]\n",
      "Steps done: 4861\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 8: -4.613491818617958\n",
      "[0, 5, 4, 2, 6, 8]\n",
      "Steps done: 4862\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 10: -1.8053794592604036\n",
      "[5, 4, 2, 6, 8]\n",
      "Steps done: 4863\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Reward for action 15: -29.86766068431491\n",
      "[4, 2, 6, 8]\n",
      "Steps done: 4864\n",
      "SV: [ 0.04084396 -0.06935895 -0.33495826]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -25.86766068431491\n",
      "[4, 2, 6, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 659\n",
      "Steps done: 4865\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 7: -102.42562068489161\n",
      "[0, 1, 7]\n",
      "Steps done: 4866\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 11: -34.87625225894604\n",
      "[0, 7]\n",
      "Steps done: 4867\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 5: -180.22019012470884\n",
      "[0, 7, 5]\n",
      "Steps done: 4868\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 15: -34.87625225894604\n",
      "[0, 7]\n",
      "Steps done: 4869\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 6: -7.123610111383275\n",
      "[0, 7, 6]\n",
      "Steps done: 4870\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 5: -84.81243904958947\n",
      "[0, 7, 6, 5]\n",
      "Steps done: 4871\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 17: -223.9903671184225\n",
      "[0, 6, 5]\n",
      "Steps done: 4872\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 7: -84.81243904958949\n",
      "[0, 6, 5, 7]\n",
      "Steps done: 4873\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 2: -109.56270279864417\n",
      "[0, 6, 5, 7, 2]\n",
      "Steps done: 4874\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 12: -84.81243904958949\n",
      "[0, 6, 5, 7]\n",
      "Steps done: 4875\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 16: -180.22019012470878\n",
      "[0, 5, 7]\n",
      "Steps done: 4876\n",
      "SV: [ 0.75485027 -0.13900137 -1.0232872 ]\n",
      "Reward for action 20: -176.22019012470878\n",
      "[0, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 660\n",
      "Steps done: 4877\n",
      "SV: [ 1.0305427  -0.34401077 -1.1976366 ]\n",
      "Reward for action 20: -216.02145071254813\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 661\n",
      "Steps done: 4878\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 7: -184.29879159763334\n",
      "[0, 1, 7]\n",
      "Steps done: 4879\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 8: -34.13627242968781\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 4880\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 17: -29.303692248556793\n",
      "[0, 1, 8]\n",
      "Steps done: 4881\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 18: -4.631148838022618\n",
      "[0, 1]\n",
      "Steps done: 4882\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 8: -29.303692248556793\n",
      "[0, 1, 8]\n",
      "Steps done: 4883\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 6: -13.306888562359116\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 4884\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 16: -29.303692248556793\n",
      "[0, 1, 8]\n",
      "Steps done: 4885\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 18: -4.631148838022618\n",
      "[0, 1]\n",
      "Steps done: 4886\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 6: -7.1808375957957855\n",
      "[0, 1, 6]\n",
      "Steps done: 4887\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 2: -8.348251937788142\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 4888\n",
      "SV: [ 0.3055126  -0.07701876 -0.07584009]\n",
      "Reward for action 20: -4.348251937788142\n",
      "[0, 1, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 662\n",
      "Steps done: 4889\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 2: -22.22689143753579\n",
      "[0, 1, 2]\n",
      "Steps done: 4890\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 3: -34.98393648535409\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 4891\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 9: -35.78317522517634\n",
      "[0, 1, 2, 3, 9]\n",
      "Steps done: 4892\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 5: -26.94952998787422\n",
      "[0, 1, 2, 3, 9, 5]\n",
      "Steps done: 4893\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 6: -11.93690545030028\n",
      "[0, 1, 2, 3, 9, 5, 6]\n",
      "Steps done: 4894\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 19: -15.783640092470337\n",
      "[0, 1, 2, 3, 5, 6]\n",
      "Steps done: 4895\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 9: -11.936905450300285\n",
      "[0, 1, 2, 3, 5, 6, 9]\n",
      "Steps done: 4896\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 8: -0.8689525412513075\n",
      "[0, 1, 2, 3, 5, 6, 9, 8]\n",
      "Steps done: 4897\n",
      "SV: [ 0.6190001   0.15719287 -0.16551207]\n",
      "Reward for action 20: 3.1310474587486925\n",
      "[0, 1, 2, 3, 5, 6, 9, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 663\n",
      "Steps done: 4898\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 8: -7.936987582598683\n",
      "[0, 1, 8]\n",
      "Steps done: 4899\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 10: -49.91623486825728\n",
      "[1, 8]\n",
      "Did target update\n",
      "Steps done: 4900\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 5: -65.87619492947323\n",
      "[1, 8, 5]\n",
      "Steps done: 4901\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 11: -93.05928016256236\n",
      "[8, 5]\n",
      "Steps done: 4902\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 6: -105.9419584474515\n",
      "[8, 5, 6]\n",
      "Steps done: 4903\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 3: -3.780394860266194\n",
      "[8, 5, 6, 3]\n",
      "Steps done: 4904\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 18: -7.096161173486502\n",
      "[5, 6, 3]\n",
      "Steps done: 4905\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 13: -118.31462537187636\n",
      "[5, 6]\n",
      "Steps done: 4906\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 7: -49.93811340793454\n",
      "[5, 6, 7]\n",
      "Steps done: 4907\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 17: -118.31462537187636\n",
      "[5, 6]\n",
      "Steps done: 4908\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 7: -49.93811340793454\n",
      "[5, 6, 7]\n",
      "Steps done: 4909\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 4: -20.81928415463317\n",
      "[5, 6, 7, 4]\n",
      "Steps done: 4910\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 17: -49.07196848635114\n",
      "[5, 6, 4]\n",
      "Steps done: 4911\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 14: -118.31462537187636\n",
      "[5, 6]\n",
      "Steps done: 4912\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 9: -80.67126221762629\n",
      "[5, 6, 9]\n",
      "Steps done: 4913\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 1: -46.52583730702007\n",
      "[5, 6, 9, 1]\n",
      "Steps done: 4914\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 7: -27.42507874785541\n",
      "[5, 6, 9, 1, 7]\n",
      "Steps done: 4915\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 4: -15.452682736479987\n",
      "[5, 6, 9, 1, 7, 4]\n",
      "Steps done: 4916\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 11: -31.606152907410866\n",
      "[5, 6, 9, 7, 4]\n",
      "Steps done: 4917\n",
      "SV: [0.41690826 0.1064822  0.11374079]\n",
      "Reward for action 20: -27.606152907410866\n",
      "[5, 6, 9, 7, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 664\n",
      "Steps done: 4918\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 3: -57.306245480777505\n",
      "[0, 1, 3]\n",
      "Steps done: 4919\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 13: -15.936131240647786\n",
      "[0, 1]\n",
      "Steps done: 4920\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 3: -57.306245480777505\n",
      "[0, 1, 3]\n",
      "Steps done: 4921\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 2: -34.548314691198314\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 4922\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 11: -0.6128247988460799\n",
      "[0, 3, 2]\n",
      "Steps done: 4923\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 1: -34.5483146911983\n",
      "[0, 3, 2, 1]\n",
      "Steps done: 4924\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 12: -57.30624548077749\n",
      "[0, 3, 1]\n",
      "Steps done: 4925\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 4: -6.196282237923819\n",
      "[0, 3, 1, 4]\n",
      "Steps done: 4926\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 10: -18.93229174981143\n",
      "[3, 1, 4]\n",
      "Steps done: 4927\n",
      "SV: [ 0.00488686  0.01785947 -0.13272174]\n",
      "Reward for action 20: -14.93229174981143\n",
      "[3, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 665\n",
      "Steps done: 4928\n",
      "SV: [-0.35982388  0.26589128  1.1589804 ]\n",
      "Reward for action 20: -199.30249742535784\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 666\n",
      "Steps done: 4929\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 2: -7.870286633738896\n",
      "[0, 1, 2]\n",
      "Steps done: 4930\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 11: -29.84918642101586\n",
      "[0, 2]\n",
      "Steps done: 4931\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 3: -28.087435291426015\n",
      "[0, 2, 3]\n",
      "Steps done: 4932\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 13: -29.84918642101586\n",
      "[0, 2]\n",
      "Steps done: 4933\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 3: -28.087435291426015\n",
      "[0, 2, 3]\n",
      "Steps done: 4934\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 13: -29.84918642101586\n",
      "[0, 2]\n",
      "Steps done: 4935\n",
      "SV: [-0.17819002  0.0957365   0.02943941]\n",
      "Reward for action 20: -25.84918642101586\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 667\n",
      "Steps done: 4936\n",
      "SV: [-0.65523857 -0.30154744 -1.3519108 ]\n",
      "Reward for action 20: -184.14809261577415\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 668\n",
      "Steps done: 4937\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 7: -6.390010950525646\n",
      "[0, 1, 7]\n",
      "Steps done: 4938\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 9: -9.872240368514474\n",
      "[0, 1, 7, 9]\n",
      "Steps done: 4939\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 8: -8.874864177114086\n",
      "[0, 1, 7, 9, 8]\n",
      "Steps done: 4940\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 3: -8.831533152926195\n",
      "[0, 1, 7, 9, 8, 3]\n",
      "Steps done: 4941\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 19: -6.8768361485679605\n",
      "[0, 1, 7, 8, 3]\n",
      "Steps done: 4942\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 13: -6.609487094981054\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 4943\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 2: -9.128791801471525\n",
      "[0, 1, 7, 8, 2]\n",
      "Steps done: 4944\n",
      "SV: [ 0.02566121  0.14484937 -0.26366055]\n",
      "Reward for action 20: -5.128791801471525\n",
      "[0, 1, 7, 8, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 669\n",
      "Steps done: 4945\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 5: -92.22120051879993\n",
      "[0, 1, 5]\n",
      "Steps done: 4946\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 11: -168.08063070912917\n",
      "[0, 5]\n",
      "Steps done: 4947\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 3: -94.83710767092025\n",
      "[0, 5, 3]\n",
      "Steps done: 4948\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 13: -168.08063070912917\n",
      "[0, 5]\n",
      "Steps done: 4949\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 2: -97.73979482059224\n",
      "[0, 5, 2]\n",
      "Steps done: 4950\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 1: -93.52861647613386\n",
      "[0, 5, 2, 1]\n",
      "Steps done: 4951\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 3: -88.61711079314128\n",
      "[0, 5, 2, 1, 3]\n",
      "Steps done: 4952\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 4: -65.93783967043018\n",
      "[0, 5, 2, 1, 3, 4]\n",
      "Steps done: 4953\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 13: -62.93811413120353\n",
      "[0, 5, 2, 1, 4]\n",
      "Steps done: 4954\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 11: -57.67418588913217\n",
      "[0, 5, 2, 4]\n",
      "Steps done: 4955\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 15: -70.6046438417942\n",
      "[0, 2, 4]\n",
      "Steps done: 4956\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 3: -63.72655127496324\n",
      "[0, 2, 4, 3]\n",
      "Steps done: 4957\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 12: -25.58842432832012\n",
      "[0, 4, 3]\n",
      "Steps done: 4958\n",
      "SV: [ 1.1074982  -0.33716044  0.6401377 ]\n",
      "Reward for action 20: -21.58842432832012\n",
      "[0, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 670\n",
      "Steps done: 4959\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 2: -15.084659539939032\n",
      "[0, 1, 2]\n",
      "Steps done: 4960\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 11: -11.008393427440414\n",
      "[0, 2]\n",
      "Steps done: 4961\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 5: -16.669752784522046\n",
      "[0, 2, 5]\n",
      "Steps done: 4962\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 4: -12.385966805246172\n",
      "[0, 2, 5, 4]\n",
      "Steps done: 4963\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 10: -15.153746856689166\n",
      "[2, 5, 4]\n",
      "Steps done: 4964\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 3: -9.40072989095143\n",
      "[2, 5, 4, 3]\n",
      "Steps done: 4965\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 13: -15.153746856689166\n",
      "[2, 5, 4]\n",
      "Steps done: 4966\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 15: -9.828644583080383\n",
      "[2, 4]\n",
      "Steps done: 4967\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 1: -24.911108216348577\n",
      "[2, 4, 1]\n",
      "Steps done: 4968\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 3: -12.010429977501374\n",
      "[2, 4, 1, 3]\n",
      "Steps done: 4969\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 13: -24.911108216348577\n",
      "[2, 4, 1]\n",
      "Steps done: 4970\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 3: -12.010429977501374\n",
      "[2, 4, 1, 3]\n",
      "Steps done: 4971\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 12: -20.70659441511264\n",
      "[4, 1, 3]\n",
      "Steps done: 4972\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 0: -13.967584959021059\n",
      "[4, 1, 3, 0]\n",
      "Steps done: 4973\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 5: -16.580269783258164\n",
      "[4, 1, 3, 0, 5]\n",
      "Steps done: 4974\n",
      "SV: [0.1541621  0.30811784 0.14089186]\n",
      "Reward for action 20: -12.580269783258164\n",
      "[4, 1, 3, 0, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 671\n",
      "Steps done: 4975\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 2: -34.85535153098472\n",
      "[0, 1, 2]\n",
      "Steps done: 4976\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 10: -301.1353404330348\n",
      "[1, 2]\n",
      "Steps done: 4977\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 0: -34.85535153098473\n",
      "[1, 2, 0]\n",
      "Steps done: 4978\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 12: -306.5365808924366\n",
      "[1, 0]\n",
      "Steps done: 4979\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 2: -34.85535153098473\n",
      "[1, 0, 2]\n",
      "Steps done: 4980\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 12: -306.5365808924366\n",
      "[1, 0]\n",
      "Steps done: 4981\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 2: -34.85535153098473\n",
      "[1, 0, 2]\n",
      "Steps done: 4982\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 12: -306.5365808924366\n",
      "[1, 0]\n",
      "Steps done: 4983\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 2: -34.85535153098473\n",
      "[1, 0, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 4984\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 10: -301.1353404330348\n",
      "[1, 2]\n",
      "Steps done: 4985\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 3: -301.3853254439848\n",
      "[1, 2, 3]\n",
      "Steps done: 4986\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 11: -302.4574444510866\n",
      "[2, 3]\n",
      "Steps done: 4987\n",
      "SV: [ 1.5322163  -0.14427748 -2.1448925 ]\n",
      "Reward for action 20: -298.4574444510866\n",
      "[2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 672\n",
      "Steps done: 4988\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 4: -56.10846675044834\n",
      "[0, 1, 4]\n",
      "Steps done: 4989\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 11: -82.02359914259617\n",
      "[0, 4]\n",
      "Steps done: 4990\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 3: -230.66848996339542\n",
      "[0, 4, 3]\n",
      "Steps done: 4991\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 13: -82.02359914259617\n",
      "[0, 4]\n",
      "Steps done: 4992\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 1: -56.10846675044834\n",
      "[0, 4, 1]\n",
      "Steps done: 4993\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 11: -82.02359914259617\n",
      "[0, 4]\n",
      "Steps done: 4994\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 3: -230.66848996339542\n",
      "[0, 4, 3]\n",
      "Steps done: 4995\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 14: -332.9576786805486\n",
      "[0, 3]\n",
      "Steps done: 4996\n",
      "SV: [-0.44688424 -0.6308941  -0.8333923 ]\n",
      "Reward for action 20: -328.9576786805486\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 673\n",
      "Steps done: 4997\n",
      "SV: [-0.1883982  -0.04140067 -0.03104821]\n",
      "Reward for action 20: 3.075268870727038\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 674\n",
      "Steps done: 4998\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 3: -9.395184327217233\n",
      "[0, 1, 3]\n",
      "Steps done: 4999\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 13: -0.30916154031805937\n",
      "[0, 1]\n",
      "Did target update\n",
      "Steps done: 5000\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 3: -9.395184327217233\n",
      "[0, 1, 3]\n",
      "Steps done: 5001\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 5: -154.39491148280308\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 5002\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 2: -1.0722770223076918\n",
      "[0, 1, 3, 5, 2]\n",
      "Steps done: 5003\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 13: -8.302479760774668\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 5004\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 10: -4.6437349231655\n",
      "[1, 5, 2]\n",
      "Steps done: 5005\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 6: -11.001743116640537\n",
      "[1, 5, 2, 6]\n",
      "Steps done: 5006\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 3: -16.030825203457187\n",
      "[1, 5, 2, 6, 3]\n",
      "Steps done: 5007\n",
      "SV: [-0.11268713 -0.05192273  0.4997253 ]\n",
      "Reward for action 20: -12.030825203457187\n",
      "[1, 5, 2, 6, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 675\n",
      "Steps done: 5008\n",
      "SV: [ 0.10065023 -0.26484084 -1.180616  ]\n",
      "Reward for action 8: -89.76561090914988\n",
      "[0, 1, 8]\n",
      "Steps done: 5009\n",
      "SV: [ 0.10065023 -0.26484084 -1.180616  ]\n",
      "Reward for action 20: -85.76561090914988\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 676\n",
      "Steps done: 5010\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 7: -173.57065834267087\n",
      "[0, 1, 7]\n",
      "Steps done: 5011\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 5: -322.1201565344398\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 5012\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 4: -317.1586920294232\n",
      "[0, 1, 7, 5, 4]\n",
      "Steps done: 5013\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 2: -381.35035481429827\n",
      "[0, 1, 7, 5, 4, 2]\n",
      "Steps done: 5014\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 9: -441.15598333007387\n",
      "[0, 1, 7, 5, 4, 2, 9]\n",
      "Steps done: 5015\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 12: -401.00277685770715\n",
      "[0, 1, 7, 5, 4, 9]\n",
      "Steps done: 5016\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 3: -424.9258149736459\n",
      "[0, 1, 7, 5, 4, 9, 3]\n",
      "Steps done: 5017\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 11: -414.0999158824363\n",
      "[0, 7, 5, 4, 9, 3]\n",
      "Steps done: 5018\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 15: -353.44778410760756\n",
      "[0, 7, 4, 9, 3]\n",
      "Steps done: 5019\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 17: -617.3167534175909\n",
      "[0, 4, 9, 3]\n",
      "Steps done: 5020\n",
      "SV: [0.3462865  0.17895159 2.29204   ]\n",
      "Reward for action 20: -613.3167534175909\n",
      "[0, 4, 9, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 677\n",
      "Steps done: 5021\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 6: -18.17741098853292\n",
      "[0, 1, 6]\n",
      "Steps done: 5022\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 5: -6.7646408804912666\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 5023\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 11: -21.012622813752813\n",
      "[0, 6, 5]\n",
      "Steps done: 5024\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 10: -18.819530365337727\n",
      "[6, 5]\n",
      "Steps done: 5025\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 7: -39.458197651086095\n",
      "[6, 5, 7]\n",
      "Steps done: 5026\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 0: -28.453051566727655\n",
      "[6, 5, 7, 0]\n",
      "Steps done: 5027\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 10: -39.458197651086095\n",
      "[6, 5, 7]\n",
      "Steps done: 5028\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 17: -18.819530365337727\n",
      "[6, 5]\n",
      "Steps done: 5029\n",
      "SV: [-0.39751717  0.25562894 -0.6028382 ]\n",
      "Reward for action 20: -14.819530365337727\n",
      "[6, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 678\n",
      "Steps done: 5030\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 2: -7.569553561489557\n",
      "[0, 1, 2]\n",
      "Steps done: 5031\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 11: -269.5652897695539\n",
      "[0, 2]\n",
      "Steps done: 5032\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 5: -156.6260662990651\n",
      "[0, 2, 5]\n",
      "Steps done: 5033\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 12: -11.55038799293922\n",
      "[0, 5]\n",
      "Steps done: 5034\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 4: -3.5773295422617357\n",
      "[0, 5, 4]\n",
      "Steps done: 5035\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 2: -22.826241279813004\n",
      "[0, 5, 4, 2]\n",
      "Steps done: 5036\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 1: -5.528358793236697\n",
      "[0, 5, 4, 2, 1]\n",
      "Steps done: 5037\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 3: -9.64292545931157\n",
      "[0, 5, 4, 2, 1, 3]\n",
      "Steps done: 5038\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 6: -4.610424490041533\n",
      "[0, 5, 4, 2, 1, 3, 6]\n",
      "Steps done: 5039\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 11: -10.144407614834932\n",
      "[0, 5, 4, 2, 3, 6]\n",
      "Steps done: 5040\n",
      "SV: [ 0.01427009 -0.04097168  0.19380805]\n",
      "Reward for action 20: -6.1444076148349325\n",
      "[0, 5, 4, 2, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 679\n",
      "Steps done: 5041\n",
      "SV: [ 0.20483159 -0.27445465 -1.4209669 ]\n",
      "Reward for action 20: -197.03161541666069\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 680\n",
      "Steps done: 5042\n",
      "SV: [-0.03836004  0.03630232  0.55479944]\n",
      "Reward for action 20: -808.0046451395001\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 681\n",
      "Steps done: 5043\n",
      "SV: [ 0.7088892  -0.2447655   0.38031062]\n",
      "Reward for action 4: -21.876967447332927\n",
      "[0, 1, 4]\n",
      "Steps done: 5044\n",
      "SV: [ 0.7088892  -0.2447655   0.38031062]\n",
      "Reward for action 10: -22.050907483332953\n",
      "[1, 4]\n",
      "Steps done: 5045\n",
      "SV: [ 0.7088892  -0.2447655   0.38031062]\n",
      "Reward for action 20: -18.050907483332953\n",
      "[1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 682\n",
      "Steps done: 5046\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 5: -97.39086416056966\n",
      "[0, 1, 5]\n",
      "Steps done: 5047\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 4: -7.536549982208494\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 5048\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 14: -97.39086416056966\n",
      "[0, 1, 5]\n",
      "Steps done: 5049\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 15: -32.66344065509658\n",
      "[0, 1]\n",
      "Steps done: 5050\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 5: -97.39086416056966\n",
      "[0, 1, 5]\n",
      "Steps done: 5051\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 10: -820.3789787198334\n",
      "[1, 5]\n",
      "Steps done: 5052\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 3: -10.778018847268493\n",
      "[1, 5, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5053\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 15: -606.1340027363478\n",
      "[1, 3]\n",
      "Steps done: 5054\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 0: -56.62954605702726\n",
      "[1, 3, 0]\n",
      "Steps done: 5055\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 10: -606.1340027363478\n",
      "[1, 3]\n",
      "Steps done: 5056\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 2: -156.9064119212295\n",
      "[1, 3, 2]\n",
      "Steps done: 5057\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 13: -110.27053279051916\n",
      "[1, 2]\n",
      "Steps done: 5058\n",
      "SV: [ 0.07524158 -0.05907549 -0.72119796]\n",
      "Reward for action 20: -106.27053279051916\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 683\n",
      "Steps done: 5059\n",
      "SV: [ 1.3648129  -0.56723434 -3.1406558 ]\n",
      "Reward for action 20: -437.03421681599576\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 684\n",
      "Steps done: 5060\n",
      "SV: [-0.05211702  0.01033382 -0.16780224]\n",
      "Reward for action 3: -3.5923873247649807\n",
      "[0, 1, 3]\n",
      "Steps done: 5061\n",
      "SV: [-0.05211702  0.01033382 -0.16780224]\n",
      "Reward for action 11: -5.219927529829035\n",
      "[0, 3]\n",
      "Steps done: 5062\n",
      "SV: [-0.05211702  0.01033382 -0.16780224]\n",
      "Reward for action 2: -5.019725755837911\n",
      "[0, 3, 2]\n",
      "Steps done: 5063\n",
      "SV: [-0.05211702  0.01033382 -0.16780224]\n",
      "Reward for action 20: -1.0197257558379107\n",
      "[0, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 685\n",
      "Steps done: 5064\n",
      "SV: [-0.48858386  0.6226797  -3.3728065 ]\n",
      "Reward for action 20: -638.1936712153531\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 686\n",
      "Steps done: 5065\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 2: -35.16438205250709\n",
      "[0, 1, 2]\n",
      "Steps done: 5066\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 12: -228.51439816828727\n",
      "[0, 1]\n",
      "Steps done: 5067\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 4: -54.139348552387375\n",
      "[0, 1, 4]\n",
      "Steps done: 5068\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 14: -228.51439816828727\n",
      "[0, 1]\n",
      "Steps done: 5069\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 2: -35.16438205250709\n",
      "[0, 1, 2]\n",
      "Steps done: 5070\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 11: -16.263470834008565\n",
      "[0, 2]\n",
      "Steps done: 5071\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 6: -41.75133580728559\n",
      "[0, 2, 6]\n",
      "Steps done: 5072\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 12: -43.23219934518879\n",
      "[0, 6]\n",
      "Steps done: 5073\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 7: -71.83947715946994\n",
      "[0, 6, 7]\n",
      "Steps done: 5074\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 2: -59.02136324044704\n",
      "[0, 6, 7, 2]\n",
      "Steps done: 5075\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 3: -26.140869971028202\n",
      "[0, 6, 7, 2, 3]\n",
      "Steps done: 5076\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 12: -25.86311216744323\n",
      "[0, 6, 7, 3]\n",
      "Steps done: 5077\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 4: -9.65448348668045\n",
      "[0, 6, 7, 3, 4]\n",
      "Steps done: 5078\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 1: -8.637016977437554\n",
      "[0, 6, 7, 3, 4, 1]\n",
      "Steps done: 5079\n",
      "SV: [-0.11792669  0.21507855  0.33971158]\n",
      "Reward for action 20: -4.637016977437554\n",
      "[0, 6, 7, 3, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 687\n",
      "Steps done: 5080\n",
      "SV: [-0.13518378  0.04831894 -0.04099124]\n",
      "Reward for action 2: -10.798930606695327\n",
      "[0, 1, 2]\n",
      "Steps done: 5081\n",
      "SV: [-0.13518378  0.04831894 -0.04099124]\n",
      "Reward for action 5: -1.3303060881854363\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 5082\n",
      "SV: [-0.13518378  0.04831894 -0.04099124]\n",
      "Reward for action 4: -6.8722892743899\n",
      "[0, 1, 2, 5, 4]\n",
      "Steps done: 5083\n",
      "SV: [-0.13518378  0.04831894 -0.04099124]\n",
      "Reward for action 20: -2.8722892743898996\n",
      "[0, 1, 2, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 688\n",
      "Steps done: 5084\n",
      "SV: [-0.6251231  0.5312401 -1.5734736]\n",
      "Reward for action 20: -109.3949612881691\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 689\n",
      "Steps done: 5085\n",
      "SV: [ 0.11353812 -0.20024832 -1.227664  ]\n",
      "Reward for action 7: -129.40200196632995\n",
      "[0, 1, 7]\n",
      "Steps done: 5086\n",
      "SV: [ 0.11353812 -0.20024832 -1.227664  ]\n",
      "Reward for action 20: -125.40200196632995\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 690\n",
      "Steps done: 5087\n",
      "SV: [-0.79188055  0.53731287 -5.280799  ]\n",
      "Reward for action 2: -1.4753210631151144\n",
      "[0, 1, 2]\n",
      "Steps done: 5088\n",
      "SV: [-0.79188055  0.53731287 -5.280799  ]\n",
      "Reward for action 11: -401.523964458192\n",
      "[0, 2]\n",
      "Steps done: 5089\n",
      "SV: [-0.79188055  0.53731287 -5.280799  ]\n",
      "Reward for action 3: -863.1649047585925\n",
      "[0, 2, 3]\n",
      "Steps done: 5090\n",
      "SV: [-0.79188055  0.53731287 -5.280799  ]\n",
      "Reward for action 12: -1159.355889995697\n",
      "[0, 3]\n",
      "Steps done: 5091\n",
      "SV: [-0.79188055  0.53731287 -5.280799  ]\n",
      "Reward for action 20: -1155.355889995697\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 691\n",
      "Steps done: 5092\n",
      "SV: [-0.23626454  0.09776469 -0.9305052 ]\n",
      "Reward for action 5: -360.51374569755404\n",
      "[0, 1, 5]\n",
      "Steps done: 5093\n",
      "SV: [-0.23626454  0.09776469 -0.9305052 ]\n",
      "Reward for action 20: -356.51374569755404\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 692\n",
      "Steps done: 5094\n",
      "SV: [ 0.07661348  0.08441271 -0.17234771]\n",
      "Reward for action 2: -13.848108304596444\n",
      "[0, 1, 2]\n",
      "Steps done: 5095\n",
      "SV: [ 0.07661348  0.08441271 -0.17234771]\n",
      "Reward for action 8: -1.351629831273017\n",
      "[0, 1, 2, 8]\n",
      "Steps done: 5096\n",
      "SV: [ 0.07661348  0.08441271 -0.17234771]\n",
      "Reward for action 11: -5.8610435188004395\n",
      "[0, 2, 8]\n",
      "Steps done: 5097\n",
      "SV: [ 0.07661348  0.08441271 -0.17234771]\n",
      "Reward for action 12: -5.736387819141616\n",
      "[0, 8]\n",
      "Steps done: 5098\n",
      "SV: [ 0.07661348  0.08441271 -0.17234771]\n",
      "Reward for action 20: -1.7363878191416164\n",
      "[0, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 693\n",
      "Steps done: 5099\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 7: -100.63060087095451\n",
      "[0, 1, 7]\n",
      "Did target update\n",
      "Steps done: 5100\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 17: -99.63208541684783\n",
      "[0, 1]\n",
      "Steps done: 5101\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 5: -83.54972863609144\n",
      "[0, 1, 5]\n",
      "Steps done: 5102\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 11: -76.45922204563813\n",
      "[0, 5]\n",
      "Steps done: 5103\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 8: -17.26927060846334\n",
      "[0, 5, 8]\n",
      "Steps done: 5104\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 1: -42.63437377782659\n",
      "[0, 5, 8, 1]\n",
      "Steps done: 5105\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 18: -83.54972863609144\n",
      "[0, 5, 1]\n",
      "Steps done: 5106\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 6: -83.97738798233681\n",
      "[0, 5, 1, 6]\n",
      "Steps done: 5107\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 3: -74.41768651435116\n",
      "[0, 5, 1, 6, 3]\n",
      "Steps done: 5108\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 8: -54.57845839124943\n",
      "[0, 5, 1, 6, 3, 8]\n",
      "Steps done: 5109\n",
      "SV: [ 0.01552611  0.257934   -0.9418392 ]\n",
      "Reward for action 20: -50.57845839124943\n",
      "[0, 5, 1, 6, 3, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 694\n",
      "Steps done: 5110\n",
      "SV: [ 1.315572    0.12316165 -0.37424433]\n",
      "Reward for action 3: -96.54063521777351\n",
      "[0, 1, 3]\n",
      "Steps done: 5111\n",
      "SV: [ 1.315572    0.12316165 -0.37424433]\n",
      "Reward for action 11: -100.50245845532096\n",
      "[0, 3]\n",
      "Steps done: 5112\n",
      "SV: [ 1.315572    0.12316165 -0.37424433]\n",
      "Reward for action 5: -7.920713618753864\n",
      "[0, 3, 5]\n",
      "Steps done: 5113\n",
      "SV: [ 1.315572    0.12316165 -0.37424433]\n",
      "Reward for action 15: -100.50245845532096\n",
      "[0, 3]\n",
      "Steps done: 5114\n",
      "SV: [ 1.315572    0.12316165 -0.37424433]\n",
      "Reward for action 20: -96.50245845532096\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 695\n",
      "Steps done: 5115\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 4: -13.119868455365568\n",
      "[0, 1, 4]\n",
      "Steps done: 5116\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 2: -26.91585365786774\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 5117\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 14: -97.87088747108115\n",
      "[0, 1, 2]\n",
      "Steps done: 5118\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 4: -26.91585365786773\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 5119\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 7: -42.764105568850276\n",
      "[0, 1, 2, 4, 7]\n",
      "Steps done: 5120\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 17: -26.91585365786773\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 5121\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 14: -97.87088747108115\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5122\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 11: -340.8088004892812\n",
      "[0, 2]\n",
      "Steps done: 5123\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 1: -97.87088747108115\n",
      "[0, 2, 1]\n",
      "Steps done: 5124\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 11: -340.8088004892812\n",
      "[0, 2]\n",
      "Steps done: 5125\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 7: -14.49197478032288\n",
      "[0, 2, 7]\n",
      "Steps done: 5126\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 1: -13.174740388714232\n",
      "[0, 2, 7, 1]\n",
      "Steps done: 5127\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 3: -14.732447568054098\n",
      "[0, 2, 7, 1, 3]\n",
      "Steps done: 5128\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 11: -22.573974707163867\n",
      "[0, 2, 7, 3]\n",
      "Steps done: 5129\n",
      "SV: [ 0.15778367  0.06996189 -0.5092578 ]\n",
      "Reward for action 20: -18.573974707163867\n",
      "[0, 2, 7, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 696\n",
      "Steps done: 5130\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 5: -11.958719001938139\n",
      "[0, 1, 5]\n",
      "Steps done: 5131\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 15: -12.711422519236944\n",
      "[0, 1]\n",
      "Steps done: 5132\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 2: -82.54626121379923\n",
      "[0, 1, 2]\n",
      "Steps done: 5133\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 11: -152.19658114958617\n",
      "[0, 2]\n",
      "Steps done: 5134\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 4: -83.58014655948048\n",
      "[0, 2, 4]\n",
      "Steps done: 5135\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 1: -66.1618315934084\n",
      "[0, 2, 4, 1]\n",
      "Steps done: 5136\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 10: -89.41094537873609\n",
      "[2, 4, 1]\n",
      "Steps done: 5137\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 0: -66.1618315934084\n",
      "[2, 4, 1, 0]\n",
      "Steps done: 5138\n",
      "SV: [ 0.5681273 -1.0685357  0.3708826]\n",
      "Reward for action 20: -62.161831593408394\n",
      "[2, 4, 1, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 697\n",
      "Steps done: 5139\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 6: -127.07253641412511\n",
      "[0, 1, 6]\n",
      "Steps done: 5140\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 11: -711.1211393507349\n",
      "[0, 6]\n",
      "Steps done: 5141\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 3: -724.4300564262492\n",
      "[0, 6, 3]\n",
      "Steps done: 5142\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 2: -441.5230339172749\n",
      "[0, 6, 3, 2]\n",
      "Steps done: 5143\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 12: -724.4300564262492\n",
      "[0, 6, 3]\n",
      "Steps done: 5144\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 4: -668.3711377192811\n",
      "[0, 6, 3, 4]\n",
      "Steps done: 5145\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 14: -724.4300564262492\n",
      "[0, 6, 3]\n",
      "Steps done: 5146\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 7: -694.9765745375946\n",
      "[0, 6, 3, 7]\n",
      "Steps done: 5147\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 17: -724.4300564262492\n",
      "[0, 6, 3]\n",
      "Steps done: 5148\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 7: -694.9765745375946\n",
      "[0, 6, 3, 7]\n",
      "Steps done: 5149\n",
      "SV: [-0.37666616  0.6230113   2.9646664 ]\n",
      "Reward for action 20: -690.9765745375946\n",
      "[0, 6, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 698\n",
      "Steps done: 5150\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 8: -136.08652692244016\n",
      "[0, 1, 8]\n",
      "Steps done: 5151\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 11: -187.0566444985592\n",
      "[0, 8]\n",
      "Steps done: 5152\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 1: -136.08652692244016\n",
      "[0, 8, 1]\n",
      "Steps done: 5153\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 11: -187.0566444985592\n",
      "[0, 8]\n",
      "Steps done: 5154\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 7: -103.68338912321182\n",
      "[0, 8, 7]\n",
      "Steps done: 5155\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 10: -73.30595065603953\n",
      "[8, 7]\n",
      "Steps done: 5156\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 0: -103.68338912321182\n",
      "[8, 7, 0]\n",
      "Steps done: 5157\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 3: -100.64933819335513\n",
      "[8, 7, 0, 3]\n",
      "Steps done: 5158\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 17: -107.76826078427204\n",
      "[8, 0, 3]\n",
      "Steps done: 5159\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 10: -112.18452366830587\n",
      "[8, 3]\n",
      "Steps done: 5160\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 6: -99.50464166251504\n",
      "[8, 3, 6]\n",
      "Steps done: 5161\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 1: -112.46896156440548\n",
      "[8, 3, 6, 1]\n",
      "Steps done: 5162\n",
      "SV: [-1.15913    -0.43831888  0.4203887 ]\n",
      "Reward for action 20: -108.46896156440548\n",
      "[8, 3, 6, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 699\n",
      "Steps done: 5163\n",
      "SV: [0.1198375 0.2546052 0.2120674]\n",
      "Reward for action 2: -0.9855942476003603\n",
      "[0, 1, 2]\n",
      "Steps done: 5164\n",
      "SV: [0.1198375 0.2546052 0.2120674]\n",
      "Reward for action 11: -5.957848855347917\n",
      "[0, 2]\n",
      "Steps done: 5165\n",
      "SV: [0.1198375 0.2546052 0.2120674]\n",
      "Reward for action 20: -1.9578488553479172\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 700\n",
      "Steps done: 5166\n",
      "SV: [-0.12597877  0.04972911 -0.84503746]\n",
      "Reward for action 20: -33.36148238409487\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 701\n",
      "Steps done: 5167\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 4: -23.970512888287388\n",
      "[0, 1, 4]\n",
      "Steps done: 5168\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 10: -20.744054220752954\n",
      "[1, 4]\n",
      "Steps done: 5169\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 9: -23.434264479414715\n",
      "[1, 4, 9]\n",
      "Steps done: 5170\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 2: -21.752267614642275\n",
      "[1, 4, 9, 2]\n",
      "Steps done: 5171\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 19: -20.734120420393843\n",
      "[1, 4, 2]\n",
      "Steps done: 5172\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 5: -18.088916630584585\n",
      "[1, 4, 2, 5]\n",
      "Steps done: 5173\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 0: -19.45164686772847\n",
      "[1, 4, 2, 5, 0]\n",
      "Steps done: 5174\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 14: -17.11269628521771\n",
      "[1, 2, 5, 0]\n",
      "Steps done: 5175\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 15: -19.2679408919958\n",
      "[1, 2, 0]\n",
      "Steps done: 5176\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 6: -16.83180358238352\n",
      "[1, 2, 0, 6]\n",
      "Steps done: 5177\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 10: -15.54805865731476\n",
      "[1, 2, 6]\n",
      "Steps done: 5178\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 5: -15.640471292310522\n",
      "[1, 2, 6, 5]\n",
      "Steps done: 5179\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 11: -13.884068488330724\n",
      "[2, 6, 5]\n",
      "Steps done: 5180\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 9: -15.935984986229832\n",
      "[2, 6, 5, 9]\n",
      "Steps done: 5181\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 16: -16.72382742905192\n",
      "[2, 5, 9]\n",
      "Steps done: 5182\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 4: -20.062076578652594\n",
      "[2, 5, 9, 4]\n",
      "Steps done: 5183\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 3: -11.10464592833762\n",
      "[2, 5, 9, 4, 3]\n",
      "Steps done: 5184\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 13: -20.062076578652594\n",
      "[2, 5, 9, 4]\n",
      "Steps done: 5185\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 8: -21.103944722854926\n",
      "[2, 5, 9, 4, 8]\n",
      "Steps done: 5186\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 15: -25.48746818591072\n",
      "[2, 9, 4, 8]\n",
      "Steps done: 5187\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 19: -23.15770693768369\n",
      "[2, 4, 8]\n",
      "Steps done: 5188\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 1: -21.091512388728816\n",
      "[2, 4, 8, 1]\n",
      "Steps done: 5189\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 9: -22.81636968151924\n",
      "[2, 4, 8, 1, 9]\n",
      "Steps done: 5190\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 3: -7.156596963735605\n",
      "[2, 4, 8, 1, 9, 3]\n",
      "Steps done: 5191\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 7: -7.945955546851527\n",
      "[2, 4, 8, 1, 9, 3, 7]\n",
      "Steps done: 5192\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Reward for action 14: -7.958172377620029\n",
      "[2, 8, 1, 9, 3, 7]\n",
      "Steps done: 5193\n",
      "SV: [-0.01637423 -0.09001807 -0.47897926]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -3.958172377620029\n",
      "[2, 8, 1, 9, 3, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 702\n",
      "Steps done: 5194\n",
      "SV: [ 0.9631529 -0.8397315  1.4441658]\n",
      "Reward for action 20: -185.94537861276297\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 703\n",
      "Steps done: 5195\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 5: -224.2627194348012\n",
      "[0, 1, 5]\n",
      "Steps done: 5196\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 11: -118.5910663713678\n",
      "[0, 5]\n",
      "Steps done: 5197\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 4: -67.49960983343382\n",
      "[0, 5, 4]\n",
      "Steps done: 5198\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 2: -71.86551504602176\n",
      "[0, 5, 4, 2]\n",
      "Steps done: 5199\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 10: -126.71133260012795\n",
      "[5, 4, 2]\n",
      "Did target update\n",
      "Steps done: 5200\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 12: -41.86569373282173\n",
      "[5, 4]\n",
      "Steps done: 5201\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 3: -13.473280345784165\n",
      "[5, 4, 3]\n",
      "Steps done: 5202\n",
      "SV: [-0.15075228  1.1634108  -1.9601816 ]\n",
      "Reward for action 20: -9.473280345784165\n",
      "[5, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 704\n",
      "Steps done: 5203\n",
      "SV: [-0.8794288  1.5315837 -1.797315 ]\n",
      "Reward for action 3: -76.54809760931893\n",
      "[0, 1, 3]\n",
      "Steps done: 5204\n",
      "SV: [-0.8794288  1.5315837 -1.797315 ]\n",
      "Reward for action 11: -8.809360082344433\n",
      "[0, 3]\n",
      "Steps done: 5205\n",
      "SV: [-0.8794288  1.5315837 -1.797315 ]\n",
      "Reward for action 4: -5.122948029192497\n",
      "[0, 3, 4]\n",
      "Steps done: 5206\n",
      "SV: [-0.8794288  1.5315837 -1.797315 ]\n",
      "Reward for action 20: -1.1229480291924974\n",
      "[0, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 705\n",
      "Steps done: 5207\n",
      "SV: [-0.0369485  0.5174848 -1.1676362]\n",
      "Reward for action 3: -125.52150763422131\n",
      "[0, 1, 3]\n",
      "Steps done: 5208\n",
      "SV: [-0.0369485  0.5174848 -1.1676362]\n",
      "Reward for action 10: -128.43673731403462\n",
      "[1, 3]\n",
      "Steps done: 5209\n",
      "SV: [-0.0369485  0.5174848 -1.1676362]\n",
      "Reward for action 4: -117.00831384718381\n",
      "[1, 3, 4]\n",
      "Steps done: 5210\n",
      "SV: [-0.0369485  0.5174848 -1.1676362]\n",
      "Reward for action 13: -95.80778362014875\n",
      "[1, 4]\n",
      "Steps done: 5211\n",
      "SV: [-0.0369485  0.5174848 -1.1676362]\n",
      "Reward for action 2: -42.69275606500153\n",
      "[1, 4, 2]\n",
      "Steps done: 5212\n",
      "SV: [-0.0369485  0.5174848 -1.1676362]\n",
      "Reward for action 20: -38.69275606500153\n",
      "[1, 4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 706\n",
      "Steps done: 5213\n",
      "SV: [0.19940056 0.31971794 1.1549089 ]\n",
      "Reward for action 20: -496.9857897076206\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 707\n",
      "Steps done: 5214\n",
      "SV: [ 1.1214322   0.59559816 -0.255174  ]\n",
      "Reward for action 6: -24.531415734388354\n",
      "[0, 1, 6]\n",
      "Steps done: 5215\n",
      "SV: [ 1.1214322   0.59559816 -0.255174  ]\n",
      "Reward for action 11: -131.17604201934023\n",
      "[0, 6]\n",
      "Steps done: 5216\n",
      "SV: [ 1.1214322   0.59559816 -0.255174  ]\n",
      "Reward for action 20: -127.17604201934023\n",
      "[0, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 708\n",
      "Steps done: 5217\n",
      "SV: [-0.37300342  0.4234961  -0.6110599 ]\n",
      "Reward for action 2: -510.24965211485426\n",
      "[0, 1, 2]\n",
      "Steps done: 5218\n",
      "SV: [-0.37300342  0.4234961  -0.6110599 ]\n",
      "Reward for action 12: -398.71023815453475\n",
      "[0, 1]\n",
      "Steps done: 5219\n",
      "SV: [-0.37300342  0.4234961  -0.6110599 ]\n",
      "Reward for action 20: -394.71023815453475\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 709\n",
      "Steps done: 5220\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 7: -24.850851078441185\n",
      "[0, 1, 7]\n",
      "Steps done: 5221\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 11: -29.312076533115615\n",
      "[0, 7]\n",
      "Steps done: 5222\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 2: -27.89917835190195\n",
      "[0, 7, 2]\n",
      "Steps done: 5223\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 17: -21.520950627586572\n",
      "[0, 2]\n",
      "Steps done: 5224\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 6: -27.588375912563194\n",
      "[0, 2, 6]\n",
      "Steps done: 5225\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 16: -21.520950627586572\n",
      "[0, 2]\n",
      "Steps done: 5226\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 4: -23.673584333248034\n",
      "[0, 2, 4]\n",
      "Steps done: 5227\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 3: -23.997220967027573\n",
      "[0, 2, 4, 3]\n",
      "Steps done: 5228\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 6: -25.48792106321827\n",
      "[0, 2, 4, 3, 6]\n",
      "Steps done: 5229\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 7: -26.70817928102462\n",
      "[0, 2, 4, 3, 6, 7]\n",
      "Steps done: 5230\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 17: -25.48792106321827\n",
      "[0, 2, 4, 3, 6]\n",
      "Steps done: 5231\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 7: -26.70817928102462\n",
      "[0, 2, 4, 3, 6, 7]\n",
      "Steps done: 5232\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 12: -26.616334246231972\n",
      "[0, 4, 3, 6, 7]\n",
      "Steps done: 5233\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 17: -25.604967527265924\n",
      "[0, 4, 3, 6]\n",
      "Steps done: 5234\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 10: -24.943042154891003\n",
      "[4, 3, 6]\n",
      "Steps done: 5235\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 7: -26.352075537607224\n",
      "[4, 3, 6, 7]\n",
      "Steps done: 5236\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 8: -27.400549915822566\n",
      "[4, 3, 6, 7, 8]\n",
      "Steps done: 5237\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 17: -26.91946159029955\n",
      "[4, 3, 6, 8]\n",
      "Steps done: 5238\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 2: -27.36025993847811\n",
      "[4, 3, 6, 8, 2]\n",
      "Steps done: 5239\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 13: -29.163158224396437\n",
      "[4, 6, 8, 2]\n",
      "Steps done: 5240\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 14: -31.173853554328915\n",
      "[6, 8, 2]\n",
      "Steps done: 5241\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 7: -30.69800497716216\n",
      "[6, 8, 2, 7]\n",
      "Steps done: 5242\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 0: -29.251907730351885\n",
      "[6, 8, 2, 7, 0]\n",
      "Steps done: 5243\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 17: -28.76241022436364\n",
      "[6, 8, 2, 0]\n",
      "Steps done: 5244\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 7: -29.251907730351885\n",
      "[6, 8, 2, 0, 7]\n",
      "Steps done: 5245\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Reward for action 16: -29.037497050031977\n",
      "[8, 2, 0, 7]\n",
      "Steps done: 5246\n",
      "SV: [0.03699416 0.02118669 0.55369127]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -25.037497050031977\n",
      "[8, 2, 0, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 710\n",
      "Steps done: 5247\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 7: -15.084936131640326\n",
      "[0, 1, 7]\n",
      "Steps done: 5248\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 2: -5.724672388889378\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5249\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 6: -8.859485277518496\n",
      "[0, 1, 7, 2, 6]\n",
      "Steps done: 5250\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 17: -5.630233924031174\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 5251\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 12: -13.654875420525919\n",
      "[0, 1, 6]\n",
      "Steps done: 5252\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 16: -13.139706341204649\n",
      "[0, 1]\n",
      "Steps done: 5253\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 3: -11.61608968151037\n",
      "[0, 1, 3]\n",
      "Steps done: 5254\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 4: -11.872616951501454\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 5255\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 14: -11.61608968151037\n",
      "[0, 1, 3]\n",
      "Steps done: 5256\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 7: -13.4805977731218\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 5257\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 13: -15.084936131640326\n",
      "[0, 1, 7]\n",
      "Steps done: 5258\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 2: -5.724672388889378\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5259\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 17: -2.416516397132966\n",
      "[0, 1, 2]\n",
      "Steps done: 5260\n",
      "SV: [-0.15106669  0.05550531 -0.36689934]\n",
      "Reward for action 20: 1.5834836028670338\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 711\n",
      "Steps done: 5261\n",
      "SV: [ 0.11489183  0.17320544 -0.1444843 ]\n",
      "Reward for action 7: -4.946241042471431\n",
      "[0, 1, 7]\n",
      "Steps done: 5262\n",
      "SV: [ 0.11489183  0.17320544 -0.1444843 ]\n",
      "Reward for action 11: -4.817264765767167\n",
      "[0, 7]\n",
      "Steps done: 5263\n",
      "SV: [ 0.11489183  0.17320544 -0.1444843 ]\n",
      "Reward for action 8: -22.868830927180593\n",
      "[0, 7, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5264\n",
      "SV: [ 0.11489183  0.17320544 -0.1444843 ]\n",
      "Reward for action 9: -19.918654760258597\n",
      "[0, 7, 8, 9]\n",
      "Steps done: 5265\n",
      "SV: [ 0.11489183  0.17320544 -0.1444843 ]\n",
      "Reward for action 20: -15.918654760258597\n",
      "[0, 7, 8, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 712\n",
      "Steps done: 5266\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 6: -25.31280086093942\n",
      "[0, 1, 6]\n",
      "Steps done: 5267\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 16: -27.549311571459533\n",
      "[0, 1]\n",
      "Steps done: 5268\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 6: -25.31280086093942\n",
      "[0, 1, 6]\n",
      "Steps done: 5269\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 10: -21.057410510971728\n",
      "[1, 6]\n",
      "Steps done: 5270\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 5: -45.78325817699034\n",
      "[1, 6, 5]\n",
      "Steps done: 5271\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 0: -22.89611494201261\n",
      "[1, 6, 5, 0]\n",
      "Steps done: 5272\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 3: -8.719575404896862\n",
      "[1, 6, 5, 0, 3]\n",
      "Steps done: 5273\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 2: -10.475308849788295\n",
      "[1, 6, 5, 0, 3, 2]\n",
      "Steps done: 5274\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 11: -50.47996034072831\n",
      "[6, 5, 0, 3, 2]\n",
      "Steps done: 5275\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 15: -78.88363056208951\n",
      "[6, 0, 3, 2]\n",
      "Steps done: 5276\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 10: -107.10651670102632\n",
      "[6, 3, 2]\n",
      "Steps done: 5277\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 13: -358.09522000902183\n",
      "[6, 2]\n",
      "Steps done: 5278\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 3: -107.10651670102634\n",
      "[6, 2, 3]\n",
      "Steps done: 5279\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 13: -358.09522000902183\n",
      "[6, 2]\n",
      "Steps done: 5280\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 1: -35.89868773932392\n",
      "[6, 2, 1]\n",
      "Steps done: 5281\n",
      "SV: [-0.25383708  0.12086227  0.31200802]\n",
      "Reward for action 20: -31.89868773932392\n",
      "[6, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 713\n",
      "Steps done: 5282\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 5: -15.649578468201485\n",
      "[0, 1, 5]\n",
      "Steps done: 5283\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 4: -10.884112106730266\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 5284\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 6: -12.350521852444311\n",
      "[0, 1, 5, 4, 6]\n",
      "Steps done: 5285\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 14: -18.665483250986977\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 5286\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 10: -24.375067707974026\n",
      "[1, 5, 6]\n",
      "Steps done: 5287\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 0: -18.66548325098697\n",
      "[1, 5, 6, 0]\n",
      "Steps done: 5288\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 15: -16.13130022696122\n",
      "[1, 6, 0]\n",
      "Steps done: 5289\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 16: -15.53684130877969\n",
      "[1, 0]\n",
      "Steps done: 5290\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 6: -16.131300226961223\n",
      "[1, 0, 6]\n",
      "Steps done: 5291\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 16: -15.53684130877969\n",
      "[1, 0]\n",
      "Steps done: 5292\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 6: -16.131300226961223\n",
      "[1, 0, 6]\n",
      "Steps done: 5293\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 7: -17.180218139917347\n",
      "[1, 0, 6, 7]\n",
      "Steps done: 5294\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 4: -13.791963653120137\n",
      "[1, 0, 6, 7, 4]\n",
      "Steps done: 5295\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 14: -17.180218139917347\n",
      "[1, 0, 6, 7]\n",
      "Steps done: 5296\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 11: -16.224843301413728\n",
      "[0, 6, 7]\n",
      "Steps done: 5297\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 2: -16.873775534693998\n",
      "[0, 6, 7, 2]\n",
      "Steps done: 5298\n",
      "SV: [ 0.08535117 -0.11826424 -0.39508137]\n",
      "Reward for action 20: -12.873775534693998\n",
      "[0, 6, 7, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 714\n",
      "Steps done: 5299\n",
      "SV: [ 0.7573742   0.30743358 -2.9382658 ]\n",
      "Reward for action 20: -682.0959103647971\n",
      "[0, 1]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 715\n",
      "Steps done: 5300\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 7: -109.44592872999526\n",
      "[0, 1, 7]\n",
      "Steps done: 5301\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 11: -108.63643638183444\n",
      "[0, 7]\n",
      "Steps done: 5302\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 5: -103.65946285877669\n",
      "[0, 7, 5]\n",
      "Steps done: 5303\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 2: -106.94549882857596\n",
      "[0, 7, 5, 2]\n",
      "Steps done: 5304\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 8: -169.40259079165088\n",
      "[0, 7, 5, 2, 8]\n",
      "Steps done: 5305\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 10: -193.84082269629772\n",
      "[7, 5, 2, 8]\n",
      "Steps done: 5306\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 9: -59.5838574579018\n",
      "[7, 5, 2, 8, 9]\n",
      "Steps done: 5307\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 18: -59.52325294820437\n",
      "[7, 5, 2, 9]\n",
      "Steps done: 5308\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 8: -59.5838574579018\n",
      "[7, 5, 2, 9, 8]\n",
      "Steps done: 5309\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 18: -59.52325294820437\n",
      "[7, 5, 2, 9]\n",
      "Steps done: 5310\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 4: -67.53561232895206\n",
      "[7, 5, 2, 9, 4]\n",
      "Steps done: 5311\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 19: -108.4724798659106\n",
      "[7, 5, 2, 4]\n",
      "Steps done: 5312\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 8: -169.47230996666002\n",
      "[7, 5, 2, 4, 8]\n",
      "Steps done: 5313\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 18: -108.4724798659106\n",
      "[7, 5, 2, 4]\n",
      "Steps done: 5314\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 1: -109.35621346405121\n",
      "[7, 5, 2, 4, 1]\n",
      "Steps done: 5315\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 14: -110.62715004770723\n",
      "[7, 5, 2, 1]\n",
      "Steps done: 5316\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 8: -170.90132588280775\n",
      "[7, 5, 2, 1, 8]\n",
      "Steps done: 5317\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 11: -193.84082269629772\n",
      "[7, 5, 2, 8]\n",
      "Steps done: 5318\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 18: -110.44775996392418\n",
      "[7, 5, 2]\n",
      "Steps done: 5319\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 8: -193.84082269629772\n",
      "[7, 5, 2, 8]\n",
      "Steps done: 5320\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 18: -110.44775996392418\n",
      "[7, 5, 2]\n",
      "Steps done: 5321\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 12: -109.76647042796654\n",
      "[7, 5]\n",
      "Steps done: 5322\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 4: -106.35698978898154\n",
      "[7, 5, 4]\n",
      "Steps done: 5323\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 1: -108.3749327399985\n",
      "[7, 5, 4, 1]\n",
      "Steps done: 5324\n",
      "SV: [-0.13585715 -0.08883976 -1.1468614 ]\n",
      "Reward for action 20: -104.3749327399985\n",
      "[7, 5, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 716\n",
      "Steps done: 5325\n",
      "SV: [ 0.44261727 -0.03502273 -0.05882637]\n",
      "Reward for action 20: -18.769902110705164\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 717\n",
      "Steps done: 5326\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 7: -82.9241062719557\n",
      "[0, 1, 7]\n",
      "Steps done: 5327\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 2: -89.72858818668956\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5328\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 4: -73.71292267638466\n",
      "[0, 1, 7, 2, 4]\n",
      "Steps done: 5329\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 6: -71.91564013756722\n",
      "[0, 1, 7, 2, 4, 6]\n",
      "Steps done: 5330\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 17: -70.81275842757253\n",
      "[0, 1, 2, 4, 6]\n",
      "Steps done: 5331\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 7: -71.91564013756722\n",
      "[0, 1, 2, 4, 6, 7]\n",
      "Steps done: 5332\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 17: -70.81275842757253\n",
      "[0, 1, 2, 4, 6]\n",
      "Steps done: 5333\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 14: -79.20215431568039\n",
      "[0, 1, 2, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5334\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 12: -66.77230024518292\n",
      "[0, 1, 6]\n",
      "Steps done: 5335\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 4: -70.55852788334273\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 5336\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 8: -68.20966252995106\n",
      "[0, 1, 6, 4, 8]\n",
      "Steps done: 5337\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 7: -68.07413140918136\n",
      "[0, 1, 6, 4, 8, 7]\n",
      "Steps done: 5338\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 5: -67.23375822081321\n",
      "[0, 1, 6, 4, 8, 7, 5]\n",
      "Steps done: 5339\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 3: -67.205580767509\n",
      "[0, 1, 6, 4, 8, 7, 5, 3]\n",
      "Steps done: 5340\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 17: -66.70724604912763\n",
      "[0, 1, 6, 4, 8, 5, 3]\n",
      "Steps done: 5341\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 7: -67.205580767509\n",
      "[0, 1, 6, 4, 8, 5, 3, 7]\n",
      "Steps done: 5342\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 17: -66.70724604912763\n",
      "[0, 1, 6, 4, 8, 5, 3]\n",
      "Steps done: 5343\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 7: -67.205580767509\n",
      "[0, 1, 6, 4, 8, 5, 3, 7]\n",
      "Steps done: 5344\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 17: -66.70724604912763\n",
      "[0, 1, 6, 4, 8, 5, 3]\n",
      "Steps done: 5345\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 14: -65.68660696300333\n",
      "[0, 1, 6, 8, 5, 3]\n",
      "Steps done: 5346\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 13: -64.70970912996945\n",
      "[0, 1, 6, 8, 5]\n",
      "Steps done: 5347\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 16: -63.458797888544495\n",
      "[0, 1, 8, 5]\n",
      "Steps done: 5348\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 7: -66.40906280997044\n",
      "[0, 1, 8, 5, 7]\n",
      "Steps done: 5349\n",
      "SV: [0.01800446 0.01703244 0.91320354]\n",
      "Reward for action 20: -62.409062809970436\n",
      "[0, 1, 8, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 718\n",
      "Steps done: 5350\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 5: -1.8165450527184506\n",
      "[0, 1, 5]\n",
      "Steps done: 5351\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 6: -1.6387273945224408\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 5352\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 2: -1.583990886362986\n",
      "[0, 1, 5, 6, 2]\n",
      "Steps done: 5353\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 4: -1.6226609388105566\n",
      "[0, 1, 5, 6, 2, 4]\n",
      "Steps done: 5354\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 3: -1.5898770837657161\n",
      "[0, 1, 5, 6, 2, 4, 3]\n",
      "Steps done: 5355\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 12: -1.6535994672363792\n",
      "[0, 1, 5, 6, 4, 3]\n",
      "Steps done: 5356\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 16: -1.721650588473297\n",
      "[0, 1, 5, 4, 3]\n",
      "Steps done: 5357\n",
      "SV: [-0.02139457  0.07877085  0.11175784]\n",
      "Reward for action 20: 2.278349411526703\n",
      "[0, 1, 5, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 719\n",
      "Steps done: 5358\n",
      "SV: [-0.44703197  0.04951273  0.16769789]\n",
      "Reward for action 2: -5.629398204016758\n",
      "[0, 1, 2]\n",
      "Steps done: 5359\n",
      "SV: [-0.44703197  0.04951273  0.16769789]\n",
      "Reward for action 10: -0.5907797323148697\n",
      "[1, 2]\n",
      "Steps done: 5360\n",
      "SV: [-0.44703197  0.04951273  0.16769789]\n",
      "Reward for action 3: -28.367097892720068\n",
      "[1, 2, 3]\n",
      "Steps done: 5361\n",
      "SV: [-0.44703197  0.04951273  0.16769789]\n",
      "Reward for action 12: -47.17562462749772\n",
      "[1, 3]\n",
      "Steps done: 5362\n",
      "SV: [-0.44703197  0.04951273  0.16769789]\n",
      "Reward for action 20: -43.17562462749772\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 720\n",
      "Steps done: 5363\n",
      "SV: [ 0.04551544 -0.05839238  0.2951696 ]\n",
      "Reward for action 3: -125.71681078750089\n",
      "[0, 1, 3]\n",
      "Steps done: 5364\n",
      "SV: [ 0.04551544 -0.05839238  0.2951696 ]\n",
      "Reward for action 11: -278.0228151999702\n",
      "[0, 3]\n",
      "Steps done: 5365\n",
      "SV: [ 0.04551544 -0.05839238  0.2951696 ]\n",
      "Reward for action 20: -274.0228151999702\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 721\n",
      "Steps done: 5366\n",
      "SV: [ 0.18864897 -0.3842711  -0.02626592]\n",
      "Reward for action 20: -10.14965216710888\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 722\n",
      "Steps done: 5367\n",
      "SV: [-0.92257416 -0.622348   -0.3077055 ]\n",
      "Reward for action 20: -105.05916918194787\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 723\n",
      "Steps done: 5368\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 6: -63.72788210708278\n",
      "[0, 1, 6]\n",
      "Steps done: 5369\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 11: -5.091714225476\n",
      "[0, 6]\n",
      "Steps done: 5370\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 2: -14.03389799588032\n",
      "[0, 6, 2]\n",
      "Steps done: 5371\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 7: -2.0510444451475474\n",
      "[0, 6, 2, 7]\n",
      "Steps done: 5372\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 10: -13.296079597765015\n",
      "[6, 2, 7]\n",
      "Steps done: 5373\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 12: -3.952413538661473\n",
      "[6, 7]\n",
      "Steps done: 5374\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 8: -6.19649307986119\n",
      "[6, 7, 8]\n",
      "Steps done: 5375\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 4: -17.237970070894278\n",
      "[6, 7, 8, 4]\n",
      "Steps done: 5376\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 1: -49.06448706774354\n",
      "[6, 7, 8, 4, 1]\n",
      "Steps done: 5377\n",
      "SV: [-0.21261357 -1.0967635   1.6972374 ]\n",
      "Reward for action 20: -45.06448706774354\n",
      "[6, 7, 8, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 724\n",
      "Steps done: 5378\n",
      "SV: [ 0.03873914  0.064416   -0.05608168]\n",
      "Reward for action 5: -0.1387172505054712\n",
      "[0, 1, 5]\n",
      "Steps done: 5379\n",
      "SV: [ 0.03873914  0.064416   -0.05608168]\n",
      "Reward for action 10: -0.32867374853391795\n",
      "[1, 5]\n",
      "Steps done: 5380\n",
      "SV: [ 0.03873914  0.064416   -0.05608168]\n",
      "Reward for action 6: -0.16392742346788108\n",
      "[1, 5, 6]\n",
      "Steps done: 5381\n",
      "SV: [ 0.03873914  0.064416   -0.05608168]\n",
      "Reward for action 8: -0.09229763419823206\n",
      "[1, 5, 6, 8]\n",
      "Steps done: 5382\n",
      "SV: [ 0.03873914  0.064416   -0.05608168]\n",
      "Reward for action 16: -0.1223360136848117\n",
      "[1, 5, 8]\n",
      "Steps done: 5383\n",
      "SV: [ 0.03873914  0.064416   -0.05608168]\n",
      "Reward for action 20: 3.8776639863151883\n",
      "[1, 5, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 725\n",
      "Steps done: 5384\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 6: -77.3070384754372\n",
      "[0, 1, 6]\n",
      "Steps done: 5385\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 10: -137.52769549363612\n",
      "[1, 6]\n",
      "Steps done: 5386\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 3: -132.3170819407211\n",
      "[1, 6, 3]\n",
      "Steps done: 5387\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 4: -109.5751087709031\n",
      "[1, 6, 3, 4]\n",
      "Steps done: 5388\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 2: -69.8415690696832\n",
      "[1, 6, 3, 4, 2]\n",
      "Steps done: 5389\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 12: -109.5751087709031\n",
      "[1, 6, 3, 4]\n",
      "Steps done: 5390\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 14: -132.3170819407211\n",
      "[1, 6, 3]\n",
      "Steps done: 5391\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 4: -109.5751087709031\n",
      "[1, 6, 3, 4]\n",
      "Steps done: 5392\n",
      "SV: [ 0.5194594   0.12563907 -0.5148783 ]\n",
      "Reward for action 20: -105.5751087709031\n",
      "[1, 6, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 726\n",
      "Steps done: 5393\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 7: -133.17574061806852\n",
      "[0, 1, 7]\n",
      "Steps done: 5394\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 10: -177.23669241482065\n",
      "[1, 7]\n",
      "Steps done: 5395\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 5: -188.21316377653707\n",
      "[1, 7, 5]\n",
      "Steps done: 5396\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 17: -492.36086810584544\n",
      "[1, 5]\n",
      "Steps done: 5397\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 7: -188.21316377653713\n",
      "[1, 5, 7]\n",
      "Steps done: 5398\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 11: -321.84157739127\n",
      "[5, 7]\n",
      "Steps done: 5399\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 2: -136.8393460366225\n",
      "[5, 7, 2]\n",
      "Did target update\n",
      "Steps done: 5400\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 0: -169.49637661917762\n",
      "[5, 7, 2, 0]\n",
      "Steps done: 5401\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 6: -218.0224893684645\n",
      "[5, 7, 2, 0, 6]\n",
      "Steps done: 5402\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 10: -200.65052173172535\n",
      "[5, 7, 2, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5403\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 16: -136.8393460366225\n",
      "[5, 7, 2]\n",
      "Steps done: 5404\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 6: -200.65052173172535\n",
      "[5, 7, 2, 6]\n",
      "Steps done: 5405\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 16: -136.8393460366225\n",
      "[5, 7, 2]\n",
      "Steps done: 5406\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 6: -200.65052173172535\n",
      "[5, 7, 2, 6]\n",
      "Steps done: 5407\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 4: -245.85025600795515\n",
      "[5, 7, 2, 6, 4]\n",
      "Steps done: 5408\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 3: -274.4024075225896\n",
      "[5, 7, 2, 6, 4, 3]\n",
      "Steps done: 5409\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 15: -242.11639993694405\n",
      "[7, 2, 6, 4, 3]\n",
      "Steps done: 5410\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 1: -212.23719743902126\n",
      "[7, 2, 6, 4, 3, 1]\n",
      "Steps done: 5411\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 8: -207.78241038190973\n",
      "[7, 2, 6, 4, 3, 1, 8]\n",
      "Steps done: 5412\n",
      "SV: [-1.2097133   0.08304902 -1.7832636 ]\n",
      "Reward for action 20: -203.78241038190973\n",
      "[7, 2, 6, 4, 3, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 727\n",
      "Steps done: 5413\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 6: -4.216200469317847\n",
      "[0, 1, 6]\n",
      "Steps done: 5414\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 7: -4.842094636615787\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 5415\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 11: -5.321598455177423\n",
      "[0, 6, 7]\n",
      "Steps done: 5416\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 8: -5.155938573965953\n",
      "[0, 6, 7, 8]\n",
      "Steps done: 5417\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 1: -4.914525777896241\n",
      "[0, 6, 7, 8, 1]\n",
      "Steps done: 5418\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 16: -4.9393251405079335\n",
      "[0, 7, 8, 1]\n",
      "Steps done: 5419\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 11: -4.701721282574147\n",
      "[0, 7, 8]\n",
      "Steps done: 5420\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 4: -5.0431671827223585\n",
      "[0, 7, 8, 4]\n",
      "Steps done: 5421\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 18: -5.271921884139765\n",
      "[0, 7, 4]\n",
      "Steps done: 5422\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 8: -5.043167182722359\n",
      "[0, 7, 4, 8]\n",
      "Steps done: 5423\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 6: -5.231774810934283\n",
      "[0, 7, 4, 8, 6]\n",
      "Steps done: 5424\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 5: -4.015170895485918\n",
      "[0, 7, 4, 8, 6, 5]\n",
      "Steps done: 5425\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 3: -2.3856656283195172\n",
      "[0, 7, 4, 8, 6, 5, 3]\n",
      "Steps done: 5426\n",
      "SV: [-0.11654886  0.14817196 -0.11732233]\n",
      "Reward for action 20: 1.6143343716804828\n",
      "[0, 7, 4, 8, 6, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 728\n",
      "Steps done: 5427\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 4: -16.524919051144238\n",
      "[0, 1, 4]\n",
      "Steps done: 5428\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 11: -18.80890422479424\n",
      "[0, 4]\n",
      "Steps done: 5429\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 3: -33.85013608910142\n",
      "[0, 4, 3]\n",
      "Steps done: 5430\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 5: -31.61597975603379\n",
      "[0, 4, 3, 5]\n",
      "Steps done: 5431\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 15: -33.85013608910142\n",
      "[0, 4, 3]\n",
      "Steps done: 5432\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 2: -22.146522149630385\n",
      "[0, 4, 3, 2]\n",
      "Steps done: 5433\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 5: -23.955655506261888\n",
      "[0, 4, 3, 2, 5]\n",
      "Steps done: 5434\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 6: -53.031111330137534\n",
      "[0, 4, 3, 2, 5, 6]\n",
      "Steps done: 5435\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 15: -60.07528919038339\n",
      "[0, 4, 3, 2, 6]\n",
      "Steps done: 5436\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 5: -53.03111133013755\n",
      "[0, 4, 3, 2, 6, 5]\n",
      "Steps done: 5437\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 1: -40.826383240046944\n",
      "[0, 4, 3, 2, 6, 5, 1]\n",
      "Steps done: 5438\n",
      "SV: [-0.0180361  -0.17586458 -0.4211165 ]\n",
      "Reward for action 20: -36.826383240046944\n",
      "[0, 4, 3, 2, 6, 5, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 729\n",
      "Steps done: 5439\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 3: -109.22953067373285\n",
      "[0, 1, 3]\n",
      "Steps done: 5440\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 11: -111.46735059250359\n",
      "[0, 3]\n",
      "Steps done: 5441\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 1: -109.22953067373288\n",
      "[0, 3, 1]\n",
      "Steps done: 5442\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 11: -111.46735059250359\n",
      "[0, 3]\n",
      "Steps done: 5443\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 6: -137.81990037090947\n",
      "[0, 3, 6]\n",
      "Steps done: 5444\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 4: -111.25522138486188\n",
      "[0, 3, 6, 4]\n",
      "Steps done: 5445\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 16: -106.98782715733097\n",
      "[0, 3, 4]\n",
      "Steps done: 5446\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 13: -104.36417078377633\n",
      "[0, 4]\n",
      "Steps done: 5447\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 5: -77.6759712330788\n",
      "[0, 4, 5]\n",
      "Steps done: 5448\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 10: -61.011453451677724\n",
      "[4, 5]\n",
      "Steps done: 5449\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 0: -77.6759712330788\n",
      "[4, 5, 0]\n",
      "Steps done: 5450\n",
      "SV: [ 0.45609757  0.1355332  -1.028892  ]\n",
      "Reward for action 20: -73.6759712330788\n",
      "[4, 5, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 730\n",
      "Steps done: 5451\n",
      "SV: [-0.7233686  -0.08871436 -0.90118724]\n",
      "Reward for action 5: -127.40335904694953\n",
      "[0, 1, 5]\n",
      "Steps done: 5452\n",
      "SV: [-0.7233686  -0.08871436 -0.90118724]\n",
      "Reward for action 20: -123.40335904694953\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 731\n",
      "Steps done: 5453\n",
      "SV: [ 0.09498443 -0.08597055  0.19320033]\n",
      "Reward for action 20: -3.2120714858210286\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 732\n",
      "Steps done: 5454\n",
      "SV: [-0.6912896  -0.02977822  0.2865834 ]\n",
      "Reward for action 20: -61.0568605881998\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 733\n",
      "Steps done: 5455\n",
      "SV: [-0.2645539  0.6855486 -3.604778 ]\n",
      "Reward for action 20: -456.779146337772\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 734\n",
      "Steps done: 5456\n",
      "SV: [-1.1900115 -0.6593632 -0.9527506]\n",
      "Reward for action 7: -38.19025962309816\n",
      "[0, 1, 7]\n",
      "Steps done: 5457\n",
      "SV: [-1.1900115 -0.6593632 -0.9527506]\n",
      "Reward for action 20: -34.19025962309816\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 735\n",
      "Steps done: 5458\n",
      "SV: [-1.3482862  1.1758043 -0.536329 ]\n",
      "Reward for action 20: -150.49765996435207\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 736\n",
      "Steps done: 5459\n",
      "SV: [-0.06165413  0.07777873 -0.31126067]\n",
      "Reward for action 6: -10.534812649882173\n",
      "[0, 1, 6]\n",
      "Steps done: 5460\n",
      "SV: [-0.06165413  0.07777873 -0.31126067]\n",
      "Reward for action 10: -10.859094810614938\n",
      "[1, 6]\n",
      "Steps done: 5461\n",
      "SV: [-0.06165413  0.07777873 -0.31126067]\n",
      "Reward for action 9: -61.418253065792875\n",
      "[1, 6, 9]\n",
      "Steps done: 5462\n",
      "SV: [-0.06165413  0.07777873 -0.31126067]\n",
      "Reward for action 20: -57.418253065792875\n",
      "[1, 6, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 737\n",
      "Steps done: 5463\n",
      "SV: [ 0.19541676 -0.02481969 -0.47127804]\n",
      "Reward for action 20: -17.32742785919527\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 738\n",
      "Steps done: 5464\n",
      "SV: [ 0.3879319  -0.64271176 -0.31898087]\n",
      "Reward for action 4: -53.31466876538004\n",
      "[0, 1, 4]\n",
      "Steps done: 5465\n",
      "SV: [ 0.3879319  -0.64271176 -0.31898087]\n",
      "Reward for action 20: -49.31466876538004\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 739\n",
      "Steps done: 5466\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 3: -109.28803966697161\n",
      "[0, 1, 3]\n",
      "Steps done: 5467\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 11: -74.53779692817494\n",
      "[0, 3]\n",
      "Steps done: 5468\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 7: -14.106199128113579\n",
      "[0, 3, 7]\n",
      "Steps done: 5469\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 9: -79.56498861913111\n",
      "[0, 3, 7, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5470\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 8: -66.8155558143606\n",
      "[0, 3, 7, 9, 8]\n",
      "Steps done: 5471\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 19: -29.66584822589511\n",
      "[0, 3, 7, 8]\n",
      "Steps done: 5472\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 9: -66.8155558143606\n",
      "[0, 3, 7, 8, 9]\n",
      "Steps done: 5473\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 2: -85.06540384397107\n",
      "[0, 3, 7, 8, 9, 2]\n",
      "Steps done: 5474\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 17: -81.55540667032852\n",
      "[0, 3, 8, 9, 2]\n",
      "Steps done: 5475\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 4: -75.00336102909381\n",
      "[0, 3, 8, 9, 2, 4]\n",
      "Steps done: 5476\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 13: -68.0530550861668\n",
      "[0, 8, 9, 2, 4]\n",
      "Steps done: 5477\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 10: -61.92747806754758\n",
      "[8, 9, 2, 4]\n",
      "Steps done: 5478\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 7: -67.15945516915124\n",
      "[8, 9, 2, 4, 7]\n",
      "Steps done: 5479\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 5: -30.50190022213695\n",
      "[8, 9, 2, 4, 7, 5]\n",
      "Steps done: 5480\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 15: -67.15945516915124\n",
      "[8, 9, 2, 4, 7]\n",
      "Steps done: 5481\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 1: -53.91000929929352\n",
      "[8, 9, 2, 4, 7, 1]\n",
      "Steps done: 5482\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 12: -72.20120337440802\n",
      "[8, 9, 4, 7, 1]\n",
      "Steps done: 5483\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 5: -28.86777931995029\n",
      "[8, 9, 4, 7, 1, 5]\n",
      "Steps done: 5484\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 14: -13.427327824015881\n",
      "[8, 9, 7, 1, 5]\n",
      "Steps done: 5485\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 4: -28.867779319950294\n",
      "[8, 9, 7, 1, 5, 4]\n",
      "Steps done: 5486\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 17: -69.48979458209753\n",
      "[8, 9, 1, 5, 4]\n",
      "Steps done: 5487\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 11: -77.80834506034972\n",
      "[8, 9, 5, 4]\n",
      "Steps done: 5488\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 3: -85.09609050101474\n",
      "[8, 9, 5, 4, 3]\n",
      "Steps done: 5489\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 0: -59.79522736467665\n",
      "[8, 9, 5, 4, 3, 0]\n",
      "Steps done: 5490\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 15: -57.16540112270141\n",
      "[8, 9, 4, 3, 0]\n",
      "Steps done: 5491\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Reward for action 19: -22.049759771437863\n",
      "[8, 4, 3, 0]\n",
      "Steps done: 5492\n",
      "SV: [-0.2546118  -0.04117632  0.6437659 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -18.049759771437863\n",
      "[8, 4, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 740\n",
      "Steps done: 5493\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 3: -88.89935416584738\n",
      "[0, 1, 3]\n",
      "Steps done: 5494\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 11: -98.93638474637284\n",
      "[0, 3]\n",
      "Steps done: 5495\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 4: -78.16920416748619\n",
      "[0, 3, 4]\n",
      "Steps done: 5496\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 6: -32.109271538626196\n",
      "[0, 3, 4, 6]\n",
      "Steps done: 5497\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 14: -11.674456052994241\n",
      "[0, 3, 6]\n",
      "Steps done: 5498\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 4: -32.109271538626174\n",
      "[0, 3, 6, 4]\n",
      "Steps done: 5499\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 10: -52.76304590079284\n",
      "[3, 6, 4]\n",
      "Did target update\n",
      "Steps done: 5500\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 14: -3.840259891256527\n",
      "[3, 6]\n",
      "Steps done: 5501\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 4: -52.76304590079284\n",
      "[3, 6, 4]\n",
      "Steps done: 5502\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 5: -90.43613749348776\n",
      "[3, 6, 4, 5]\n",
      "Steps done: 5503\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 16: -79.23561734023133\n",
      "[3, 4, 5]\n",
      "Steps done: 5504\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 6: -90.43613749348776\n",
      "[3, 4, 5, 6]\n",
      "Steps done: 5505\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 13: -203.64693367232746\n",
      "[4, 5, 6]\n",
      "Steps done: 5506\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 1: -162.64762063462064\n",
      "[4, 5, 6, 1]\n",
      "Steps done: 5507\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 3: -95.74359581195613\n",
      "[4, 5, 6, 1, 3]\n",
      "Steps done: 5508\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 15: -77.3647294494305\n",
      "[4, 6, 1, 3]\n",
      "Steps done: 5509\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 11: -52.76304590079284\n",
      "[4, 6, 3]\n",
      "Steps done: 5510\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 5: -90.43613749348776\n",
      "[4, 6, 3, 5]\n",
      "Steps done: 5511\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 15: -52.76304590079284\n",
      "[4, 6, 3]\n",
      "Steps done: 5512\n",
      "SV: [ 0.16886859 -0.10408992  0.7411469 ]\n",
      "Reward for action 20: -48.76304590079284\n",
      "[4, 6, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 741\n",
      "Steps done: 5513\n",
      "SV: [ 0.218127  -1.6055841 -1.0350066]\n",
      "Reward for action 20: -74.96213901035703\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 742\n",
      "Steps done: 5514\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 3: -68.52046911159444\n",
      "[0, 1, 3]\n",
      "Steps done: 5515\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 2: -73.05046007645676\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 5516\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 11: -50.399891605515805\n",
      "[0, 3, 2]\n",
      "Steps done: 5517\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 13: -87.30474552135347\n",
      "[0, 2]\n",
      "Steps done: 5518\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 6: -136.61913223597892\n",
      "[0, 2, 6]\n",
      "Steps done: 5519\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 3: -101.67486326962094\n",
      "[0, 2, 6, 3]\n",
      "Steps done: 5520\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 7: -55.71386769293038\n",
      "[0, 2, 6, 3, 7]\n",
      "Steps done: 5521\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 17: -101.67486326962094\n",
      "[0, 2, 6, 3]\n",
      "Steps done: 5522\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 16: -50.3998916055158\n",
      "[0, 2, 3]\n",
      "Steps done: 5523\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 4: -158.79614615687268\n",
      "[0, 2, 3, 4]\n",
      "Steps done: 5524\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 9: -141.89312107828997\n",
      "[0, 2, 3, 4, 9]\n",
      "Steps done: 5525\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 7: -84.42360969275808\n",
      "[0, 2, 3, 4, 9, 7]\n",
      "Steps done: 5526\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 17: -141.89312107828997\n",
      "[0, 2, 3, 4, 9]\n",
      "Steps done: 5527\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 7: -84.42360969275808\n",
      "[0, 2, 3, 4, 9, 7]\n",
      "Steps done: 5528\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 17: -141.89312107828997\n",
      "[0, 2, 3, 4, 9]\n",
      "Steps done: 5529\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 7: -84.42360969275808\n",
      "[0, 2, 3, 4, 9, 7]\n",
      "Steps done: 5530\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 1: -91.46807927872138\n",
      "[0, 2, 3, 4, 9, 7, 1]\n",
      "Steps done: 5531\n",
      "SV: [ 0.35512576 -0.19064334 -0.9046456 ]\n",
      "Reward for action 20: -87.46807927872138\n",
      "[0, 2, 3, 4, 9, 7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 743\n",
      "Steps done: 5532\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 7: -15.488779793599763\n",
      "[0, 1, 7]\n",
      "Steps done: 5533\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 2: -118.25320210942307\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5534\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 12: -15.488779793599763\n",
      "[0, 1, 7]\n",
      "Steps done: 5535\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 6: -8.07141243699066\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 5536\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 11: -54.41304267129834\n",
      "[0, 7, 6]\n",
      "Steps done: 5537\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 3: -1.5380088226170967\n",
      "[0, 7, 6, 3]\n",
      "Steps done: 5538\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 17: -1.2501464450065405\n",
      "[0, 6, 3]\n",
      "Steps done: 5539\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 13: -328.227040944043\n",
      "[0, 6]\n",
      "Steps done: 5540\n",
      "SV: [0.2796477 0.5080288 1.825337 ]\n",
      "Reward for action 20: -324.227040944043\n",
      "[0, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 744\n",
      "Steps done: 5541\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 6: -3.160035784280538\n",
      "[0, 1, 6]\n",
      "Steps done: 5542\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 10: -6.587590291400728\n",
      "[1, 6]\n",
      "Steps done: 5543\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 2: -5.496817471640567\n",
      "[1, 6, 2]\n",
      "Steps done: 5544\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 7: -31.9088475588758\n",
      "[1, 6, 2, 7]\n",
      "Steps done: 5545\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 8: -149.04909959150746\n",
      "[1, 6, 2, 7, 8]\n",
      "Steps done: 5546\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 17: -119.21737507528755\n",
      "[1, 6, 2, 8]\n",
      "Steps done: 5547\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 4: -113.61368617367025\n",
      "[1, 6, 2, 8, 4]\n",
      "Steps done: 5548\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 14: -119.21737507528755\n",
      "[1, 6, 2, 8]\n",
      "Steps done: 5549\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 3: -176.97072377762112\n",
      "[1, 6, 2, 8, 3]\n",
      "Steps done: 5550\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 13: -119.21737507528755\n",
      "[1, 6, 2, 8]\n",
      "Steps done: 5551\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 11: -97.37757650053213\n",
      "[6, 2, 8]\n",
      "Steps done: 5552\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 12: -1.8759522643636912\n",
      "[6, 8]\n",
      "Steps done: 5553\n",
      "SV: [ 0.03483607  0.12597789 -0.28760815]\n",
      "Reward for action 20: 2.1240477356363088\n",
      "[6, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 745\n",
      "Steps done: 5554\n",
      "SV: [ 0.29403663  0.19316538 -3.5917535 ]\n",
      "Reward for action 20: -105.48997071269666\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 746\n",
      "Steps done: 5555\n",
      "SV: [0.5036911  0.5843035  0.65151316]\n",
      "Reward for action 20: -144.19612751638633\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 747\n",
      "Steps done: 5556\n",
      "SV: [ 0.17343575 -0.57787067 -0.43239266]\n",
      "Reward for action 7: -3.423720928442575\n",
      "[0, 1, 7]\n",
      "Steps done: 5557\n",
      "SV: [ 0.17343575 -0.57787067 -0.43239266]\n",
      "Reward for action 20: 0.5762790715574249\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 748\n",
      "Steps done: 5558\n",
      "SV: [-0.19622596 -0.32674935  1.0290412 ]\n",
      "Reward for action 7: -104.00083753473974\n",
      "[0, 1, 7]\n",
      "Steps done: 5559\n",
      "SV: [-0.19622596 -0.32674935  1.0290412 ]\n",
      "Reward for action 11: -57.7478945036843\n",
      "[0, 7]\n",
      "Steps done: 5560\n",
      "SV: [-0.19622596 -0.32674935  1.0290412 ]\n",
      "Reward for action 5: -65.3045437592775\n",
      "[0, 7, 5]\n",
      "Steps done: 5561\n",
      "SV: [-0.19622596 -0.32674935  1.0290412 ]\n",
      "Reward for action 20: -61.30454375927749\n",
      "[0, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 749\n",
      "Steps done: 5562\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 9: -2.098709726837876\n",
      "[0, 1, 9]\n",
      "Steps done: 5563\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 3: -2.0926241530699965\n",
      "[0, 1, 9, 3]\n",
      "Steps done: 5564\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 8: -4.617402664152375\n",
      "[0, 1, 9, 3, 8]\n",
      "Steps done: 5565\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 2: -4.703552936706392\n",
      "[0, 1, 9, 3, 8, 2]\n",
      "Steps done: 5566\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 18: -2.659476119086666\n",
      "[0, 1, 9, 3, 2]\n",
      "Steps done: 5567\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 12: -2.0926241530699965\n",
      "[0, 1, 9, 3]\n",
      "Steps done: 5568\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 19: -16.06088510068269\n",
      "[0, 1, 3]\n",
      "Steps done: 5569\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 13: -13.795204092042452\n",
      "[0, 1]\n",
      "Steps done: 5570\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 8: -8.925742332403024\n",
      "[0, 1, 8]\n",
      "Steps done: 5571\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 10: -5.851056127006339\n",
      "[1, 8]\n",
      "Steps done: 5572\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 5: -6.215634885922686\n",
      "[1, 8, 5]\n",
      "Steps done: 5573\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 2: -4.593540435361183\n",
      "[1, 8, 5, 2]\n",
      "Steps done: 5574\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 11: -9.635359592412486\n",
      "[8, 5, 2]\n",
      "Steps done: 5575\n",
      "SV: [-0.27785698 -0.09624059  0.3043906 ]\n",
      "Reward for action 20: -5.635359592412486\n",
      "[8, 5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 750\n",
      "Steps done: 5576\n",
      "SV: [-1.4626269 -0.8348797  2.9412072]\n",
      "Reward for action 20: -294.8722495212198\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 751\n",
      "Steps done: 5577\n",
      "SV: [ 0.31477028  0.12873289 -0.5452131 ]\n",
      "Reward for action 2: -83.75810192353094\n",
      "[0, 1, 2]\n",
      "Steps done: 5578\n",
      "SV: [ 0.31477028  0.12873289 -0.5452131 ]\n",
      "Reward for action 20: -79.75810192353094\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 752\n",
      "Steps done: 5579\n",
      "SV: [-0.19081879  0.17524706  0.8737117 ]\n",
      "Reward for action 2: -68.79908360070507\n",
      "[0, 1, 2]\n",
      "Steps done: 5580\n",
      "SV: [-0.19081879  0.17524706  0.8737117 ]\n",
      "Reward for action 20: -64.79908360070507\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 753\n",
      "Steps done: 5581\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 3: -236.22340996791766\n",
      "[0, 1, 3]\n",
      "Steps done: 5582\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 10: -913.1143699645168\n",
      "[1, 3]\n",
      "Steps done: 5583\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 0: -236.22340996791772\n",
      "[1, 3, 0]\n",
      "Steps done: 5584\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 4: -549.777903934764\n",
      "[1, 3, 0, 4]\n",
      "Steps done: 5585\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 11: -368.64706569270754\n",
      "[3, 0, 4]\n",
      "Steps done: 5586\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 13: -974.8521452008356\n",
      "[0, 4]\n",
      "Steps done: 5587\n",
      "SV: [-0.48704568 -0.08535942 -2.055594  ]\n",
      "Reward for action 20: -970.8521452008356\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 754\n",
      "Steps done: 5588\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 2: -25.476244213219687\n",
      "[0, 1, 2]\n",
      "Steps done: 5589\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 8: -29.070792486311085\n",
      "[0, 1, 2, 8]\n",
      "Steps done: 5590\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 4: -21.36751769354013\n",
      "[0, 1, 2, 8, 4]\n",
      "Steps done: 5591\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 12: -4.819950206520209\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 5592\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 11: -7.5988384063942025\n",
      "[0, 8, 4]\n",
      "Steps done: 5593\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 2: -33.483671954997135\n",
      "[0, 8, 4, 2]\n",
      "Steps done: 5594\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 1: -21.36751769354014\n",
      "[0, 8, 4, 2, 1]\n",
      "Steps done: 5595\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 11: -33.483671954997135\n",
      "[0, 8, 4, 2]\n",
      "Steps done: 5596\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 1: -21.36751769354014\n",
      "[0, 8, 4, 2, 1]\n",
      "Steps done: 5597\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 3: -18.80274256906347\n",
      "[0, 8, 4, 2, 1, 3]\n",
      "Steps done: 5598\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 9: -0.3563602777878312\n",
      "[0, 8, 4, 2, 1, 3, 9]\n",
      "Steps done: 5599\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 10: -1.7924833927362216\n",
      "[8, 4, 2, 1, 3, 9]\n",
      "Did target update\n",
      "Steps done: 5600\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 13: -3.259177126742748\n",
      "[8, 4, 2, 1, 9]\n",
      "Steps done: 5601\n",
      "SV: [ 0.22015916 -0.32016262  0.6620898 ]\n",
      "Reward for action 20: 0.7408228732572519\n",
      "[8, 4, 2, 1, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 755\n",
      "Steps done: 5602\n",
      "SV: [ 0.09058382  0.1634231  -0.10019658]\n",
      "Reward for action 3: -6.110428033569541\n",
      "[0, 1, 3]\n",
      "Steps done: 5603\n",
      "SV: [ 0.09058382  0.1634231  -0.10019658]\n",
      "Reward for action 11: -3.2503934940305887\n",
      "[0, 3]\n",
      "Steps done: 5604\n",
      "SV: [ 0.09058382  0.1634231  -0.10019658]\n",
      "Reward for action 20: 0.7496065059694113\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 756\n",
      "Steps done: 5605\n",
      "SV: [-0.05188885 -0.1379247  -0.10243674]\n",
      "Reward for action 4: -12.634127325017392\n",
      "[0, 1, 4]\n",
      "Steps done: 5606\n",
      "SV: [-0.05188885 -0.1379247  -0.10243674]\n",
      "Reward for action 14: -13.29709585869779\n",
      "[0, 1]\n",
      "Steps done: 5607\n",
      "SV: [-0.05188885 -0.1379247  -0.10243674]\n",
      "Reward for action 20: -9.29709585869779\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 758\n",
      "Steps done: 5608\n",
      "SV: [-0.00428551 -0.00474482 -0.1431746 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 5: -1.154803848100198\n",
      "[0, 1, 5]\n",
      "Steps done: 5609\n",
      "SV: [-0.00428551 -0.00474482 -0.1431746 ]\n",
      "Reward for action 20: 2.8451961518998017\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 759\n",
      "Steps done: 5610\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 7: -20.06210327171794\n",
      "[0, 1, 7]\n",
      "Steps done: 5611\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 2: -181.9696767265086\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5612\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 10: -149.35296871524915\n",
      "[1, 7, 2]\n",
      "Steps done: 5613\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 5: -308.59912602629885\n",
      "[1, 7, 2, 5]\n",
      "Steps done: 5614\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 11: -637.7871483559422\n",
      "[7, 2, 5]\n",
      "Steps done: 5615\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 17: -526.6777783125825\n",
      "[2, 5]\n",
      "Steps done: 5616\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 6: -324.26045980116044\n",
      "[2, 5, 6]\n",
      "Steps done: 5617\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 16: -526.6777783125825\n",
      "[2, 5]\n",
      "Steps done: 5618\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 7: -637.7871483559422\n",
      "[2, 5, 7]\n",
      "Steps done: 5619\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 4: -71.82913886820333\n",
      "[2, 5, 7, 4]\n",
      "Steps done: 5620\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 14: -637.7871483559422\n",
      "[2, 5, 7]\n",
      "Steps done: 5621\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 15: -794.9193694454593\n",
      "[2, 7]\n",
      "Steps done: 5622\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 9: -61.825761910810535\n",
      "[2, 7, 9]\n",
      "Steps done: 5623\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 12: -658.3409281509814\n",
      "[7, 9]\n",
      "Steps done: 5624\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 2: -61.825761910810535\n",
      "[7, 9, 2]\n",
      "Steps done: 5625\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 12: -658.3409281509814\n",
      "[7, 9]\n",
      "Steps done: 5626\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 3: -687.0559178639697\n",
      "[7, 9, 3]\n",
      "Steps done: 5627\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 13: -658.3409281509814\n",
      "[7, 9]\n",
      "Steps done: 5628\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 2: -61.825761910810535\n",
      "[7, 9, 2]\n",
      "Steps done: 5629\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 8: -92.72681969882004\n",
      "[7, 9, 2, 8]\n",
      "Steps done: 5630\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 12: -80.34628130556769\n",
      "[7, 9, 8]\n",
      "Steps done: 5631\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 2: -92.72681969882004\n",
      "[7, 9, 8, 2]\n",
      "Steps done: 5632\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 3: -14.915806819064008\n",
      "[7, 9, 8, 2, 3]\n",
      "Steps done: 5633\n",
      "SV: [-0.37876606 -0.18978028  1.3590163 ]\n",
      "Reward for action 20: -10.915806819064008\n",
      "[7, 9, 8, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 760\n",
      "Steps done: 5634\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 4: -42.92943952447993\n",
      "[0, 1, 4]\n",
      "Steps done: 5635\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 14: -48.226562016033455\n",
      "[0, 1]\n",
      "Steps done: 5636\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 6: -47.061196032738366\n",
      "[0, 1, 6]\n",
      "Steps done: 5637\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 16: -48.226562016033455\n",
      "[0, 1]\n",
      "Steps done: 5638\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 5: -45.937445760768995\n",
      "[0, 1, 5]\n",
      "Steps done: 5639\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 10: -43.304977072316234\n",
      "[1, 5]\n",
      "Steps done: 5640\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 7: -46.37428902162183\n",
      "[1, 5, 7]\n",
      "Steps done: 5641\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 17: -43.304977072316234\n",
      "[1, 5]\n",
      "Steps done: 5642\n",
      "SV: [0.0655063 0.0262056 0.7403093]\n",
      "Reward for action 20: -39.304977072316234\n",
      "[1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 761\n",
      "Steps done: 5643\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 4: -147.17124240232948\n",
      "[0, 1, 4]\n",
      "Steps done: 5644\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 14: -146.38236999469092\n",
      "[0, 1]\n",
      "Steps done: 5645\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 2: -14.747209948447043\n",
      "[0, 1, 2]\n",
      "Steps done: 5646\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 3: -13.403360442726665\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 5647\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 5: -49.09965369367132\n",
      "[0, 1, 2, 3, 5]\n",
      "Steps done: 5648\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 11: -43.13500388108662\n",
      "[0, 2, 3, 5]\n",
      "Steps done: 5649\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 10: -111.41892165285472\n",
      "[2, 3, 5]\n",
      "Steps done: 5650\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 1: -71.02500912244048\n",
      "[2, 3, 5, 1]\n",
      "Steps done: 5651\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 4: -94.69747749640047\n",
      "[2, 3, 5, 1, 4]\n",
      "Steps done: 5652\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 12: -150.29834413743473\n",
      "[3, 5, 1, 4]\n",
      "Steps done: 5653\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 14: -152.574275378189\n",
      "[3, 5, 1]\n",
      "Steps done: 5654\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 11: -155.45014947126793\n",
      "[3, 5]\n",
      "Steps done: 5655\n",
      "SV: [ 2.2555366 -0.8402471  1.0772978]\n",
      "Reward for action 20: -151.45014947126793\n",
      "[3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 762\n",
      "Steps done: 5656\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 8: -10.187204851474524\n",
      "[0, 1, 8]\n",
      "Steps done: 5657\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 3: -1.819792316749481\n",
      "[0, 1, 8, 3]\n",
      "Steps done: 5658\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 9: -8.356360773049323\n",
      "[0, 1, 8, 3, 9]\n",
      "Steps done: 5659\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 6: -12.323797666826394\n",
      "[0, 1, 8, 3, 9, 6]\n",
      "Steps done: 5660\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 2: -8.721716542489322\n",
      "[0, 1, 8, 3, 9, 6, 2]\n",
      "Steps done: 5661\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 7: -8.62423971852864\n",
      "[0, 1, 8, 3, 9, 6, 2, 7]\n",
      "Steps done: 5662\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 16: -7.150325159893832\n",
      "[0, 1, 8, 3, 9, 2, 7]\n",
      "Steps done: 5663\n",
      "SV: [-0.10257583  0.00195686  0.24380541]\n",
      "Reward for action 20: -3.150325159893832\n",
      "[0, 1, 8, 3, 9, 2, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 763\n",
      "Steps done: 5664\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 3: -15.316512988421632\n",
      "[0, 1, 3]\n",
      "Steps done: 5665\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 4: -17.97313509463346\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 5666\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 6: -13.143074835401979\n",
      "[0, 1, 3, 4, 6]\n",
      "Steps done: 5667\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 11: -23.249164592050818\n",
      "[0, 3, 4, 6]\n",
      "Steps done: 5668\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 5: -12.973283991521999\n",
      "[0, 3, 4, 6, 5]\n",
      "Steps done: 5669\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 8: -29.03662076524727\n",
      "[0, 3, 4, 6, 5, 8]\n",
      "Steps done: 5670\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 10: -27.98405611017962\n",
      "[3, 4, 6, 5, 8]\n",
      "Steps done: 5671\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 0: -29.03662076524728\n",
      "[3, 4, 6, 5, 8, 0]\n",
      "Steps done: 5672\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 18: -12.973283991522006\n",
      "[3, 4, 6, 5, 0]\n",
      "Steps done: 5673\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 15: -23.249164592050818\n",
      "[3, 4, 6, 0]\n",
      "Steps done: 5674\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 7: -47.57401453480398\n",
      "[3, 4, 6, 0, 7]\n",
      "Steps done: 5675\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 14: -53.63099067654215\n",
      "[3, 6, 0, 7]\n",
      "Steps done: 5676\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 16: -69.27506358339238\n",
      "[3, 0, 7]\n",
      "Steps done: 5677\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 10: -129.39434960354706\n",
      "[3, 7]\n",
      "Steps done: 5678\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 5: -118.21080615414367\n",
      "[3, 7, 5]\n",
      "Steps done: 5679\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 9: -63.9107752993085\n",
      "[3, 7, 5, 9]\n",
      "Steps done: 5680\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 15: -56.89391297256378\n",
      "[3, 7, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5681\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 5: -63.91077529930849\n",
      "[3, 7, 9, 5]\n",
      "Steps done: 5682\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 13: -90.90595111282335\n",
      "[7, 9, 5]\n",
      "Steps done: 5683\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 4: -26.98991954603653\n",
      "[7, 9, 5, 4]\n",
      "Steps done: 5684\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 14: -90.90595111282335\n",
      "[7, 9, 5]\n",
      "Steps done: 5685\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 3: -63.91077529930849\n",
      "[7, 9, 5, 3]\n",
      "Steps done: 5686\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 15: -56.893912972563804\n",
      "[7, 9, 3]\n",
      "Steps done: 5687\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 19: -129.39434960354706\n",
      "[7, 3]\n",
      "Steps done: 5688\n",
      "SV: [ 0.5037995  -0.25005925 -0.09967658]\n",
      "Reward for action 20: -125.39434960354706\n",
      "[7, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 764\n",
      "Steps done: 5689\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 4: -0.716108184717458\n",
      "[0, 1, 4]\n",
      "Steps done: 5690\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 3: -1.3836058286971549\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 5691\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 7: -3.956089241754824\n",
      "[0, 1, 4, 3, 7]\n",
      "Steps done: 5692\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 17: -1.3836058286971549\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 5693\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 10: -0.8447597006135166\n",
      "[1, 4, 3]\n",
      "Steps done: 5694\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 13: -0.7724712564161208\n",
      "[1, 4]\n",
      "Steps done: 5695\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 0: -0.716108184717458\n",
      "[1, 4, 0]\n",
      "Steps done: 5696\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 2: -0.8912071224872098\n",
      "[1, 4, 0, 2]\n",
      "Steps done: 5697\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 6: -1.1477737470211502\n",
      "[1, 4, 0, 2, 6]\n",
      "Steps done: 5698\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 9: -0.7968733042869783\n",
      "[1, 4, 0, 2, 6, 9]\n",
      "Steps done: 5699\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 16: -0.6829170250284509\n",
      "[1, 4, 0, 2, 9]\n",
      "Did target update\n",
      "Steps done: 5700\n",
      "SV: [0.16725653 0.06532636 0.2789652 ]\n",
      "Reward for action 20: 3.317082974971549\n",
      "[1, 4, 0, 2, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 765\n",
      "Steps done: 5701\n",
      "SV: [-1.220183   0.8069146 -2.1207778]\n",
      "Reward for action 5: -824.5936588579875\n",
      "[0, 1, 5]\n",
      "Steps done: 5702\n",
      "SV: [-1.220183   0.8069146 -2.1207778]\n",
      "Reward for action 15: -918.8039755471713\n",
      "[0, 1]\n",
      "Steps done: 5703\n",
      "SV: [-1.220183   0.8069146 -2.1207778]\n",
      "Reward for action 20: -914.8039755471713\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 766\n",
      "Steps done: 5704\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 2: -96.64078057189651\n",
      "[0, 1, 2]\n",
      "Steps done: 5705\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 3: -92.0766691942571\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 5706\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 11: -91.59001346788047\n",
      "[0, 2, 3]\n",
      "Steps done: 5707\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 12: -85.16741005173148\n",
      "[0, 3]\n",
      "Steps done: 5708\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 5: -88.9318049119881\n",
      "[0, 3, 5]\n",
      "Steps done: 5709\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 10: -93.93136063984504\n",
      "[3, 5]\n",
      "Steps done: 5710\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 4: -90.86356417830247\n",
      "[3, 5, 4]\n",
      "Steps done: 5711\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 15: -90.77789620088332\n",
      "[3, 4]\n",
      "Steps done: 5712\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 0: -88.58972700166262\n",
      "[3, 4, 0]\n",
      "Steps done: 5713\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 5: -89.23973332518173\n",
      "[3, 4, 0, 5]\n",
      "Steps done: 5714\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 15: -88.58972700166262\n",
      "[3, 4, 0]\n",
      "Steps done: 5715\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 14: -85.16741005173148\n",
      "[3, 0]\n",
      "Steps done: 5716\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 5: -88.93180491198805\n",
      "[3, 0, 5]\n",
      "Steps done: 5717\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 2: -91.58577326987287\n",
      "[3, 0, 5, 2]\n",
      "Steps done: 5718\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 15: -91.5900134678805\n",
      "[3, 0, 2]\n",
      "Steps done: 5719\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 5: -91.58577326987287\n",
      "[3, 0, 2, 5]\n",
      "Steps done: 5720\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 13: -94.41038688178845\n",
      "[0, 2, 5]\n",
      "Steps done: 5721\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 3: -91.58577326987292\n",
      "[0, 2, 5, 3]\n",
      "Steps done: 5722\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 4: -90.94945730924591\n",
      "[0, 2, 5, 3, 4]\n",
      "Steps done: 5723\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 12: -89.23973332518173\n",
      "[0, 5, 3, 4]\n",
      "Steps done: 5724\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 13: -88.59163596444283\n",
      "[0, 5, 4]\n",
      "Steps done: 5725\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 15: -89.91833757527661\n",
      "[0, 4]\n",
      "Steps done: 5726\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 2: -94.5707432232046\n",
      "[0, 4, 2]\n",
      "Steps done: 5727\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 1: -94.18539714507125\n",
      "[0, 4, 2, 1]\n",
      "Steps done: 5728\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 14: -96.64078057189651\n",
      "[0, 2, 1]\n",
      "Steps done: 5729\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Reward for action 5: -93.92259721215017\n",
      "[0, 2, 1, 5]\n",
      "Steps done: 5730\n",
      "SV: [-0.04790346 -0.04134599  1.008466  ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -89.92259721215017\n",
      "[0, 2, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 767\n",
      "Steps done: 5731\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 7: -10.246926476505038\n",
      "[0, 1, 7]\n",
      "Steps done: 5732\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 2: -8.118357248342683\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5733\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 10: -6.647483418711004\n",
      "[1, 7, 2]\n",
      "Steps done: 5734\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 17: -22.514249454188956\n",
      "[1, 2]\n",
      "Steps done: 5735\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 4: -11.659793541695672\n",
      "[1, 2, 4]\n",
      "Steps done: 5736\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 14: -22.514249454188956\n",
      "[1, 2]\n",
      "Steps done: 5737\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 5: -10.70576381437147\n",
      "[1, 2, 5]\n",
      "Steps done: 5738\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 3: -8.06676043960054\n",
      "[1, 2, 5, 3]\n",
      "Steps done: 5739\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 15: -8.382706010289638\n",
      "[1, 2, 3]\n",
      "Steps done: 5740\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 5: -8.066760439600543\n",
      "[1, 2, 3, 5]\n",
      "Steps done: 5741\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 15: -8.382706010289638\n",
      "[1, 2, 3]\n",
      "Steps done: 5742\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 6: -28.507129436081605\n",
      "[1, 2, 3, 6]\n",
      "Steps done: 5743\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 0: -19.827367114454752\n",
      "[1, 2, 3, 6, 0]\n",
      "Steps done: 5744\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 4: -15.687922773007127\n",
      "[1, 2, 3, 6, 0, 4]\n",
      "Steps done: 5745\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 14: -19.827367114454752\n",
      "[1, 2, 3, 6, 0]\n",
      "Steps done: 5746\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 12: -29.09775009272866\n",
      "[1, 3, 6, 0]\n",
      "Steps done: 5747\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 4: -20.120778899700174\n",
      "[1, 3, 6, 0, 4]\n",
      "Steps done: 5748\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 14: -29.09775009272866\n",
      "[1, 3, 6, 0]\n",
      "Steps done: 5749\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 5: -23.90280899884856\n",
      "[1, 3, 6, 0, 5]\n",
      "Steps done: 5750\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 7: -18.569043943922154\n",
      "[1, 3, 6, 0, 5, 7]\n",
      "Steps done: 5751\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 15: -22.79348246868982\n",
      "[1, 3, 6, 0, 7]\n",
      "Steps done: 5752\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 17: -29.09775009272866\n",
      "[1, 3, 6, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5753\n",
      "SV: [ 0.01259227  0.26262978 -0.3706995 ]\n",
      "Reward for action 20: -25.09775009272866\n",
      "[1, 3, 6, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 768\n",
      "Steps done: 5754\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 5: -10.739802447113043\n",
      "[0, 1, 5]\n",
      "Steps done: 5755\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 7: -12.338993572728011\n",
      "[0, 1, 5, 7]\n",
      "Steps done: 5756\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 15: -18.807286539833314\n",
      "[0, 1, 7]\n",
      "Steps done: 5757\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 10: -22.180034175998667\n",
      "[1, 7]\n",
      "Steps done: 5758\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 3: -39.22631502503284\n",
      "[1, 7, 3]\n",
      "Steps done: 5759\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 6: -35.88794861644324\n",
      "[1, 7, 3, 6]\n",
      "Steps done: 5760\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 13: -13.791636165266281\n",
      "[1, 7, 6]\n",
      "Steps done: 5761\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 17: -18.31025144394947\n",
      "[1, 6]\n",
      "Steps done: 5762\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 3: -62.11546238925794\n",
      "[1, 6, 3]\n",
      "Steps done: 5763\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 13: -18.31025144394947\n",
      "[1, 6]\n",
      "Steps done: 5764\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 2: -20.2170831057812\n",
      "[1, 6, 2]\n",
      "Steps done: 5765\n",
      "SV: [ 0.33222517  0.1194097  -0.23818778]\n",
      "Reward for action 20: -16.2170831057812\n",
      "[1, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 769\n",
      "Steps done: 5766\n",
      "SV: [-0.26563582 -0.07725813 -1.1731777 ]\n",
      "Reward for action 2: -117.5881204164258\n",
      "[0, 1, 2]\n",
      "Steps done: 5767\n",
      "SV: [-0.26563582 -0.07725813 -1.1731777 ]\n",
      "Reward for action 3: -112.04935651495295\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 5768\n",
      "SV: [-0.26563582 -0.07725813 -1.1731777 ]\n",
      "Reward for action 10: -117.87189623561626\n",
      "[1, 2, 3]\n",
      "Steps done: 5769\n",
      "SV: [-0.26563582 -0.07725813 -1.1731777 ]\n",
      "Reward for action 11: -116.47625363692592\n",
      "[2, 3]\n",
      "Steps done: 5770\n",
      "SV: [-0.26563582 -0.07725813 -1.1731777 ]\n",
      "Reward for action 20: -112.47625363692592\n",
      "[2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 770\n",
      "Steps done: 5771\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 4: -76.57507995606959\n",
      "[0, 1, 4]\n",
      "Steps done: 5772\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 3: -70.51425974597052\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 5773\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 13: -76.57507995606959\n",
      "[0, 1, 4]\n",
      "Steps done: 5774\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 14: -29.16566544661428\n",
      "[0, 1]\n",
      "Steps done: 5775\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 4: -76.57507995606959\n",
      "[0, 1, 4]\n",
      "Steps done: 5776\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 14: -29.16566544661428\n",
      "[0, 1]\n",
      "Steps done: 5777\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 8: -159.8735530972089\n",
      "[0, 1, 8]\n",
      "Steps done: 5778\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 18: -29.16566544661428\n",
      "[0, 1]\n",
      "Steps done: 5779\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 3: -52.72497260638354\n",
      "[0, 1, 3]\n",
      "Steps done: 5780\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 8: -124.83775324803226\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 5781\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 18: -52.72497260638354\n",
      "[0, 1, 3]\n",
      "Steps done: 5782\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 7: -55.95488350383573\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 5783\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 4: -67.80562800872171\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 5784\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 14: -55.95488350383573\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 5785\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 17: -52.72497260638354\n",
      "[0, 1, 3]\n",
      "Steps done: 5786\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 11: -42.755378922872104\n",
      "[0, 3]\n",
      "Steps done: 5787\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 7: -43.77162319075871\n",
      "[0, 3, 7]\n",
      "Steps done: 5788\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 5: -45.91539553672405\n",
      "[0, 3, 7, 5]\n",
      "Steps done: 5789\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 6: -43.424451748793\n",
      "[0, 3, 7, 5, 6]\n",
      "Steps done: 5790\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 15: -44.57937788312416\n",
      "[0, 3, 7, 6]\n",
      "Steps done: 5791\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 17: -42.509276623944004\n",
      "[0, 3, 6]\n",
      "Steps done: 5792\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 7: -44.57937788312416\n",
      "[0, 3, 6, 7]\n",
      "Steps done: 5793\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 1: -53.61959821384263\n",
      "[0, 3, 6, 7, 1]\n",
      "Steps done: 5794\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 5: -53.48619896866252\n",
      "[0, 3, 6, 7, 1, 5]\n",
      "Steps done: 5795\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 11: -43.424451748793\n",
      "[0, 3, 6, 7, 5]\n",
      "Steps done: 5796\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Reward for action 17: -41.73495757599035\n",
      "[0, 3, 6, 5]\n",
      "Steps done: 5797\n",
      "SV: [-0.01015355  0.20477132 -1.0113391 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -37.73495757599035\n",
      "[0, 3, 6, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 771\n",
      "Steps done: 5798\n",
      "SV: [ 0.00247298 -0.04995454  0.55693495]\n",
      "Reward for action 3: -26.02165738221854\n",
      "[0, 1, 3]\n",
      "Steps done: 5799\n",
      "SV: [ 0.00247298 -0.04995454  0.55693495]\n",
      "Reward for action 8: -26.955299018169345\n",
      "[0, 1, 3, 8]\n",
      "Did target update\n",
      "Steps done: 5800\n",
      "SV: [ 0.00247298 -0.04995454  0.55693495]\n",
      "Reward for action 18: -26.02165738221854\n",
      "[0, 1, 3]\n",
      "Steps done: 5801\n",
      "SV: [ 0.00247298 -0.04995454  0.55693495]\n",
      "Reward for action 4: -25.507446252123614\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 5802\n",
      "SV: [ 0.00247298 -0.04995454  0.55693495]\n",
      "Reward for action 6: -25.599145977247584\n",
      "[0, 1, 3, 4, 6]\n",
      "Steps done: 5803\n",
      "SV: [ 0.00247298 -0.04995454  0.55693495]\n",
      "Reward for action 20: -21.599145977247584\n",
      "[0, 1, 3, 4, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 772\n",
      "Steps done: 5804\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 3: -46.5696199936329\n",
      "[0, 1, 3]\n",
      "Steps done: 5805\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 13: -136.35914001828175\n",
      "[0, 1]\n",
      "Steps done: 5806\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 9: -362.97629543334995\n",
      "[0, 1, 9]\n",
      "Steps done: 5807\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 2: -27.22919955264845\n",
      "[0, 1, 9, 2]\n",
      "Steps done: 5808\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 10: -2.201995343173595\n",
      "[1, 9, 2]\n",
      "Steps done: 5809\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 19: -816.659112582731\n",
      "[1, 2]\n",
      "Steps done: 5810\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 6: -15.180471203329299\n",
      "[1, 2, 6]\n",
      "Steps done: 5811\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 16: -816.659112582731\n",
      "[1, 2]\n",
      "Steps done: 5812\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 4: -135.0984764162051\n",
      "[1, 2, 4]\n",
      "Steps done: 5813\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 9: -5.4826228423063785\n",
      "[1, 2, 4, 9]\n",
      "Steps done: 5814\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 12: -272.10684050824983\n",
      "[1, 4, 9]\n",
      "Steps done: 5815\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 14: -355.86726612995807\n",
      "[1, 9]\n",
      "Steps done: 5816\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 3: -38.97491400024034\n",
      "[1, 9, 3]\n",
      "Steps done: 5817\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 13: -355.86726612995807\n",
      "[1, 9]\n",
      "Steps done: 5818\n",
      "SV: [-0.34402052  0.11302111  1.6784123 ]\n",
      "Reward for action 20: -351.86726612995807\n",
      "[1, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 773\n",
      "Steps done: 5819\n",
      "SV: [-0.10088308 -0.44948918 -1.3880733 ]\n",
      "Reward for action 20: -167.52717536662013\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 774\n",
      "Steps done: 5820\n",
      "SV: [-0.00601031 -0.39138168  1.9516782 ]\n",
      "Reward for action 5: -11.809067566049048\n",
      "[0, 1, 5]\n",
      "Steps done: 5821\n",
      "SV: [-0.00601031 -0.39138168  1.9516782 ]\n",
      "Reward for action 10: -236.9473611354138\n",
      "[1, 5]\n",
      "Steps done: 5822\n",
      "SV: [-0.00601031 -0.39138168  1.9516782 ]\n",
      "Reward for action 20: -232.9473611354138\n",
      "[1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 775\n",
      "Steps done: 5823\n",
      "SV: [-0.12387935 -0.15556167  0.5063603 ]\n",
      "Reward for action 3: -12.355583562847219\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5824\n",
      "SV: [-0.12387935 -0.15556167  0.5063603 ]\n",
      "Reward for action 5: -0.4795022650947111\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 5825\n",
      "SV: [-0.12387935 -0.15556167  0.5063603 ]\n",
      "Reward for action 13: -151.04477940962667\n",
      "[0, 1, 5]\n",
      "Steps done: 5826\n",
      "SV: [-0.12387935 -0.15556167  0.5063603 ]\n",
      "Reward for action 3: -0.47950226509471133\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 5827\n",
      "SV: [-0.12387935 -0.15556167  0.5063603 ]\n",
      "Reward for action 20: 3.520497734905289\n",
      "[0, 1, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 776\n",
      "Steps done: 5828\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 4: -12.024745354978359\n",
      "[0, 1, 4]\n",
      "Steps done: 5829\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 11: -0.8748923349262849\n",
      "[0, 4]\n",
      "Steps done: 5830\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 3: -8.14510249100704\n",
      "[0, 4, 3]\n",
      "Steps done: 5831\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 6: -14.541243350920459\n",
      "[0, 4, 3, 6]\n",
      "Steps done: 5832\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 14: -21.413988061912413\n",
      "[0, 3, 6]\n",
      "Steps done: 5833\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 5: -18.162084542474904\n",
      "[0, 3, 6, 5]\n",
      "Steps done: 5834\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 2: -11.900245732168091\n",
      "[0, 3, 6, 5, 2]\n",
      "Steps done: 5835\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 12: -18.162084542474904\n",
      "[0, 3, 6, 5]\n",
      "Steps done: 5836\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 7: -1.321695980389723\n",
      "[0, 3, 6, 5, 7]\n",
      "Steps done: 5837\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 15: -3.662075418748026\n",
      "[0, 3, 6, 7]\n",
      "Steps done: 5838\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 2: -9.218601569589495\n",
      "[0, 3, 6, 7, 2]\n",
      "Steps done: 5839\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 13: -11.555208717378337\n",
      "[0, 6, 7, 2]\n",
      "Steps done: 5840\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 8: -22.851042671047193\n",
      "[0, 6, 7, 2, 8]\n",
      "Steps done: 5841\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 4: -22.19458798874759\n",
      "[0, 6, 7, 2, 8, 4]\n",
      "Steps done: 5842\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 14: -22.851042671047193\n",
      "[0, 6, 7, 2, 8]\n",
      "Steps done: 5843\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 17: -3.753506187957324\n",
      "[0, 6, 2, 8]\n",
      "Steps done: 5844\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 4: -4.584923394730609\n",
      "[0, 6, 2, 8, 4]\n",
      "Steps done: 5845\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 14: -3.753506187957324\n",
      "[0, 6, 2, 8]\n",
      "Steps done: 5846\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 12: -2.2066208939966554\n",
      "[0, 6, 8]\n",
      "Steps done: 5847\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 10: -151.34621505361963\n",
      "[6, 8]\n",
      "Steps done: 5848\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 5: -2.6571288756912566\n",
      "[6, 8, 5]\n",
      "Steps done: 5849\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 2: -0.11596632795698068\n",
      "[6, 8, 5, 2]\n",
      "Steps done: 5850\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 15: -0.48511948087336965\n",
      "[6, 8, 2]\n",
      "Steps done: 5851\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 18: -5.786366131396017\n",
      "[6, 2]\n",
      "Steps done: 5852\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 0: -3.9086312735529383\n",
      "[6, 2, 0]\n",
      "Steps done: 5853\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Reward for action 3: -3.664436698067468\n",
      "[6, 2, 0, 3]\n",
      "Steps done: 5854\n",
      "SV: [ 0.11072023  0.397346   -0.2970253 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: 0.3355633019325319\n",
      "[6, 2, 0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 777\n",
      "Steps done: 5855\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 7: -52.69810848402492\n",
      "[0, 1, 7]\n",
      "Steps done: 5856\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 3: -4.956642071205759\n",
      "[0, 1, 7, 3]\n",
      "Steps done: 5857\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 8: -28.03683175153563\n",
      "[0, 1, 7, 3, 8]\n",
      "Steps done: 5858\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 17: -90.54035040944369\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 5859\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 10: -72.9341944056235\n",
      "[1, 3, 8]\n",
      "Steps done: 5860\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 2: -30.744449844213257\n",
      "[1, 3, 8, 2]\n",
      "Steps done: 5861\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 18: -28.74370033020427\n",
      "[1, 3, 2]\n",
      "Steps done: 5862\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 7: -26.833034604313227\n",
      "[1, 3, 2, 7]\n",
      "Steps done: 5863\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 6: -4.497717018206388\n",
      "[1, 3, 2, 7, 6]\n",
      "Steps done: 5864\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 17: -6.394518550550588\n",
      "[1, 3, 2, 6]\n",
      "Steps done: 5865\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 11: -7.570388807801035\n",
      "[3, 2, 6]\n",
      "Steps done: 5866\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 12: -18.35345866314283\n",
      "[3, 6]\n",
      "Steps done: 5867\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 2: -7.570388807801035\n",
      "[3, 6, 2]\n",
      "Steps done: 5868\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 8: -9.88064194312739\n",
      "[3, 6, 2, 8]\n",
      "Steps done: 5869\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 12: -13.812396517885308\n",
      "[3, 6, 8]\n",
      "Steps done: 5870\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 5: -34.59400347090867\n",
      "[3, 6, 8, 5]\n",
      "Steps done: 5871\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 9: -45.02839019216903\n",
      "[3, 6, 8, 5, 9]\n",
      "Steps done: 5872\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 7: -38.42574512889448\n",
      "[3, 6, 8, 5, 9, 7]\n",
      "Steps done: 5873\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 17: -45.02839019216903\n",
      "[3, 6, 8, 5, 9]\n",
      "Steps done: 5874\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 4: -67.83600229161661\n",
      "[3, 6, 8, 5, 9, 4]\n",
      "Steps done: 5875\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 19: -54.022321145069476\n",
      "[3, 6, 8, 5, 4]\n",
      "Steps done: 5876\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 7: -39.586894753286145\n",
      "[3, 6, 8, 5, 4, 7]\n",
      "Steps done: 5877\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 1: -19.04585511545605\n",
      "[3, 6, 8, 5, 4, 7, 1]\n",
      "Steps done: 5878\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 14: -24.12307163777704\n",
      "[3, 6, 8, 5, 7, 1]\n",
      "Steps done: 5879\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 15: -8.71506017684157\n",
      "[3, 6, 8, 7, 1]\n",
      "Steps done: 5880\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Reward for action 18: -2.590970791257958\n",
      "[3, 6, 7, 1]\n",
      "Steps done: 5881\n",
      "SV: [-0.4753557  -0.19485752  0.69588125]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: 1.4090292087420422\n",
      "[3, 6, 7, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 778\n",
      "Steps done: 5882\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 6: -1.2354257152483723\n",
      "[0, 1, 6]\n",
      "Steps done: 5883\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 16: -17.113139316902455\n",
      "[0, 1]\n",
      "Steps done: 5884\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 6: -1.2354257152483723\n",
      "[0, 1, 6]\n",
      "Steps done: 5885\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 10: -48.87058521725677\n",
      "[1, 6]\n",
      "Steps done: 5886\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 0: -1.2354257152483723\n",
      "[1, 6, 0]\n",
      "Steps done: 5887\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 16: -17.113139316902455\n",
      "[1, 0]\n",
      "Steps done: 5888\n",
      "SV: [ 0.10571523  0.91826886 -3.2471967 ]\n",
      "Reward for action 20: -13.113139316902455\n",
      "[1, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 779\n",
      "Steps done: 5889\n",
      "SV: [-0.10734224  0.63788825  0.95623344]\n",
      "Reward for action 20: -105.29469904676748\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 780\n",
      "Steps done: 5890\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 2: -75.23956110968898\n",
      "[0, 1, 2]\n",
      "Steps done: 5891\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 7: -36.5076080183479\n",
      "[0, 1, 2, 7]\n",
      "Steps done: 5892\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 17: -75.23956110968898\n",
      "[0, 1, 2]\n",
      "Steps done: 5893\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 6: -99.16397307334016\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 5894\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 7: -56.67878823298698\n",
      "[0, 1, 2, 6, 7]\n",
      "Steps done: 5895\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 5: -30.96744553564474\n",
      "[0, 1, 2, 6, 7, 5]\n",
      "Steps done: 5896\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 11: -24.28441937595326\n",
      "[0, 2, 6, 7, 5]\n",
      "Steps done: 5897\n",
      "SV: [-0.2026091  -0.43452698 -0.20314768]\n",
      "Reward for action 20: -20.28441937595326\n",
      "[0, 2, 6, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 781\n",
      "Steps done: 5898\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 2: -9.207307851389547\n",
      "[0, 1, 2]\n",
      "Steps done: 5899\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 7: -8.839605840667659\n",
      "[0, 1, 2, 7]\n",
      "Did target update\n",
      "Steps done: 5900\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 12: -8.612443818360834\n",
      "[0, 1, 7]\n",
      "Steps done: 5901\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 2: -8.839605840667659\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 5902\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 12: -8.612443818360834\n",
      "[0, 1, 7]\n",
      "Steps done: 5903\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 9: -16.613575693917582\n",
      "[0, 1, 7, 9]\n",
      "Steps done: 5904\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 5: -10.642822569536214\n",
      "[0, 1, 7, 9, 5]\n",
      "Steps done: 5905\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 3: -9.098057311628509\n",
      "[0, 1, 7, 9, 5, 3]\n",
      "Steps done: 5906\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 8: -8.76168875666656\n",
      "[0, 1, 7, 9, 5, 3, 8]\n",
      "Steps done: 5907\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 13: -9.250635005400705\n",
      "[0, 1, 7, 9, 5, 8]\n",
      "Steps done: 5908\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 6: -25.479989560702972\n",
      "[0, 1, 7, 9, 5, 8, 6]\n",
      "Steps done: 5909\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 2: -22.213275232417928\n",
      "[0, 1, 7, 9, 5, 8, 6, 2]\n",
      "Steps done: 5910\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 12: -25.479989560702972\n",
      "[0, 1, 7, 9, 5, 8, 6]\n",
      "Steps done: 5911\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 2: -22.213275232417928\n",
      "[0, 1, 7, 9, 5, 8, 6, 2]\n",
      "Steps done: 5912\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 16: -8.724463803600935\n",
      "[0, 1, 7, 9, 5, 8, 2]\n",
      "Steps done: 5913\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 10: -6.92099823852299\n",
      "[1, 7, 9, 5, 8, 2]\n",
      "Steps done: 5914\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 17: -8.13221176139853\n",
      "[1, 9, 5, 8, 2]\n",
      "Steps done: 5915\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 0: -10.304183110251707\n",
      "[1, 9, 5, 8, 2, 0]\n",
      "Steps done: 5916\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 12: -11.833904513051515\n",
      "[1, 9, 5, 8, 0]\n",
      "Steps done: 5917\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 15: -18.10350771605702\n",
      "[1, 9, 8, 0]\n",
      "Steps done: 5918\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 6: -45.63461312908947\n",
      "[1, 9, 8, 0, 6]\n",
      "Steps done: 5919\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 16: -18.10350771605702\n",
      "[1, 9, 8, 0]\n",
      "Steps done: 5920\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 4: -11.66314772809467\n",
      "[1, 9, 8, 0, 4]\n",
      "Steps done: 5921\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 5: -9.263088243193513\n",
      "[1, 9, 8, 0, 4, 5]\n",
      "Steps done: 5922\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 7: -8.200609369742676\n",
      "[1, 9, 8, 0, 4, 5, 7]\n",
      "Steps done: 5923\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Reward for action 10: -6.381599979520994\n",
      "[1, 9, 8, 4, 5, 7]\n",
      "Steps done: 5924\n",
      "SV: [-0.01566853  0.01430813  0.34317908]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -2.3815999795209937\n",
      "[1, 9, 8, 4, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 782\n",
      "Steps done: 5925\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 6: -33.235366252983134\n",
      "[0, 1, 6]\n",
      "Steps done: 5926\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 10: -31.688422901606515\n",
      "[1, 6]\n",
      "Steps done: 5927\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 2: -33.53998133125183\n",
      "[1, 6, 2]\n",
      "Steps done: 5928\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 4: -32.091494854012\n",
      "[1, 6, 2, 4]\n",
      "Steps done: 5929\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 16: -29.465966491234507\n",
      "[1, 2, 4]\n",
      "Steps done: 5930\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 11: -29.59021467129424\n",
      "[2, 4]\n",
      "Steps done: 5931\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 6: -34.7023651096699\n",
      "[2, 4, 6]\n",
      "Steps done: 5932\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 7: -32.67972703907458\n",
      "[2, 4, 6, 7]\n",
      "Steps done: 5933\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 0: -33.720933177775976\n",
      "[2, 4, 6, 7, 0]\n",
      "Steps done: 5934\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 16: -31.313332777602447\n",
      "[2, 4, 7, 0]\n",
      "Steps done: 5935\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 17: -33.72639725392974\n",
      "[2, 4, 0]\n",
      "Steps done: 5936\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 3: -31.283816611626456\n",
      "[2, 4, 0, 3]\n",
      "Steps done: 5937\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 10: -29.46481102007835\n",
      "[2, 4, 3]\n",
      "Steps done: 5938\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 7: -29.397475757108182\n",
      "[2, 4, 3, 7]\n",
      "Steps done: 5939\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 8: -30.150873362145255\n",
      "[2, 4, 3, 7, 8]\n",
      "Steps done: 5940\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 17: -30.757053647799143\n",
      "[2, 4, 3, 8]\n",
      "Steps done: 5941\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 9: -31.297932555468677\n",
      "[2, 4, 3, 8, 9]\n",
      "Steps done: 5942\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 13: -31.298584075008293\n",
      "[2, 4, 8, 9]\n",
      "Steps done: 5943\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 5: -31.684743062374544\n",
      "[2, 4, 8, 9, 5]\n",
      "Steps done: 5944\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 6: -27.074603903854445\n",
      "[2, 4, 8, 9, 5, 6]\n",
      "Steps done: 5945\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 14: -28.416053633769835\n",
      "[2, 8, 9, 5, 6]\n",
      "Steps done: 5946\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 16: -39.71002520235961\n",
      "[2, 8, 9, 5]\n",
      "Steps done: 5947\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 7: -30.5282726575238\n",
      "[2, 8, 9, 5, 7]\n",
      "Steps done: 5948\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 12: -29.2082316521316\n",
      "[8, 9, 5, 7]\n",
      "Steps done: 5949\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 18: -25.85132581162319\n",
      "[9, 5, 7]\n",
      "Steps done: 5950\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Reward for action 4: -27.46904395589062\n",
      "[9, 5, 7, 4]\n",
      "Steps done: 5951\n",
      "SV: [-0.08605187  0.02042851 -0.59507006]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -23.46904395589062\n",
      "[9, 5, 7, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 783\n",
      "Steps done: 5952\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 9: -184.47289493137168\n",
      "[0, 1, 9]\n",
      "Steps done: 5953\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 5: -129.75012880697682\n",
      "[0, 1, 9, 5]\n",
      "Steps done: 5954\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 4: -113.92403725513182\n",
      "[0, 1, 9, 5, 4]\n",
      "Steps done: 5955\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 7: -110.552944351846\n",
      "[0, 1, 9, 5, 4, 7]\n",
      "Steps done: 5956\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 11: -78.74404823210378\n",
      "[0, 9, 5, 4, 7]\n",
      "Steps done: 5957\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 19: -95.38234184743165\n",
      "[0, 5, 4, 7]\n",
      "Steps done: 5958\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 3: -95.05449862992597\n",
      "[0, 5, 4, 7, 3]\n",
      "Steps done: 5959\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 9: -74.59257324401317\n",
      "[0, 5, 4, 7, 3, 9]\n",
      "Steps done: 5960\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 10: -66.95909859152911\n",
      "[5, 4, 7, 3, 9]\n",
      "Steps done: 5961\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 8: -66.59573785678967\n",
      "[5, 4, 7, 3, 9, 8]\n",
      "Steps done: 5962\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 15: -66.17052454268034\n",
      "[4, 7, 3, 9, 8]\n",
      "Steps done: 5963\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 14: -83.46918557670716\n",
      "[7, 3, 9, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 5964\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 4: -66.17052454268031\n",
      "[7, 3, 9, 8, 4]\n",
      "Steps done: 5965\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 14: -83.46918557670716\n",
      "[7, 3, 9, 8]\n",
      "Steps done: 5966\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 4: -66.17052454268031\n",
      "[7, 3, 9, 8, 4]\n",
      "Steps done: 5967\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 13: -68.39407907456068\n",
      "[7, 9, 8, 4]\n",
      "Steps done: 5968\n",
      "SV: [ 0.8888078   0.45271984 -0.7686282 ]\n",
      "Reward for action 20: -64.39407907456068\n",
      "[7, 9, 8, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 784\n",
      "Steps done: 5969\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 9: -69.10630363035733\n",
      "[0, 1, 9]\n",
      "Steps done: 5970\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 8: -112.92510101123534\n",
      "[0, 1, 9, 8]\n",
      "Steps done: 5971\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 11: -131.23476563184624\n",
      "[0, 9, 8]\n",
      "Steps done: 5972\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 2: -88.46855157623693\n",
      "[0, 9, 8, 2]\n",
      "Steps done: 5973\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 5: -86.86680538400216\n",
      "[0, 9, 8, 2, 5]\n",
      "Steps done: 5974\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 18: -54.09893591455255\n",
      "[0, 9, 2, 5]\n",
      "Steps done: 5975\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 12: -80.99045994666412\n",
      "[0, 9, 5]\n",
      "Steps done: 5976\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 4: -68.41498619349758\n",
      "[0, 9, 5, 4]\n",
      "Steps done: 5977\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 7: -57.944717737460465\n",
      "[0, 9, 5, 4, 7]\n",
      "Steps done: 5978\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 10: -74.35848233854833\n",
      "[9, 5, 4, 7]\n",
      "Steps done: 5979\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 6: -45.317902063110566\n",
      "[9, 5, 4, 7, 6]\n",
      "Steps done: 5980\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 1: -39.73632706776682\n",
      "[9, 5, 4, 7, 6, 1]\n",
      "Steps done: 5981\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 11: -45.317902063110566\n",
      "[9, 5, 4, 7, 6]\n",
      "Steps done: 5982\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 14: -52.07532554240231\n",
      "[9, 5, 7, 6]\n",
      "Steps done: 5983\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 16: -83.62304508515317\n",
      "[9, 5, 7]\n",
      "Steps done: 5984\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 8: -136.56071534239504\n",
      "[9, 5, 7, 8]\n",
      "Steps done: 5985\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 19: -129.21666333001707\n",
      "[5, 7, 8]\n",
      "Steps done: 5986\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 3: -80.3261606600411\n",
      "[5, 7, 8, 3]\n",
      "Steps done: 5987\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 15: -111.36648990824361\n",
      "[7, 8, 3]\n",
      "Steps done: 5988\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 2: -69.88399375908872\n",
      "[7, 8, 3, 2]\n",
      "Steps done: 5989\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 0: -51.846841245747285\n",
      "[7, 8, 3, 2, 0]\n",
      "Steps done: 5990\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 10: -69.88399375908872\n",
      "[7, 8, 3, 2]\n",
      "Steps done: 5991\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 5: -58.915828708560475\n",
      "[7, 8, 3, 2, 5]\n",
      "Steps done: 5992\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 6: -44.007431384655405\n",
      "[7, 8, 3, 2, 5, 6]\n",
      "Steps done: 5993\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 17: -49.82410325806609\n",
      "[8, 3, 2, 5, 6]\n",
      "Steps done: 5994\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Reward for action 18: -20.330068018995355\n",
      "[3, 2, 5, 6]\n",
      "Steps done: 5995\n",
      "SV: [ 0.10063068  0.05077258 -0.5194304 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -16.330068018995355\n",
      "[3, 2, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 785\n",
      "Steps done: 5996\n",
      "SV: [-0.30944934  0.6339485   0.21173663]\n",
      "Reward for action 2: -18.597184689179386\n",
      "[0, 1, 2]\n",
      "Steps done: 5997\n",
      "SV: [-0.30944934  0.6339485   0.21173663]\n",
      "Reward for action 20: -14.597184689179386\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 786\n",
      "Steps done: 5998\n",
      "SV: [-0.4228235  0.8416036  1.5971565]\n",
      "Reward for action 20: -206.57566782077674\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 787\n",
      "Steps done: 5999\n",
      "SV: [-0.06919867 -1.0137485  -1.0849718 ]\n",
      "Reward for action 20: -105.86397091314365\n",
      "[0, 1]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 789\n",
      "Steps done: 6000\n",
      "SV: [ 0.06108168  0.21143547 -0.03533384]\n",
      "Reward for action 9: -28.126365250759566\n",
      "[0, 1, 9]\n",
      "Steps done: 6001\n",
      "SV: [ 0.06108168  0.21143547 -0.03533384]\n",
      "Reward for action 8: -31.221062385636717\n",
      "[0, 1, 9, 8]\n",
      "Steps done: 6002\n",
      "SV: [ 0.06108168  0.21143547 -0.03533384]\n",
      "Reward for action 4: -5.4979438303649415\n",
      "[0, 1, 9, 8, 4]\n",
      "Steps done: 6003\n",
      "SV: [ 0.06108168  0.21143547 -0.03533384]\n",
      "Reward for action 20: -1.4979438303649415\n",
      "[0, 1, 9, 8, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 790\n",
      "Steps done: 6004\n",
      "SV: [0.3519331  0.21694468 0.15822583]\n",
      "Reward for action 20: -35.78938942779246\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 791\n",
      "Steps done: 6005\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 5: -2.6592601312879274\n",
      "[0, 1, 5]\n",
      "Steps done: 6006\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 6: -1.8847009157852348\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 6007\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 11: -2.113332073770852\n",
      "[0, 5, 6]\n",
      "Steps done: 6008\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 2: -1.7622307923177645\n",
      "[0, 5, 6, 2]\n",
      "Steps done: 6009\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 9: -1.7408880136246148\n",
      "[0, 5, 6, 2, 9]\n",
      "Steps done: 6010\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 4: -1.7965717904074676\n",
      "[0, 5, 6, 2, 9, 4]\n",
      "Steps done: 6011\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 3: -1.8030111362678032\n",
      "[0, 5, 6, 2, 9, 4, 3]\n",
      "Steps done: 6012\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 15: -1.7943811400270748\n",
      "[0, 6, 2, 9, 4, 3]\n",
      "Steps done: 6013\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 8: -1.7548978646253008\n",
      "[0, 6, 2, 9, 4, 3, 8]\n",
      "Steps done: 6014\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 13: -1.750790573844976\n",
      "[0, 6, 2, 9, 4, 8]\n",
      "Steps done: 6015\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 7: -1.6796415912868785\n",
      "[0, 6, 2, 9, 4, 8, 7]\n",
      "Steps done: 6016\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 3: -1.7035184610054388\n",
      "[0, 6, 2, 9, 4, 8, 7, 3]\n",
      "Steps done: 6017\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 5: -1.7254542355761353\n",
      "[0, 6, 2, 9, 4, 8, 7, 3, 5]\n",
      "Steps done: 6018\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 18: -1.7218549728925439\n",
      "[0, 6, 2, 9, 4, 7, 3, 5]\n",
      "Steps done: 6019\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 12: -1.7129352147387997\n",
      "[0, 6, 9, 4, 7, 3, 5]\n",
      "Steps done: 6020\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 14: -1.7168413946977494\n",
      "[0, 6, 9, 7, 3, 5]\n",
      "Steps done: 6021\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 2: -1.7218071071147973\n",
      "[0, 6, 9, 7, 3, 5, 2]\n",
      "Steps done: 6022\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 12: -1.7168413946977494\n",
      "[0, 6, 9, 7, 3, 5]\n",
      "Steps done: 6023\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 2: -1.7218071071147973\n",
      "[0, 6, 9, 7, 3, 5, 2]\n",
      "Steps done: 6024\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 4: -1.7218549728925447\n",
      "[0, 6, 9, 7, 3, 5, 2, 4]\n",
      "Steps done: 6025\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 13: -1.701853555485498\n",
      "[0, 6, 9, 7, 5, 2, 4]\n",
      "Steps done: 6026\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 1: -1.7750911669813945\n",
      "[0, 6, 9, 7, 5, 2, 4, 1]\n",
      "Steps done: 6027\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 10: -1.7829039444873676\n",
      "[6, 9, 7, 5, 2, 4, 1]\n",
      "Steps done: 6028\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 15: -1.7621260625720738\n",
      "[6, 9, 7, 2, 4, 1]\n",
      "Steps done: 6029\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 12: -1.7628068754728679\n",
      "[6, 9, 7, 4, 1]\n",
      "Steps done: 6030\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Reward for action 19: -1.9900479242612414\n",
      "[6, 7, 4, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6031\n",
      "SV: [-0.00577956 -0.01689662 -0.15895681]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: 2.0099520757387586\n",
      "[6, 7, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 792\n",
      "Steps done: 6032\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 2: -3.6892554187574813\n",
      "[0, 1, 2]\n",
      "Steps done: 6033\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 5: -139.0097902118515\n",
      "[0, 1, 2, 5]\n",
      "Steps done: 6034\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 12: -667.0711607762242\n",
      "[0, 1, 5]\n",
      "Steps done: 6035\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 10: -837.3997932098207\n",
      "[1, 5]\n",
      "Steps done: 6036\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 4: -27.2713780137817\n",
      "[1, 5, 4]\n",
      "Steps done: 6037\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 3: -79.4794008046717\n",
      "[1, 5, 4, 3]\n",
      "Steps done: 6038\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 2: -27.151970301970575\n",
      "[1, 5, 4, 3, 2]\n",
      "Steps done: 6039\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 12: -79.4794008046717\n",
      "[1, 5, 4, 3]\n",
      "Steps done: 6040\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 15: -7.326091564502944\n",
      "[1, 4, 3]\n",
      "Steps done: 6041\n",
      "SV: [ 0.87626487 -0.43416855 -1.284929  ]\n",
      "Reward for action 20: -3.326091564502944\n",
      "[1, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 793\n",
      "Steps done: 6042\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 2: -15.439802732635464\n",
      "[0, 1, 2]\n",
      "Steps done: 6043\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 11: -12.620872263626799\n",
      "[0, 2]\n",
      "Steps done: 6044\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 7: -10.183247174022258\n",
      "[0, 2, 7]\n",
      "Steps done: 6045\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 8: -35.22895330034017\n",
      "[0, 2, 7, 8]\n",
      "Steps done: 6046\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 9: -6.594950148993307\n",
      "[0, 2, 7, 8, 9]\n",
      "Steps done: 6047\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 3: -3.286626462485243\n",
      "[0, 2, 7, 8, 9, 3]\n",
      "Steps done: 6048\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 5: -2.2714524388708863\n",
      "[0, 2, 7, 8, 9, 3, 5]\n",
      "Steps done: 6049\n",
      "SV: [ 0.11947107 -0.02502659  0.39294785]\n",
      "Reward for action 20: 1.7285475611291137\n",
      "[0, 2, 7, 8, 9, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 794\n",
      "Steps done: 6050\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 4: -42.72612456551685\n",
      "[0, 1, 4]\n",
      "Steps done: 6051\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 6: -51.43563171912387\n",
      "[0, 1, 4, 6]\n",
      "Steps done: 6052\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 5: -58.45140393864713\n",
      "[0, 1, 4, 6, 5]\n",
      "Steps done: 6053\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 11: -72.1178880307603\n",
      "[0, 4, 6, 5]\n",
      "Steps done: 6054\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 2: -63.37921756219383\n",
      "[0, 4, 6, 5, 2]\n",
      "Steps done: 6055\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 7: -69.74038158610335\n",
      "[0, 4, 6, 5, 2, 7]\n",
      "Steps done: 6056\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 12: -60.44017761103949\n",
      "[0, 4, 6, 5, 7]\n",
      "Steps done: 6057\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 14: -46.10677923197973\n",
      "[0, 6, 5, 7]\n",
      "Steps done: 6058\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 17: -79.85962660869708\n",
      "[0, 6, 5]\n",
      "Steps done: 6059\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 15: -57.92142406689616\n",
      "[0, 6]\n",
      "Steps done: 6060\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 3: -61.158086390218095\n",
      "[0, 6, 3]\n",
      "Steps done: 6061\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 13: -57.92142406689616\n",
      "[0, 6]\n",
      "Steps done: 6062\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 9: -102.85495255805301\n",
      "[0, 6, 9]\n",
      "Steps done: 6063\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 3: -80.57802029596287\n",
      "[0, 6, 9, 3]\n",
      "Steps done: 6064\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 2: -82.05840475892795\n",
      "[0, 6, 9, 3, 2]\n",
      "Steps done: 6065\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 13: -97.2285069478708\n",
      "[0, 6, 9, 2]\n",
      "Steps done: 6066\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 5: -42.23723395916028\n",
      "[0, 6, 9, 2, 5]\n",
      "Steps done: 6067\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 15: -97.2285069478708\n",
      "[0, 6, 9, 2]\n",
      "Steps done: 6068\n",
      "SV: [-0.05751727 -0.1424139   0.8747528 ]\n",
      "Reward for action 20: -93.2285069478708\n",
      "[0, 6, 9, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 795\n",
      "Steps done: 6069\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 4: -3.971287871969814\n",
      "[0, 1, 4]\n",
      "Steps done: 6070\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 14: -13.737087277798814\n",
      "[0, 1]\n",
      "Steps done: 6071\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 4: -3.971287871969814\n",
      "[0, 1, 4]\n",
      "Steps done: 6072\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 14: -13.737087277798814\n",
      "[0, 1]\n",
      "Steps done: 6073\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 4: -3.971287871969814\n",
      "[0, 1, 4]\n",
      "Steps done: 6074\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 3: -6.285240582360451\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 6075\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 11: -7.115813091321662\n",
      "[0, 4, 3]\n",
      "Steps done: 6076\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 1: -6.285240582360451\n",
      "[0, 4, 3, 1]\n",
      "Steps done: 6077\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 14: -3.7848052357173496\n",
      "[0, 3, 1]\n",
      "Steps done: 6078\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 4: -6.285240582360452\n",
      "[0, 3, 1, 4]\n",
      "Steps done: 6079\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 11: -7.115813091321663\n",
      "[0, 3, 4]\n",
      "Steps done: 6080\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 14: -4.680716748235489\n",
      "[0, 3]\n",
      "Steps done: 6081\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 4: -7.115813091321663\n",
      "[0, 3, 4]\n",
      "Steps done: 6082\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 14: -4.680716748235489\n",
      "[0, 3]\n",
      "Steps done: 6083\n",
      "SV: [ 0.29146954 -0.32669148 -0.07993635]\n",
      "Reward for action 20: -0.680716748235489\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 796\n",
      "Steps done: 6084\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 8: -5.666595034672006\n",
      "[0, 1, 8]\n",
      "Steps done: 6085\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 11: -7.836556463349387\n",
      "[0, 8]\n",
      "Steps done: 6086\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 3: -28.242576114731126\n",
      "[0, 8, 3]\n",
      "Steps done: 6087\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 13: -7.836556463349387\n",
      "[0, 8]\n",
      "Steps done: 6088\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 5: -6.306944139701784\n",
      "[0, 8, 5]\n",
      "Steps done: 6089\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 3: -18.320022063374335\n",
      "[0, 8, 5, 3]\n",
      "Steps done: 6090\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 13: -6.306944139701784\n",
      "[0, 8, 5]\n",
      "Steps done: 6091\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 15: -7.836556463349387\n",
      "[0, 8]\n",
      "Steps done: 6092\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 3: -28.242576114731126\n",
      "[0, 8, 3]\n",
      "Steps done: 6093\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 13: -7.836556463349387\n",
      "[0, 8]\n",
      "Steps done: 6094\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 6: -8.3586693955941\n",
      "[0, 8, 6]\n",
      "Steps done: 6095\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 2: -7.024757633137917\n",
      "[0, 8, 6, 2]\n",
      "Steps done: 6096\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 4: -5.390869451298254\n",
      "[0, 8, 6, 2, 4]\n",
      "Steps done: 6097\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 3: -8.922765921156383\n",
      "[0, 8, 6, 2, 4, 3]\n",
      "Steps done: 6098\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 13: -5.390869451298254\n",
      "[0, 8, 6, 2, 4]\n",
      "Steps done: 6099\n",
      "SV: [-0.12072963  0.09841972 -0.2043666 ]\n",
      "Reward for action 20: -1.3908694512982542\n",
      "[0, 8, 6, 2, 4]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 797\n",
      "Steps done: 6100\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 6: -20.9040252935419\n",
      "[0, 1, 6]\n",
      "Steps done: 6101\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 9: -20.03089597141925\n",
      "[0, 1, 6, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6102\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 2: -14.296767734440346\n",
      "[0, 1, 6, 9, 2]\n",
      "Steps done: 6103\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 5: -16.25366432140506\n",
      "[0, 1, 6, 9, 2, 5]\n",
      "Steps done: 6104\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 16: -17.032908017504322\n",
      "[0, 1, 9, 2, 5]\n",
      "Steps done: 6105\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 3: -19.321032907637235\n",
      "[0, 1, 9, 2, 5, 3]\n",
      "Steps done: 6106\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 13: -17.032908017504322\n",
      "[0, 1, 9, 2, 5]\n",
      "Steps done: 6107\n",
      "SV: [ 0.28346652  0.07614597 -0.08199876]\n",
      "Reward for action 20: -13.032908017504322\n",
      "[0, 1, 9, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 798\n",
      "Steps done: 6108\n",
      "SV: [-0.64846593  0.01471351 -0.60305005]\n",
      "Reward for action 8: -20.109013506562636\n",
      "[0, 1, 8]\n",
      "Steps done: 6109\n",
      "SV: [-0.64846593  0.01471351 -0.60305005]\n",
      "Reward for action 20: -16.109013506562636\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 799\n",
      "Steps done: 6110\n",
      "SV: [ 0.027601   -0.02444328 -0.4091713 ]\n",
      "Reward for action 20: -10.73141353841631\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 800\n",
      "Steps done: 6111\n",
      "SV: [0.25068533 0.13535069 0.44357204]\n",
      "Reward for action 4: -244.01075934481733\n",
      "[0, 1, 4]\n",
      "Steps done: 6112\n",
      "SV: [0.25068533 0.13535069 0.44357204]\n",
      "Reward for action 20: -240.01075934481733\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 801\n",
      "Steps done: 6113\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 8: -323.39588809828655\n",
      "[0, 1, 8]\n",
      "Steps done: 6114\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 4: -69.27397746470035\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 6115\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 14: -323.39588809828655\n",
      "[0, 1, 8]\n",
      "Steps done: 6116\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 4: -69.27397746470035\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 6117\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 14: -323.39588809828655\n",
      "[0, 1, 8]\n",
      "Steps done: 6118\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 10: -474.7065970100143\n",
      "[1, 8]\n",
      "Steps done: 6119\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 6: -53.55676537125291\n",
      "[1, 8, 6]\n",
      "Steps done: 6120\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 2: -12.216719103834444\n",
      "[1, 8, 6, 2]\n",
      "Steps done: 6121\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 12: -53.55676537125291\n",
      "[1, 8, 6]\n",
      "Steps done: 6122\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 3: -26.819614829204202\n",
      "[1, 8, 6, 3]\n",
      "Steps done: 6123\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 4: -4.026134032351474\n",
      "[1, 8, 6, 3, 4]\n",
      "Steps done: 6124\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 14: -26.819614829204202\n",
      "[1, 8, 6, 3]\n",
      "Steps done: 6125\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 4: -4.026134032351474\n",
      "[1, 8, 6, 3, 4]\n",
      "Steps done: 6126\n",
      "SV: [ 0.28198698 -0.2536261   1.4853152 ]\n",
      "Reward for action 20: -0.026134032351474268\n",
      "[1, 8, 6, 3, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 802\n",
      "Steps done: 6127\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 6: -78.0236230703026\n",
      "[0, 1, 6]\n",
      "Steps done: 6128\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 2: -86.79654073488162\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 6129\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 4: -85.57974888725632\n",
      "[0, 1, 6, 2, 4]\n",
      "Steps done: 6130\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 10: -77.2474254149178\n",
      "[1, 6, 2, 4]\n",
      "Steps done: 6131\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 12: -77.57036993051175\n",
      "[1, 6, 4]\n",
      "Steps done: 6132\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 16: -48.73069880506865\n",
      "[1, 4]\n",
      "Steps done: 6133\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 2: -55.45972880108802\n",
      "[1, 4, 2]\n",
      "Steps done: 6134\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 7: -39.647892866343874\n",
      "[1, 4, 2, 7]\n",
      "Steps done: 6135\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 5: -6.605007175116322\n",
      "[1, 4, 2, 7, 5]\n",
      "Steps done: 6136\n",
      "SV: [-0.8729004   0.08574902 -0.2828212 ]\n",
      "Reward for action 20: -2.605007175116322\n",
      "[1, 4, 2, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 803\n",
      "Steps done: 6137\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 3: -6.707276403197885\n",
      "[0, 1, 3]\n",
      "Steps done: 6138\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 5: -6.88185577831031\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 6139\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 11: -6.779875751527698\n",
      "[0, 3, 5]\n",
      "Steps done: 6140\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 1: -6.881855778310305\n",
      "[0, 3, 5, 1]\n",
      "Steps done: 6141\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 11: -6.779875751527698\n",
      "[0, 3, 5]\n",
      "Steps done: 6142\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 4: -6.746542760611508\n",
      "[0, 3, 5, 4]\n",
      "Steps done: 6143\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 1: -6.853706701699544\n",
      "[0, 3, 5, 4, 1]\n",
      "Steps done: 6144\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 13: -7.272863774619194\n",
      "[0, 5, 4, 1]\n",
      "Steps done: 6145\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 6: -7.620510634008394\n",
      "[0, 5, 4, 1, 6]\n",
      "Steps done: 6146\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 3: -7.165214018003553\n",
      "[0, 5, 4, 1, 6, 3]\n",
      "Steps done: 6147\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 14: -7.277776475596755\n",
      "[0, 5, 1, 6, 3]\n",
      "Steps done: 6148\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 11: -7.231858974033623\n",
      "[0, 5, 6, 3]\n",
      "Steps done: 6149\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 13: -7.962318191696491\n",
      "[0, 5, 6]\n",
      "Steps done: 6150\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 16: -7.2950225211124655\n",
      "[0, 5]\n",
      "Steps done: 6151\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 3: -6.779875751527698\n",
      "[0, 5, 3]\n",
      "Steps done: 6152\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 13: -7.2950225211124655\n",
      "[0, 5]\n",
      "Steps done: 6153\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 6: -7.962318191696491\n",
      "[0, 5, 6]\n",
      "Steps done: 6154\n",
      "SV: [0.01001792 0.01683468 0.30138758]\n",
      "Reward for action 20: -3.9623181916964914\n",
      "[0, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 804\n",
      "Steps done: 6155\n",
      "SV: [ 0.17895836  0.01293974 -0.2577777 ]\n",
      "Reward for action 20: -10.242800312818506\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 805\n",
      "Steps done: 6156\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 5: -486.8253216271334\n",
      "[0, 1, 5]\n",
      "Steps done: 6157\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 11: -816.222219307528\n",
      "[0, 5]\n",
      "Steps done: 6158\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 3: -331.7715109033836\n",
      "[0, 5, 3]\n",
      "Steps done: 6159\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 1: -253.4792512967901\n",
      "[0, 5, 3, 1]\n",
      "Steps done: 6160\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 7: -2.4795643501307256\n",
      "[0, 5, 3, 1, 7]\n",
      "Steps done: 6161\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 13: -1.4683749378101592\n",
      "[0, 5, 1, 7]\n",
      "Steps done: 6162\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 11: -3.163615563257334\n",
      "[0, 5, 7]\n",
      "Steps done: 6163\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 6: -31.50228520986174\n",
      "[0, 5, 7, 6]\n",
      "Steps done: 6164\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 15: -122.90491543722352\n",
      "[0, 7, 6]\n",
      "Steps done: 6165\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 2: -130.93856736671822\n",
      "[0, 7, 6, 2]\n",
      "Steps done: 6166\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 16: -310.59083239693166\n",
      "[0, 7, 2]\n",
      "Steps done: 6167\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 1: -164.2216083993515\n",
      "[0, 7, 2, 1]\n",
      "Steps done: 6168\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 5: -0.8294251086945215\n",
      "[0, 7, 2, 1, 5]\n",
      "Steps done: 6169\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 4: -0.9487681666108604\n",
      "[0, 7, 2, 1, 5, 4]\n",
      "Steps done: 6170\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 6: -6.188476212582778\n",
      "[0, 7, 2, 1, 5, 4, 6]\n",
      "Steps done: 6171\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 16: -0.9487681666108604\n",
      "[0, 7, 2, 1, 5, 4]\n",
      "Steps done: 6172\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 11: -1.9712472247936619\n",
      "[0, 7, 2, 5, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6173\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 17: -159.7145985004508\n",
      "[0, 2, 5, 4]\n",
      "Steps done: 6174\n",
      "SV: [ 0.0384753   0.03732754 -0.35145032]\n",
      "Reward for action 20: -155.7145985004508\n",
      "[0, 2, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 806\n",
      "Steps done: 6175\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 2: -25.053500001488366\n",
      "[0, 1, 2]\n",
      "Steps done: 6176\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 4: -19.43555032396628\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 6177\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 12: -27.08116655028336\n",
      "[0, 1, 4]\n",
      "Steps done: 6178\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 9: -7.865964793848934\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 6179\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 3: -12.143758589155679\n",
      "[0, 1, 4, 9, 3]\n",
      "Steps done: 6180\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 5: -8.688400855872\n",
      "[0, 1, 4, 9, 3, 5]\n",
      "Steps done: 6181\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 14: -13.433657353560369\n",
      "[0, 1, 9, 3, 5]\n",
      "Steps done: 6182\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 4: -8.688400855872\n",
      "[0, 1, 9, 3, 5, 4]\n",
      "Steps done: 6183\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 13: -12.00849790650525\n",
      "[0, 1, 9, 5, 4]\n",
      "Steps done: 6184\n",
      "SV: [-2.2514772 -1.4327488 -1.8788046]\n",
      "Reward for action 20: -8.00849790650525\n",
      "[0, 1, 9, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 807\n",
      "Steps done: 6185\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 2: -14.737420000958194\n",
      "[0, 1, 2]\n",
      "Steps done: 6186\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 3: -14.634747316767044\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 6187\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 13: -14.737420000958194\n",
      "[0, 1, 2]\n",
      "Steps done: 6188\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 12: -14.02945231395336\n",
      "[0, 1]\n",
      "Steps done: 6189\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 5: -12.976395685127455\n",
      "[0, 1, 5]\n",
      "Steps done: 6190\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 3: -13.56350309878231\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 6191\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 15: -14.118053227977724\n",
      "[0, 1, 3]\n",
      "Steps done: 6192\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 10: -14.58509053442083\n",
      "[1, 3]\n",
      "Steps done: 6193\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 4: -17.29250361993062\n",
      "[1, 3, 4]\n",
      "Steps done: 6194\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 2: -18.163883267003357\n",
      "[1, 3, 4, 2]\n",
      "Steps done: 6195\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 13: -20.77008294046423\n",
      "[1, 4, 2]\n",
      "Steps done: 6196\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 12: -18.375833673141305\n",
      "[1, 4]\n",
      "Steps done: 6197\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 2: -20.77008294046423\n",
      "[1, 4, 2]\n",
      "Steps done: 6198\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 0: -16.17087840400522\n",
      "[1, 4, 2, 0]\n",
      "Steps done: 6199\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 12: -15.613389745903923\n",
      "[1, 4, 0]\n",
      "Did target update\n",
      "Steps done: 6200\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 5: -16.728586536891218\n",
      "[1, 4, 0, 5]\n",
      "Steps done: 6201\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 2: -16.519030028662012\n",
      "[1, 4, 0, 5, 2]\n",
      "Steps done: 6202\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 3: -16.011149308703622\n",
      "[1, 4, 0, 5, 2, 3]\n",
      "Steps done: 6203\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 14: -14.238626537614532\n",
      "[1, 0, 5, 2, 3]\n",
      "Steps done: 6204\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 13: -14.08966177417287\n",
      "[1, 0, 5, 2]\n",
      "Steps done: 6205\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 10: -17.421131785752348\n",
      "[1, 5, 2]\n",
      "Steps done: 6206\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 0: -14.089661774172862\n",
      "[1, 5, 2, 0]\n",
      "Steps done: 6207\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 11: -13.205288124530858\n",
      "[5, 2, 0]\n",
      "Steps done: 6208\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 15: -15.055542962804203\n",
      "[2, 0]\n",
      "Steps done: 6209\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 1: -14.737420000958195\n",
      "[2, 0, 1]\n",
      "Steps done: 6210\n",
      "SV: [-0.14359418  0.07408892  0.5499682 ]\n",
      "Reward for action 20: -10.737420000958195\n",
      "[2, 0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 808\n",
      "Steps done: 6211\n",
      "SV: [ 3.1019418   0.81837875 -3.843631  ]\n",
      "Reward for action 2: -27.859826522235206\n",
      "[0, 1, 2]\n",
      "Steps done: 6212\n",
      "SV: [ 3.1019418   0.81837875 -3.843631  ]\n",
      "Reward for action 10: -50.81780452540403\n",
      "[1, 2]\n",
      "Steps done: 6213\n",
      "SV: [ 3.1019418   0.81837875 -3.843631  ]\n",
      "Reward for action 4: -32.62402227150749\n",
      "[1, 2, 4]\n",
      "Steps done: 6214\n",
      "SV: [ 3.1019418   0.81837875 -3.843631  ]\n",
      "Reward for action 20: -28.624022271507492\n",
      "[1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 809\n",
      "Steps done: 6215\n",
      "SV: [0.01778215 0.55277306 0.2798086 ]\n",
      "Reward for action 5: -6.981085944368374\n",
      "[0, 1, 5]\n",
      "Steps done: 6216\n",
      "SV: [0.01778215 0.55277306 0.2798086 ]\n",
      "Reward for action 2: -20.560913656522263\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 6217\n",
      "SV: [0.01778215 0.55277306 0.2798086 ]\n",
      "Reward for action 6: -6.450692768692771\n",
      "[0, 1, 5, 2, 6]\n",
      "Steps done: 6218\n",
      "SV: [0.01778215 0.55277306 0.2798086 ]\n",
      "Reward for action 20: -2.450692768692771\n",
      "[0, 1, 5, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 810\n",
      "Steps done: 6219\n",
      "SV: [ 0.01283628  0.02322972 -0.48563552]\n",
      "Reward for action 3: -19.384343237662907\n",
      "[0, 1, 3]\n",
      "Steps done: 6220\n",
      "SV: [ 0.01283628  0.02322972 -0.48563552]\n",
      "Reward for action 7: -19.014143650242186\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 6221\n",
      "SV: [ 0.01283628  0.02322972 -0.48563552]\n",
      "Reward for action 8: -19.106519617369187\n",
      "[0, 1, 3, 7, 8]\n",
      "Steps done: 6222\n",
      "SV: [ 0.01283628  0.02322972 -0.48563552]\n",
      "Reward for action 17: -19.298390871997235\n",
      "[0, 1, 3, 8]\n",
      "Steps done: 6223\n",
      "SV: [ 0.01283628  0.02322972 -0.48563552]\n",
      "Reward for action 20: -15.298390871997235\n",
      "[0, 1, 3, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 811\n",
      "Steps done: 6224\n",
      "SV: [ 0.8305195  -0.33280087  0.0058098 ]\n",
      "Reward for action 2: -46.68953401816882\n",
      "[0, 1, 2]\n",
      "Steps done: 6225\n",
      "SV: [ 0.8305195  -0.33280087  0.0058098 ]\n",
      "Reward for action 11: -43.6196206303103\n",
      "[0, 2]\n",
      "Steps done: 6226\n",
      "SV: [ 0.8305195  -0.33280087  0.0058098 ]\n",
      "Reward for action 20: -39.6196206303103\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 812\n",
      "Steps done: 6227\n",
      "SV: [-0.00123397 -0.00837656  0.44498673]\n",
      "Reward for action 2: -16.65154662150873\n",
      "[0, 1, 2]\n",
      "Steps done: 6228\n",
      "SV: [-0.00123397 -0.00837656  0.44498673]\n",
      "Reward for action 20: -12.651546621508729\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 813\n",
      "Steps done: 6229\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 9: -54.268053530603645\n",
      "[0, 1, 9]\n",
      "Steps done: 6230\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 5: -44.32027680952666\n",
      "[0, 1, 9, 5]\n",
      "Steps done: 6231\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 10: -55.99538290141205\n",
      "[1, 9, 5]\n",
      "Steps done: 6232\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 11: -58.86093717121642\n",
      "[9, 5]\n",
      "Steps done: 6233\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 1: -55.99538290141205\n",
      "[9, 5, 1]\n",
      "Steps done: 6234\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 2: -45.531468848995125\n",
      "[9, 5, 1, 2]\n",
      "Steps done: 6235\n",
      "SV: [-0.38454327  0.14655422 -0.60986775]\n",
      "Reward for action 20: -41.531468848995125\n",
      "[9, 5, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 814\n",
      "Steps done: 6236\n",
      "SV: [ 1.0993793   0.04751396 -2.122087  ]\n",
      "Reward for action 2: -84.97527555536665\n",
      "[0, 1, 2]\n",
      "Steps done: 6237\n",
      "SV: [ 1.0993793   0.04751396 -2.122087  ]\n",
      "Reward for action 11: -52.9563117960069\n",
      "[0, 2]\n",
      "Steps done: 6238\n",
      "SV: [ 1.0993793   0.04751396 -2.122087  ]\n",
      "Reward for action 5: -53.0011948660249\n",
      "[0, 2, 5]\n",
      "Steps done: 6239\n",
      "SV: [ 1.0993793   0.04751396 -2.122087  ]\n",
      "Reward for action 20: -49.0011948660249\n",
      "[0, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 815\n",
      "Steps done: 6240\n",
      "SV: [-0.0175326  -0.15923761 -0.37505025]\n",
      "Reward for action 20: -4.71678926900036\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 816\n",
      "Steps done: 6241\n",
      "SV: [ 0.43949208  0.25324374 -1.1399287 ]\n",
      "Reward for action 5: -26.69836146964244\n",
      "[0, 1, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6242\n",
      "SV: [ 0.43949208  0.25324374 -1.1399287 ]\n",
      "Reward for action 6: -51.63920097175502\n",
      "[0, 1, 5, 6]\n",
      "Steps done: 6243\n",
      "SV: [ 0.43949208  0.25324374 -1.1399287 ]\n",
      "Reward for action 20: -47.63920097175502\n",
      "[0, 1, 5, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 817\n",
      "Steps done: 6244\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 2: -8.99413222631511\n",
      "[0, 1, 2]\n",
      "Steps done: 6245\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 10: -61.053307684379504\n",
      "[1, 2]\n",
      "Steps done: 6246\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 5: -4.949262668924291\n",
      "[1, 2, 5]\n",
      "Steps done: 6247\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 15: -61.053307684379504\n",
      "[1, 2]\n",
      "Steps done: 6248\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 0: -8.994132226315106\n",
      "[1, 2, 0]\n",
      "Steps done: 6249\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 10: -61.053307684379504\n",
      "[1, 2]\n",
      "Steps done: 6250\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 5: -4.949262668924291\n",
      "[1, 2, 5]\n",
      "Steps done: 6251\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 12: -21.788327420173424\n",
      "[1, 5]\n",
      "Steps done: 6252\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 2: -4.949262668924291\n",
      "[1, 5, 2]\n",
      "Steps done: 6253\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 0: -14.532290279167789\n",
      "[1, 5, 2, 0]\n",
      "Steps done: 6254\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 11: -73.73163088432113\n",
      "[5, 2, 0]\n",
      "Steps done: 6255\n",
      "SV: [-0.02851851 -0.22582607 -0.0106939 ]\n",
      "Reward for action 20: -69.73163088432113\n",
      "[5, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 818\n",
      "Steps done: 6256\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 2: -28.907412988179022\n",
      "[0, 1, 2]\n",
      "Steps done: 6257\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 12: -0.21201330987578607\n",
      "[0, 1]\n",
      "Steps done: 6258\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 4: -8.65919121002067\n",
      "[0, 1, 4]\n",
      "Steps done: 6259\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 2: -19.572015089192377\n",
      "[0, 1, 4, 2]\n",
      "Steps done: 6260\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 14: -28.907412988179022\n",
      "[0, 1, 2]\n",
      "Steps done: 6261\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 10: -69.4688737506994\n",
      "[1, 2]\n",
      "Steps done: 6262\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 4: -0.9714386290756956\n",
      "[1, 2, 4]\n",
      "Steps done: 6263\n",
      "SV: [ 0.03612666  0.3375578  -1.0423344 ]\n",
      "Reward for action 20: 3.0285613709243044\n",
      "[1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 819\n",
      "Steps done: 6264\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 5: -0.15269158026962545\n",
      "[0, 1, 5]\n",
      "Steps done: 6265\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 8: -0.5013364412831884\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 6266\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 11: -0.5919380894880583\n",
      "[0, 5, 8]\n",
      "Steps done: 6267\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 3: -6.504435786300506\n",
      "[0, 5, 8, 3]\n",
      "Steps done: 6268\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 13: -0.5919380894880583\n",
      "[0, 5, 8]\n",
      "Steps done: 6269\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 1: -0.5013364412831888\n",
      "[0, 5, 8, 1]\n",
      "Steps done: 6270\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 11: -0.5919380894880583\n",
      "[0, 5, 8]\n",
      "Steps done: 6271\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 10: -0.11840746427444372\n",
      "[5, 8]\n",
      "Steps done: 6272\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 0: -0.5919380894880583\n",
      "[5, 8, 0]\n",
      "Steps done: 6273\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 10: -0.11840746427444372\n",
      "[5, 8]\n",
      "Steps done: 6274\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 7: -10.617620379734376\n",
      "[5, 8, 7]\n",
      "Steps done: 6275\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 0: -11.101081453541177\n",
      "[5, 8, 7, 0]\n",
      "Steps done: 6276\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 3: -1.7893495245741988\n",
      "[5, 8, 7, 0, 3]\n",
      "Steps done: 6277\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 15: -111.80492080015756\n",
      "[8, 7, 0, 3]\n",
      "Steps done: 6278\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 13: -285.01590333535364\n",
      "[8, 7, 0]\n",
      "Steps done: 6279\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 17: -4.7728127127756625\n",
      "[8, 0]\n",
      "Steps done: 6280\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 3: -220.72949306446938\n",
      "[8, 0, 3]\n",
      "Steps done: 6281\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 1: -135.97097749208945\n",
      "[8, 0, 3, 1]\n",
      "Steps done: 6282\n",
      "SV: [-0.08901445  0.01485249  0.20965968]\n",
      "Reward for action 20: -131.97097749208945\n",
      "[8, 0, 3, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 820\n",
      "Steps done: 6283\n",
      "SV: [ 0.3796147   1.4636893  -0.48814145]\n",
      "Reward for action 2: -124.07881491651526\n",
      "[0, 1, 2]\n",
      "Steps done: 6284\n",
      "SV: [ 0.3796147   1.4636893  -0.48814145]\n",
      "Reward for action 20: -120.07881491651526\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 821\n",
      "Steps done: 6285\n",
      "SV: [ 0.5507552 -0.1390314 -0.2924908]\n",
      "Reward for action 3: -20.988374255106987\n",
      "[0, 1, 3]\n",
      "Steps done: 6286\n",
      "SV: [ 0.5507552 -0.1390314 -0.2924908]\n",
      "Reward for action 2: -10.72250321290365\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 6287\n",
      "SV: [ 0.5507552 -0.1390314 -0.2924908]\n",
      "Reward for action 20: -6.72250321290365\n",
      "[0, 1, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 822\n",
      "Steps done: 6288\n",
      "SV: [ 0.77252626  0.65046406 -0.03762046]\n",
      "Reward for action 3: -153.38350104422608\n",
      "[0, 1, 3]\n",
      "Steps done: 6289\n",
      "SV: [ 0.77252626  0.65046406 -0.03762046]\n",
      "Reward for action 11: -165.11641274503384\n",
      "[0, 3]\n",
      "Steps done: 6290\n",
      "SV: [ 0.77252626  0.65046406 -0.03762046]\n",
      "Reward for action 20: -161.11641274503384\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 823\n",
      "Steps done: 6291\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 5: -597.038241241116\n",
      "[0, 1, 5]\n",
      "Steps done: 6292\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 15: -544.998009243504\n",
      "[0, 1]\n",
      "Steps done: 6293\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 7: -446.63261516268665\n",
      "[0, 1, 7]\n",
      "Steps done: 6294\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 10: -619.3714612352384\n",
      "[1, 7]\n",
      "Steps done: 6295\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 6: -670.2023267738373\n",
      "[1, 7, 6]\n",
      "Steps done: 6296\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 17: -709.1823320974036\n",
      "[1, 6]\n",
      "Steps done: 6297\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 4: -694.5793529595281\n",
      "[1, 6, 4]\n",
      "Steps done: 6298\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 14: -709.1823320974036\n",
      "[1, 6]\n",
      "Steps done: 6299\n",
      "SV: [-0.47008872 -0.76037145  2.8637078 ]\n",
      "Reward for action 20: -705.1823320974036\n",
      "[1, 6]\n",
      "Did target update\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 824\n",
      "Steps done: 6300\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 5: -3.289404252097392\n",
      "[0, 1, 5]\n",
      "Steps done: 6301\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 4: -204.9809270687523\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 6302\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 10: -611.6972519108378\n",
      "[1, 5, 4]\n",
      "Steps done: 6303\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 11: -911.184237209347\n",
      "[5, 4]\n",
      "Steps done: 6304\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 3: -623.5964579975109\n",
      "[5, 4, 3]\n",
      "Steps done: 6305\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 14: -221.21988905213576\n",
      "[5, 3]\n",
      "Steps done: 6306\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 4: -623.5964579975109\n",
      "[5, 3, 4]\n",
      "Steps done: 6307\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 0: -231.27168709229554\n",
      "[5, 3, 4, 0]\n",
      "Steps done: 6308\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 15: -127.82087070334394\n",
      "[3, 4, 0]\n",
      "Steps done: 6309\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 13: -860.1673621687637\n",
      "[4, 0]\n",
      "Steps done: 6310\n",
      "SV: [-4.1171265e-04  6.3181299e-01  1.6684389e+00]\n",
      "Reward for action 20: -856.1673621687637\n",
      "[4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 825\n",
      "Steps done: 6311\n",
      "SV: [ 0.12118088 -0.44844046  0.78315395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 3: -28.733597859455603\n",
      "[0, 1, 3]\n",
      "Steps done: 6312\n",
      "SV: [ 0.12118088 -0.44844046  0.78315395]\n",
      "Reward for action 10: -246.08265878660956\n",
      "[1, 3]\n",
      "Steps done: 6313\n",
      "SV: [ 0.12118088 -0.44844046  0.78315395]\n",
      "Reward for action 20: -242.08265878660956\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 826\n",
      "Steps done: 6314\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 4: -6.7819742392751765\n",
      "[0, 1, 4]\n",
      "Steps done: 6315\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 9: -5.448204532612534\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 6316\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 2: -16.586905307351127\n",
      "[0, 1, 4, 9, 2]\n",
      "Steps done: 6317\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 11: -9.990838549819426\n",
      "[0, 4, 9, 2]\n",
      "Steps done: 6318\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 1: -16.586905307351127\n",
      "[0, 4, 9, 2, 1]\n",
      "Steps done: 6319\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 8: -17.79296960491063\n",
      "[0, 4, 9, 2, 1, 8]\n",
      "Steps done: 6320\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 5: -16.87918392152918\n",
      "[0, 4, 9, 2, 1, 8, 5]\n",
      "Steps done: 6321\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 10: -38.26946431151107\n",
      "[4, 9, 2, 1, 8, 5]\n",
      "Steps done: 6322\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 3: -25.37051111555506\n",
      "[4, 9, 2, 1, 8, 5, 3]\n",
      "Steps done: 6323\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 11: -29.09714146521209\n",
      "[4, 9, 2, 8, 5, 3]\n",
      "Steps done: 6324\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 12: -12.040962435092604\n",
      "[4, 9, 8, 5, 3]\n",
      "Steps done: 6325\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 19: -5.532535042694484\n",
      "[4, 8, 5, 3]\n",
      "Steps done: 6326\n",
      "SV: [ 0.21409483 -0.19870785  0.54953283]\n",
      "Reward for action 20: -1.5325350426944837\n",
      "[4, 8, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 827\n",
      "Steps done: 6327\n",
      "SV: [ 0.11955521 -0.74775684 -1.1029897 ]\n",
      "Reward for action 3: -134.28964971537067\n",
      "[0, 1, 3]\n",
      "Steps done: 6328\n",
      "SV: [ 0.11955521 -0.74775684 -1.1029897 ]\n",
      "Reward for action 4: -216.59283178089606\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 6329\n",
      "SV: [ 0.11955521 -0.74775684 -1.1029897 ]\n",
      "Reward for action 11: -251.72659581879378\n",
      "[0, 3, 4]\n",
      "Steps done: 6330\n",
      "SV: [ 0.11955521 -0.74775684 -1.1029897 ]\n",
      "Reward for action 14: -139.1777695224172\n",
      "[0, 3]\n",
      "Steps done: 6331\n",
      "SV: [ 0.11955521 -0.74775684 -1.1029897 ]\n",
      "Reward for action 20: -135.1777695224172\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 828\n",
      "Steps done: 6332\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 7: -29.644644557981316\n",
      "[0, 1, 7]\n",
      "Steps done: 6333\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 17: -48.59255469796316\n",
      "[0, 1]\n",
      "Steps done: 6334\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 7: -29.644644557981316\n",
      "[0, 1, 7]\n",
      "Steps done: 6335\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 5: -27.327690240717107\n",
      "[0, 1, 7, 5]\n",
      "Steps done: 6336\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 10: -26.12764457311237\n",
      "[1, 7, 5]\n",
      "Steps done: 6337\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 11: -84.18230484084422\n",
      "[7, 5]\n",
      "Steps done: 6338\n",
      "SV: [0.08747775 1.109461   0.16261713]\n",
      "Reward for action 20: -80.18230484084422\n",
      "[7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 829\n",
      "Steps done: 6339\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 2: -18.59305988598798\n",
      "[0, 1, 2]\n",
      "Steps done: 6340\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 10: -18.88648188902637\n",
      "[1, 2]\n",
      "Steps done: 6341\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 0: -18.59305988598798\n",
      "[1, 2, 0]\n",
      "Steps done: 6342\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 6: -16.68622847848076\n",
      "[1, 2, 0, 6]\n",
      "Steps done: 6343\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 12: -17.27499076997518\n",
      "[1, 0, 6]\n",
      "Steps done: 6344\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 11: -14.823922202248925\n",
      "[0, 6]\n",
      "Steps done: 6345\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 3: -14.861018253774679\n",
      "[0, 6, 3]\n",
      "Steps done: 6346\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 10: -15.042085762275304\n",
      "[6, 3]\n",
      "Steps done: 6347\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 7: -14.690893785557389\n",
      "[6, 3, 7]\n",
      "Steps done: 6348\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 13: -15.178089623638124\n",
      "[6, 7]\n",
      "Steps done: 6349\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 3: -14.690893785557389\n",
      "[6, 7, 3]\n",
      "Steps done: 6350\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 13: -15.178089623638124\n",
      "[6, 7]\n",
      "Steps done: 6351\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 3: -14.690893785557389\n",
      "[6, 7, 3]\n",
      "Steps done: 6352\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 13: -15.178089623638124\n",
      "[6, 7]\n",
      "Steps done: 6353\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 3: -14.690893785557389\n",
      "[6, 7, 3]\n",
      "Steps done: 6354\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 13: -15.178089623638124\n",
      "[6, 7]\n",
      "Steps done: 6355\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 3: -14.690893785557389\n",
      "[6, 7, 3]\n",
      "Steps done: 6356\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 5: -14.310991475193205\n",
      "[6, 7, 3, 5]\n",
      "Steps done: 6357\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 17: -13.51895130461303\n",
      "[6, 3, 5]\n",
      "Steps done: 6358\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 7: -14.310991475193198\n",
      "[6, 3, 5, 7]\n",
      "Steps done: 6359\n",
      "SV: [ 0.00416796 -0.00880654 -0.45485017]\n",
      "Reward for action 20: -10.310991475193198\n",
      "[6, 3, 5, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 830\n",
      "Steps done: 6360\n",
      "SV: [ 1.566833  -0.4887502  1.2728195]\n",
      "Reward for action 20: -39.07630212688535\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 831\n",
      "Steps done: 6361\n",
      "SV: [ 1.2021668  -0.11420254 -0.24330504]\n",
      "Reward for action 20: -119.62387556754221\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 832\n",
      "Steps done: 6362\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 6: -37.44603091774524\n",
      "[0, 1, 6]\n",
      "Steps done: 6363\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 11: -14.6214619673147\n",
      "[0, 6]\n",
      "Steps done: 6364\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 3: -41.30076899452315\n",
      "[0, 6, 3]\n",
      "Steps done: 6365\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 13: -14.6214619673147\n",
      "[0, 6]\n",
      "Steps done: 6366\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 3: -41.30076899452315\n",
      "[0, 6, 3]\n",
      "Steps done: 6367\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 13: -14.6214619673147\n",
      "[0, 6]\n",
      "Steps done: 6368\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 7: -44.51672286282799\n",
      "[0, 6, 7]\n",
      "Steps done: 6369\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 5: -94.74586673798068\n",
      "[0, 6, 7, 5]\n",
      "Steps done: 6370\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 17: -73.30566619302938\n",
      "[0, 6, 5]\n",
      "Steps done: 6371\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 2: -87.47576535080724\n",
      "[0, 6, 5, 2]\n",
      "Steps done: 6372\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 12: -73.30566619302938\n",
      "[0, 6, 5]\n",
      "Steps done: 6373\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 10: -123.67973216345568\n",
      "[6, 5]\n",
      "Steps done: 6374\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 4: -138.46638981057382\n",
      "[6, 5, 4]\n",
      "Steps done: 6375\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 14: -123.67973216345568\n",
      "[6, 5]\n",
      "Steps done: 6376\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 2: -128.34123110736604\n",
      "[6, 5, 2]\n",
      "Steps done: 6377\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 15: -74.96984752921625\n",
      "[6, 2]\n",
      "Steps done: 6378\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 7: -63.12798286751961\n",
      "[6, 2, 7]\n",
      "Steps done: 6379\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 17: -74.96984752921625\n",
      "[6, 2]\n",
      "Steps done: 6380\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 1: -23.119759357703543\n",
      "[6, 2, 1]\n",
      "Steps done: 6381\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 16: -177.88011422841132\n",
      "[2, 1]\n",
      "Steps done: 6382\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 6: -23.119759357703543\n",
      "[2, 1, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6383\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 16: -177.88011422841132\n",
      "[2, 1]\n",
      "Steps done: 6384\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 5: -36.65355650378159\n",
      "[2, 1, 5]\n",
      "Steps done: 6385\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 15: -177.88011422841132\n",
      "[2, 1]\n",
      "Steps done: 6386\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 6: -23.119759357703543\n",
      "[2, 1, 6]\n",
      "Steps done: 6387\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Reward for action 7: -27.039537527055835\n",
      "[2, 1, 6, 7]\n",
      "Steps done: 6388\n",
      "SV: [ 0.27404562  0.12895557 -0.90049356]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -23.039537527055835\n",
      "[2, 1, 6, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 833\n",
      "Steps done: 6389\n",
      "SV: [-0.6489599   0.73840976 -0.6303857 ]\n",
      "Reward for action 20: -193.7826289063566\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 834\n",
      "Steps done: 6390\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 3: -23.301571690640333\n",
      "[0, 1, 3]\n",
      "Steps done: 6391\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 6: -33.44249199034324\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 6392\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 11: -77.33784381810169\n",
      "[0, 3, 6]\n",
      "Steps done: 6393\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 5: -41.28230472582165\n",
      "[0, 3, 6, 5]\n",
      "Steps done: 6394\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 10: -83.60159533030976\n",
      "[3, 6, 5]\n",
      "Steps done: 6395\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 15: -239.07300676910285\n",
      "[3, 6]\n",
      "Steps done: 6396\n",
      "SV: [-0.16387412 -0.05513066 -0.4480253 ]\n",
      "Reward for action 20: -235.07300676910285\n",
      "[3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 835\n",
      "Steps done: 6397\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 9: -113.85824337690234\n",
      "[0, 1, 9]\n",
      "Steps done: 6398\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 2: -84.86933826379676\n",
      "[0, 1, 9, 2]\n",
      "Steps done: 6399\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 6: -57.11601273484655\n",
      "[0, 1, 9, 2, 6]\n",
      "Did target update\n",
      "Steps done: 6400\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 10: -54.212404412526254\n",
      "[1, 9, 2, 6]\n",
      "Steps done: 6401\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 11: -47.12302682657422\n",
      "[9, 2, 6]\n",
      "Steps done: 6402\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 16: -166.9225312215075\n",
      "[9, 2]\n",
      "Steps done: 6403\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 6: -47.12302682657422\n",
      "[9, 2, 6]\n",
      "Steps done: 6404\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 3: -56.592279281672546\n",
      "[9, 2, 6, 3]\n",
      "Steps done: 6405\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 19: -47.9423160533445\n",
      "[2, 6, 3]\n",
      "Steps done: 6406\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 8: -48.81857755267645\n",
      "[2, 6, 3, 8]\n",
      "Steps done: 6407\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 13: -48.698620914642824\n",
      "[2, 6, 8]\n",
      "Steps done: 6408\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 9: -54.446661174856\n",
      "[2, 6, 8, 9]\n",
      "Steps done: 6409\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 16: -103.52584288993764\n",
      "[2, 8, 9]\n",
      "Steps done: 6410\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 19: -50.158906247720815\n",
      "[2, 8]\n",
      "Steps done: 6411\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 9: -103.52584288993764\n",
      "[2, 8, 9]\n",
      "Steps done: 6412\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 7: -30.188330178331455\n",
      "[2, 8, 9, 7]\n",
      "Steps done: 6413\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 3: -41.19282212132295\n",
      "[2, 8, 9, 7, 3]\n",
      "Steps done: 6414\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 13: -30.188330178331455\n",
      "[2, 8, 9, 7]\n",
      "Steps done: 6415\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 17: -103.52584288993764\n",
      "[2, 8, 9]\n",
      "Steps done: 6416\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 1: -79.74614052000791\n",
      "[2, 8, 9, 1]\n",
      "Steps done: 6417\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 6: -54.94600526214902\n",
      "[2, 8, 9, 1, 6]\n",
      "Steps done: 6418\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 16: -79.74614052000791\n",
      "[2, 8, 9, 1]\n",
      "Steps done: 6419\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 12: -110.31837417055867\n",
      "[8, 9, 1]\n",
      "Steps done: 6420\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 6: -57.160925936021975\n",
      "[8, 9, 1, 6]\n",
      "Steps done: 6421\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 3: -58.02123661872632\n",
      "[8, 9, 1, 6, 3]\n",
      "Steps done: 6422\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Reward for action 7: -39.31861490243208\n",
      "[8, 9, 1, 6, 3, 7]\n",
      "Steps done: 6423\n",
      "SV: [ 0.21114972 -0.02457144  0.7490535 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -35.31861490243208\n",
      "[8, 9, 1, 6, 3, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 836\n",
      "Steps done: 6424\n",
      "SV: [-1.81803218e-04  1.08830445e-01 -5.56888282e-01]\n",
      "Reward for action 5: -32.538020975495186\n",
      "[0, 1, 5]\n",
      "Steps done: 6425\n",
      "SV: [-1.81803218e-04  1.08830445e-01 -5.56888282e-01]\n",
      "Reward for action 3: -46.815962332315834\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 6426\n",
      "SV: [-1.81803218e-04  1.08830445e-01 -5.56888282e-01]\n",
      "Reward for action 13: -32.538020975495186\n",
      "[0, 1, 5]\n",
      "Steps done: 6427\n",
      "SV: [-1.81803218e-04  1.08830445e-01 -5.56888282e-01]\n",
      "Reward for action 20: -28.538020975495186\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 837\n",
      "Steps done: 6428\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 8: -14.765850961886525\n",
      "[0, 1, 8]\n",
      "Steps done: 6429\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 6: -13.663570814839405\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 6430\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 7: -6.892349550948587\n",
      "[0, 1, 8, 6, 7]\n",
      "Steps done: 6431\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 4: -7.159157839300171\n",
      "[0, 1, 8, 6, 7, 4]\n",
      "Steps done: 6432\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 3: -6.751841104893609\n",
      "[0, 1, 8, 6, 7, 4, 3]\n",
      "Steps done: 6433\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 16: -7.030373883025038\n",
      "[0, 1, 8, 7, 4, 3]\n",
      "Steps done: 6434\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 2: -6.886461177243317\n",
      "[0, 1, 8, 7, 4, 3, 2]\n",
      "Steps done: 6435\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 12: -7.030373883025038\n",
      "[0, 1, 8, 7, 4, 3]\n",
      "Steps done: 6436\n",
      "SV: [ 0.2051233   0.16837348 -0.13245296]\n",
      "Reward for action 20: -3.030373883025038\n",
      "[0, 1, 8, 7, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 838\n",
      "Steps done: 6437\n",
      "SV: [ 0.22493401 -0.07184403 -0.4098998 ]\n",
      "Reward for action 4: -17.860840106364762\n",
      "[0, 1, 4]\n",
      "Steps done: 6438\n",
      "SV: [ 0.22493401 -0.07184403 -0.4098998 ]\n",
      "Reward for action 20: -13.860840106364762\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 839\n",
      "Steps done: 6439\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 4: -67.05544716064995\n",
      "[0, 1, 4]\n",
      "Steps done: 6440\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 10: -150.34141655874458\n",
      "[1, 4]\n",
      "Steps done: 6441\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 3: -155.41561035967692\n",
      "[1, 4, 3]\n",
      "Steps done: 6442\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 5: -55.72990884519884\n",
      "[1, 4, 3, 5]\n",
      "Steps done: 6443\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 14: -9.362406461726557\n",
      "[1, 3, 5]\n",
      "Steps done: 6444\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 6: -5.130053800505082\n",
      "[1, 3, 5, 6]\n",
      "Steps done: 6445\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 2: -1.9493228421466018\n",
      "[1, 3, 5, 6, 2]\n",
      "Steps done: 6446\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 16: -1.1113762207746158\n",
      "[1, 3, 5, 2]\n",
      "Steps done: 6447\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 11: -17.73269143266273\n",
      "[3, 5, 2]\n",
      "Steps done: 6448\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 6: -1.811561114475523\n",
      "[3, 5, 2, 6]\n",
      "Steps done: 6449\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 4: -3.51876261837121\n",
      "[3, 5, 2, 6, 4]\n",
      "Steps done: 6450\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 8: -6.757360394317895\n",
      "[3, 5, 2, 6, 4, 8]\n",
      "Steps done: 6451\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 0: -0.5592543673301447\n",
      "[3, 5, 2, 6, 4, 8, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6452\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 13: -1.4051946852689978\n",
      "[5, 2, 6, 4, 8, 0]\n",
      "Steps done: 6453\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 14: -1.4234651752926557\n",
      "[5, 2, 6, 8, 0]\n",
      "Steps done: 6454\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 18: -1.0065825816815108\n",
      "[5, 2, 6, 0]\n",
      "Steps done: 6455\n",
      "SV: [ 0.8525298  -0.8929283   0.50141376]\n",
      "Reward for action 20: 2.993417418318489\n",
      "[5, 2, 6, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 840\n",
      "Steps done: 6456\n",
      "SV: [ 0.05038777  0.0383545  -0.05040821]\n",
      "Reward for action 2: -24.313047224813626\n",
      "[0, 1, 2]\n",
      "Steps done: 6457\n",
      "SV: [ 0.05038777  0.0383545  -0.05040821]\n",
      "Reward for action 4: -8.096612038573411\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 6458\n",
      "SV: [ 0.05038777  0.0383545  -0.05040821]\n",
      "Reward for action 3: -5.1198840312005665\n",
      "[0, 1, 2, 4, 3]\n",
      "Steps done: 6459\n",
      "SV: [ 0.05038777  0.0383545  -0.05040821]\n",
      "Reward for action 13: -8.096612038573411\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 6460\n",
      "SV: [ 0.05038777  0.0383545  -0.05040821]\n",
      "Reward for action 20: -4.0966120385734115\n",
      "[0, 1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 841\n",
      "Steps done: 6461\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 8: -1.3133305623976461\n",
      "[0, 1, 8]\n",
      "Steps done: 6462\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 6: -0.7829022790775774\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 6463\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 11: -0.2186980961389307\n",
      "[0, 8, 6]\n",
      "Steps done: 6464\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 18: -1.1854153428720438\n",
      "[0, 6]\n",
      "Steps done: 6465\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 8: -0.21869809613893082\n",
      "[0, 6, 8]\n",
      "Steps done: 6466\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 16: -0.22579644688446174\n",
      "[0, 8]\n",
      "Steps done: 6467\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 6: -0.2186980961389307\n",
      "[0, 8, 6]\n",
      "Steps done: 6468\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 16: -0.22579644688446174\n",
      "[0, 8]\n",
      "Steps done: 6469\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 5: -6.622722180526898\n",
      "[0, 8, 5]\n",
      "Steps done: 6470\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 6: -3.921029643166583\n",
      "[0, 8, 5, 6]\n",
      "Steps done: 6471\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 15: -0.2186980961389307\n",
      "[0, 8, 6]\n",
      "Steps done: 6472\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 10: -2.0815769165893503\n",
      "[8, 6]\n",
      "Steps done: 6473\n",
      "SV: [-0.10851856  0.05647638 -0.17639402]\n",
      "Reward for action 20: 1.9184230834106497\n",
      "[8, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 842\n",
      "Steps done: 6474\n",
      "SV: [-1.0202835 -1.7500404 -1.0893588]\n",
      "Reward for action 20: -124.31943945602444\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 843\n",
      "Steps done: 6475\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 4: -130.71627136161314\n",
      "[0, 1, 4]\n",
      "Steps done: 6476\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 3: -130.67396479864672\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 6477\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 2: -146.9749196750583\n",
      "[0, 1, 4, 3, 2]\n",
      "Steps done: 6478\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 7: -140.8459383587714\n",
      "[0, 1, 4, 3, 2, 7]\n",
      "Steps done: 6479\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 17: -146.9749196750583\n",
      "[0, 1, 4, 3, 2]\n",
      "Steps done: 6480\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 14: -162.8228566533843\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 6481\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 4: -146.9749196750582\n",
      "[0, 1, 3, 2, 4]\n",
      "Steps done: 6482\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 11: -161.8603432052632\n",
      "[0, 3, 2, 4]\n",
      "Steps done: 6483\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 5: -152.2633572770822\n",
      "[0, 3, 2, 4, 5]\n",
      "Steps done: 6484\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 12: -128.66026185022358\n",
      "[0, 3, 4, 5]\n",
      "Steps done: 6485\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 10: -142.76848222025959\n",
      "[3, 4, 5]\n",
      "Steps done: 6486\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 2: -164.12601636631953\n",
      "[3, 4, 5, 2]\n",
      "Steps done: 6487\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 15: -153.06243217013676\n",
      "[3, 4, 2]\n",
      "Steps done: 6488\n",
      "SV: [ 0.35969296  0.30102018 -1.1778039 ]\n",
      "Reward for action 20: -149.06243217013676\n",
      "[3, 4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 844\n",
      "Steps done: 6489\n",
      "SV: [-0.12975757  0.0301218   0.6735428 ]\n",
      "Reward for action 20: -81.99169695874077\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 845\n",
      "Steps done: 6490\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 7: -297.4721582631642\n",
      "[0, 1, 7]\n",
      "Steps done: 6491\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 8: -215.72527849414374\n",
      "[0, 1, 7, 8]\n",
      "Steps done: 6492\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 11: -153.49049456802115\n",
      "[0, 7, 8]\n",
      "Steps done: 6493\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 4: -154.67740134675702\n",
      "[0, 7, 8, 4]\n",
      "Steps done: 6494\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 3: -178.96000197994977\n",
      "[0, 7, 8, 4, 3]\n",
      "Steps done: 6495\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 17: -140.28075324721365\n",
      "[0, 8, 4, 3]\n",
      "Steps done: 6496\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 13: -132.22264044225892\n",
      "[0, 8, 4]\n",
      "Steps done: 6497\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 5: -112.74995660912892\n",
      "[0, 8, 4, 5]\n",
      "Steps done: 6498\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 7: -130.63394067645558\n",
      "[0, 8, 4, 5, 7]\n",
      "Steps done: 6499\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 1: -153.5717210471821\n",
      "[0, 8, 4, 5, 7, 1]\n",
      "Did target update\n",
      "Steps done: 6500\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 11: -130.63394067645558\n",
      "[0, 8, 4, 5, 7]\n",
      "Steps done: 6501\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 18: -135.98679410726297\n",
      "[0, 4, 5, 7]\n",
      "Steps done: 6502\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 3: -159.60795181060752\n",
      "[0, 4, 5, 7, 3]\n",
      "Steps done: 6503\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 13: -135.98679410726297\n",
      "[0, 4, 5, 7]\n",
      "Steps done: 6504\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 3: -159.60795181060752\n",
      "[0, 4, 5, 7, 3]\n",
      "Steps done: 6505\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 6: -134.26509452007537\n",
      "[0, 4, 5, 7, 3, 6]\n",
      "Steps done: 6506\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 10: -133.31960521305012\n",
      "[4, 5, 7, 3, 6]\n",
      "Steps done: 6507\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 13: -88.07251199438527\n",
      "[4, 5, 7, 6]\n",
      "Steps done: 6508\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 16: -141.32723824875194\n",
      "[4, 5, 7]\n",
      "Steps done: 6509\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 3: -167.32444499987434\n",
      "[4, 5, 7, 3]\n",
      "Steps done: 6510\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 8: -151.29143169145863\n",
      "[4, 5, 7, 3, 8]\n",
      "Steps done: 6511\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 14: -156.04996334617405\n",
      "[5, 7, 3, 8]\n",
      "Steps done: 6512\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 13: -120.5369171535755\n",
      "[5, 7, 8]\n",
      "Steps done: 6513\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 2: -138.397030074812\n",
      "[5, 7, 8, 2]\n",
      "Steps done: 6514\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 17: -122.48822252062274\n",
      "[5, 8, 2]\n",
      "Steps done: 6515\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Reward for action 6: -108.35475528455834\n",
      "[5, 8, 2, 6]\n",
      "Steps done: 6516\n",
      "SV: [ 0.29673132  0.16635837 -1.3480048 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -104.35475528455834\n",
      "[5, 8, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 846\n",
      "Steps done: 6517\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 3: -6.231439121953562\n",
      "[0, 1, 3]\n",
      "Steps done: 6518\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 7: -0.6059943857943638\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 6519\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 6: -2.684926087497534\n",
      "[0, 1, 3, 7, 6]\n",
      "Steps done: 6520\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 17: -11.781867537512428\n",
      "[0, 1, 3, 6]\n",
      "Steps done: 6521\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 16: -6.231439121953562\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6522\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 10: -0.7441358188175364\n",
      "[1, 3]\n",
      "Steps done: 6523\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 5: -102.21507546769507\n",
      "[1, 3, 5]\n",
      "Steps done: 6524\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 6: -80.07830326179985\n",
      "[1, 3, 5, 6]\n",
      "Steps done: 6525\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 11: -93.38172966642159\n",
      "[3, 5, 6]\n",
      "Steps done: 6526\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 8: -99.55591908424317\n",
      "[3, 5, 6, 8]\n",
      "Steps done: 6527\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 18: -93.38172966642159\n",
      "[3, 5, 6]\n",
      "Steps done: 6528\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 8: -99.55591908424317\n",
      "[3, 5, 6, 8]\n",
      "Steps done: 6529\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 4: -21.15153498550772\n",
      "[3, 5, 6, 8, 4]\n",
      "Steps done: 6530\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 14: -99.55591908424317\n",
      "[3, 5, 6, 8]\n",
      "Steps done: 6531\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 0: -100.05295501959938\n",
      "[3, 5, 6, 8, 0]\n",
      "Steps done: 6532\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 18: -106.00930117959919\n",
      "[3, 5, 6, 0]\n",
      "Steps done: 6533\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 8: -100.05295501959938\n",
      "[3, 5, 6, 0, 8]\n",
      "Steps done: 6534\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 18: -106.00930117959919\n",
      "[3, 5, 6, 0]\n",
      "Steps done: 6535\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 8: -100.05295501959938\n",
      "[3, 5, 6, 0, 8]\n",
      "Steps done: 6536\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 2: -41.09578890839156\n",
      "[3, 5, 6, 0, 8, 2]\n",
      "Steps done: 6537\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 4: -17.473527986603752\n",
      "[3, 5, 6, 0, 8, 2, 4]\n",
      "Steps done: 6538\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 14: -41.09578890839156\n",
      "[3, 5, 6, 0, 8, 2]\n",
      "Steps done: 6539\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 7: -26.481060972420607\n",
      "[3, 5, 6, 0, 8, 2, 7]\n",
      "Steps done: 6540\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 13: -40.21960520859538\n",
      "[5, 6, 0, 8, 2, 7]\n",
      "Steps done: 6541\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 16: -29.207616190872802\n",
      "[5, 0, 8, 2, 7]\n",
      "Steps done: 6542\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Reward for action 4: -8.01291549012889\n",
      "[5, 0, 8, 2, 7, 4]\n",
      "Steps done: 6543\n",
      "SV: [0.15011962 0.1406647  0.21761988]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -4.01291549012889\n",
      "[5, 0, 8, 2, 7, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 847\n",
      "Steps done: 6544\n",
      "SV: [ 0.35415384 -0.2442055   0.67174345]\n",
      "Reward for action 20: -5.488671941504904\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 848\n",
      "Steps done: 6545\n",
      "SV: [ 0.3610794   0.48511115 -0.5688885 ]\n",
      "Reward for action 8: -101.21331506597579\n",
      "[0, 1, 8]\n",
      "Steps done: 6546\n",
      "SV: [ 0.3610794   0.48511115 -0.5688885 ]\n",
      "Reward for action 11: -45.5722467193508\n",
      "[0, 8]\n",
      "Steps done: 6547\n",
      "SV: [ 0.3610794   0.48511115 -0.5688885 ]\n",
      "Reward for action 1: -101.21331506597579\n",
      "[0, 8, 1]\n",
      "Steps done: 6548\n",
      "SV: [ 0.3610794   0.48511115 -0.5688885 ]\n",
      "Reward for action 20: -97.21331506597579\n",
      "[0, 8, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 849\n",
      "Steps done: 6549\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 7: -23.166412396510356\n",
      "[0, 1, 7]\n",
      "Steps done: 6550\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 3: -22.33786164081199\n",
      "[0, 1, 7, 3]\n",
      "Steps done: 6551\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 11: -15.451498534569645\n",
      "[0, 7, 3]\n",
      "Steps done: 6552\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 10: -107.7147683007901\n",
      "[7, 3]\n",
      "Steps done: 6553\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 1: -21.657150902393475\n",
      "[7, 3, 1]\n",
      "Steps done: 6554\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 11: -107.7147683007901\n",
      "[7, 3]\n",
      "Steps done: 6555\n",
      "SV: [ 0.00404956  0.23015653 -0.8110483 ]\n",
      "Reward for action 20: -103.7147683007901\n",
      "[7, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 850\n",
      "Steps done: 6556\n",
      "SV: [-0.00915108  0.00877706  0.20562749]\n",
      "Reward for action 4: -47.004006754736885\n",
      "[0, 1, 4]\n",
      "Steps done: 6557\n",
      "SV: [-0.00915108  0.00877706  0.20562749]\n",
      "Reward for action 10: -108.73231948558093\n",
      "[1, 4]\n",
      "Steps done: 6558\n",
      "SV: [-0.00915108  0.00877706  0.20562749]\n",
      "Reward for action 20: -104.73231948558093\n",
      "[1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 851\n",
      "Steps done: 6559\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 6: -18.368990726866706\n",
      "[0, 1, 6]\n",
      "Steps done: 6560\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 11: -19.72579645403672\n",
      "[0, 6]\n",
      "Steps done: 6561\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 1: -18.368990726866706\n",
      "[0, 6, 1]\n",
      "Steps done: 6562\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 5: -18.80454554782189\n",
      "[0, 6, 1, 5]\n",
      "Steps done: 6563\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 2: -18.79445706045706\n",
      "[0, 6, 1, 5, 2]\n",
      "Steps done: 6564\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 16: -20.05515139624288\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 6565\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 6: -18.79445706045707\n",
      "[0, 1, 5, 2, 6]\n",
      "Steps done: 6566\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 16: -20.05515139624288\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 6567\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 6: -18.79445706045707\n",
      "[0, 1, 5, 2, 6]\n",
      "Steps done: 6568\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 11: -20.5590825444047\n",
      "[0, 5, 2, 6]\n",
      "Steps done: 6569\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 16: -22.61146684591375\n",
      "[0, 5, 2]\n",
      "Steps done: 6570\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 6: -20.5590825444047\n",
      "[0, 5, 2, 6]\n",
      "Steps done: 6571\n",
      "SV: [0.1132743  0.11914193 0.5096188 ]\n",
      "Reward for action 20: -16.5590825444047\n",
      "[0, 5, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 852\n",
      "Steps done: 6572\n",
      "SV: [1.5529021  0.61257356 7.070063  ]\n",
      "Reward for action 4: -10.254317236590776\n",
      "[0, 1, 4]\n",
      "Steps done: 6573\n",
      "SV: [1.5529021  0.61257356 7.070063  ]\n",
      "Reward for action 10: -1296.721846979336\n",
      "[1, 4]\n",
      "Steps done: 6574\n",
      "SV: [1.5529021  0.61257356 7.070063  ]\n",
      "Reward for action 2: -377.9187402229629\n",
      "[1, 4, 2]\n",
      "Steps done: 6575\n",
      "SV: [1.5529021  0.61257356 7.070063  ]\n",
      "Reward for action 11: -1031.32291910595\n",
      "[4, 2]\n",
      "Steps done: 6576\n",
      "SV: [1.5529021  0.61257356 7.070063  ]\n",
      "Reward for action 20: -1027.32291910595\n",
      "[4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 853\n",
      "Steps done: 6577\n",
      "SV: [ 0.2979814  1.4964384 -2.3075714]\n",
      "Reward for action 2: -254.01280110747808\n",
      "[0, 1, 2]\n",
      "Steps done: 6578\n",
      "SV: [ 0.2979814  1.4964384 -2.3075714]\n",
      "Reward for action 20: -250.01280110747808\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 854\n",
      "Steps done: 6579\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 9: -15.651657434784328\n",
      "[0, 1, 9]\n",
      "Steps done: 6580\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 6: -2.391815244325988\n",
      "[0, 1, 9, 6]\n",
      "Steps done: 6581\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 4: -10.316740927678133\n",
      "[0, 1, 9, 6, 4]\n",
      "Steps done: 6582\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 11: -12.305971580178005\n",
      "[0, 9, 6, 4]\n",
      "Steps done: 6583\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 2: -133.9039722364087\n",
      "[0, 9, 6, 4, 2]\n",
      "Steps done: 6584\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 7: -9.264930547572696\n",
      "[0, 9, 6, 4, 2, 7]\n",
      "Steps done: 6585\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 16: -5.2691380309445055\n",
      "[0, 9, 4, 2, 7]\n",
      "Steps done: 6586\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 8: -4.224094781890576\n",
      "[0, 9, 4, 2, 7, 8]\n",
      "Steps done: 6587\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 1: -19.153608156693174\n",
      "[0, 9, 4, 2, 7, 8, 1]\n",
      "Steps done: 6588\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 10: -20.02174472774515\n",
      "[9, 4, 2, 7, 8, 1]\n",
      "Steps done: 6589\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 6: -12.841875643790502\n",
      "[9, 4, 2, 7, 8, 1, 6]\n",
      "Steps done: 6590\n",
      "SV: [ 0.34454247 -0.34007347  1.184653  ]\n",
      "Reward for action 20: -8.841875643790502\n",
      "[9, 4, 2, 7, 8, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6591\n",
      "SV: [ 0.08470234 -0.48695835 -0.5881025 ]\n",
      "Reward for action 5: -46.48668060813015\n",
      "[0, 1, 5]\n",
      "Steps done: 6592\n",
      "SV: [ 0.08470234 -0.48695835 -0.5881025 ]\n",
      "Reward for action 15: -39.40719137788905\n",
      "[0, 1]\n",
      "Steps done: 6593\n",
      "SV: [ 0.08470234 -0.48695835 -0.5881025 ]\n",
      "Reward for action 4: -38.87268478967071\n",
      "[0, 1, 4]\n",
      "Steps done: 6594\n",
      "SV: [ 0.08470234 -0.48695835 -0.5881025 ]\n",
      "Reward for action 14: -39.40719137788905\n",
      "[0, 1]\n",
      "Steps done: 6595\n",
      "SV: [ 0.08470234 -0.48695835 -0.5881025 ]\n",
      "Reward for action 20: -35.40719137788905\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 856\n",
      "Steps done: 6596\n",
      "SV: [-0.01877373 -0.18299222  0.01951193]\n",
      "Reward for action 3: -21.393138954432505\n",
      "[0, 1, 3]\n",
      "Steps done: 6597\n",
      "SV: [-0.01877373 -0.18299222  0.01951193]\n",
      "Reward for action 11: -4.031130018896707\n",
      "[0, 3]\n",
      "Steps done: 6598\n",
      "SV: [-0.01877373 -0.18299222  0.01951193]\n",
      "Reward for action 20: -0.031130018896707412\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 857\n",
      "Steps done: 6599\n",
      "SV: [ 0.890796   1.2143782 -1.9886109]\n",
      "Reward for action 2: -78.13738602240474\n",
      "[0, 1, 2]\n",
      "Did target update\n",
      "Steps done: 6600\n",
      "SV: [ 0.890796   1.2143782 -1.9886109]\n",
      "Reward for action 3: -111.36633405156459\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 6601\n",
      "SV: [ 0.890796   1.2143782 -1.9886109]\n",
      "Reward for action 11: -89.95730309879463\n",
      "[0, 2, 3]\n",
      "Steps done: 6602\n",
      "SV: [ 0.890796   1.2143782 -1.9886109]\n",
      "Reward for action 12: -149.02313165407367\n",
      "[0, 3]\n",
      "Steps done: 6603\n",
      "SV: [ 0.890796   1.2143782 -1.9886109]\n",
      "Reward for action 20: -145.02313165407367\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 858\n",
      "Steps done: 6604\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 7: -13.357617982679159\n",
      "[0, 1, 7]\n",
      "Steps done: 6605\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 2: -18.75874811668103\n",
      "[0, 1, 7, 2]\n",
      "Steps done: 6606\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 6: -54.38981486454665\n",
      "[0, 1, 7, 2, 6]\n",
      "Steps done: 6607\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 10: -53.00055533962424\n",
      "[1, 7, 2, 6]\n",
      "Steps done: 6608\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 9: -39.336281066027915\n",
      "[1, 7, 2, 6, 9]\n",
      "Steps done: 6609\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 4: -5.332123745524045\n",
      "[1, 7, 2, 6, 9, 4]\n",
      "Steps done: 6610\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 17: -37.50891414901792\n",
      "[1, 2, 6, 9, 4]\n",
      "Steps done: 6611\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 11: -51.66109721996166\n",
      "[2, 6, 9, 4]\n",
      "Steps done: 6612\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 14: -243.2384651435368\n",
      "[2, 6, 9]\n",
      "Steps done: 6613\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 16: -313.83951901154694\n",
      "[2, 9]\n",
      "Steps done: 6614\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 7: -11.472316188743012\n",
      "[2, 9, 7]\n",
      "Steps done: 6615\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 3: -6.957131042693583\n",
      "[2, 9, 7, 3]\n",
      "Steps done: 6616\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 12: -98.30071863598708\n",
      "[9, 7, 3]\n",
      "Steps done: 6617\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 17: -23.097090717408157\n",
      "[9, 3]\n",
      "Steps done: 6618\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 6: -98.50670797459671\n",
      "[9, 3, 6]\n",
      "Steps done: 6619\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 7: -6.712770591894107\n",
      "[9, 3, 6, 7]\n",
      "Steps done: 6620\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 16: -98.30071863598704\n",
      "[9, 3, 7]\n",
      "Steps done: 6621\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 5: -5.9755426538863405\n",
      "[9, 3, 7, 5]\n",
      "Steps done: 6622\n",
      "SV: [0.41274497 1.0339594  1.1355217 ]\n",
      "Reward for action 20: -1.9755426538863405\n",
      "[9, 3, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 859\n",
      "Steps done: 6623\n",
      "SV: [ 0.55489546 -0.03079243 -1.4597217 ]\n",
      "Reward for action 5: -23.395796705034343\n",
      "[0, 1, 5]\n",
      "Steps done: 6624\n",
      "SV: [ 0.55489546 -0.03079243 -1.4597217 ]\n",
      "Reward for action 20: -19.395796705034343\n",
      "[0, 1, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 860\n",
      "Steps done: 6625\n",
      "SV: [ 0.39855728  0.239863   -0.23239437]\n",
      "Reward for action 20: -18.690119726971847\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 861\n",
      "Steps done: 6626\n",
      "SV: [ 0.39202788 -0.37113342 -1.0103693 ]\n",
      "Reward for action 20: -90.1693356825631\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 863\n",
      "Steps done: 6627\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 2: -47.68737142777661\n",
      "[0, 1, 2]\n",
      "Steps done: 6628\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 4: -38.632557121027915\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 6629\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 14: -47.68737142777661\n",
      "[0, 1, 2]\n",
      "Steps done: 6630\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 10: -41.441738912733804\n",
      "[1, 2]\n",
      "Steps done: 6631\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 5: -60.09751146986159\n",
      "[1, 2, 5]\n",
      "Steps done: 6632\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 3: -56.646155167795975\n",
      "[1, 2, 5, 3]\n",
      "Steps done: 6633\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 11: -69.93401775451771\n",
      "[2, 5, 3]\n",
      "Steps done: 6634\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 15: -69.834751799569\n",
      "[2, 3]\n",
      "Steps done: 6635\n",
      "SV: [0.5695576  0.25375447 0.69020146]\n",
      "Reward for action 20: -65.834751799569\n",
      "[2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 864\n",
      "Steps done: 6636\n",
      "SV: [ 0.1155022  -0.0886048  -0.62953264]\n",
      "Reward for action 20: -29.960626937724818\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 865\n",
      "Steps done: 6637\n",
      "SV: [-0.13384211 -0.0117044   0.18390094]\n",
      "Reward for action 20: -68.08694643621563\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 866\n",
      "Steps done: 6638\n",
      "SV: [0.04590745 0.08273656 0.17630063]\n",
      "Reward for action 9: -21.33591216547169\n",
      "[0, 1, 9]\n",
      "Steps done: 6639\n",
      "SV: [0.04590745 0.08273656 0.17630063]\n",
      "Reward for action 8: -7.255116682175674\n",
      "[0, 1, 9, 8]\n",
      "Steps done: 6640\n",
      "SV: [0.04590745 0.08273656 0.17630063]\n",
      "Reward for action 4: -4.222304375700628\n",
      "[0, 1, 9, 8, 4]\n",
      "Steps done: 6641\n",
      "SV: [0.04590745 0.08273656 0.17630063]\n",
      "Reward for action 18: -8.230889514882668\n",
      "[0, 1, 9, 4]\n",
      "Steps done: 6642\n",
      "SV: [0.04590745 0.08273656 0.17630063]\n",
      "Reward for action 14: -21.33591216547169\n",
      "[0, 1, 9]\n",
      "Steps done: 6643\n",
      "SV: [0.04590745 0.08273656 0.17630063]\n",
      "Reward for action 20: -17.33591216547169\n",
      "[0, 1, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 867\n",
      "Steps done: 6644\n",
      "SV: [ 0.29533878 -0.05011423  0.5854281 ]\n",
      "Reward for action 4: -30.58876297228742\n",
      "[0, 1, 4]\n",
      "Steps done: 6645\n",
      "SV: [ 0.29533878 -0.05011423  0.5854281 ]\n",
      "Reward for action 11: -126.52735323423121\n",
      "[0, 4]\n",
      "Steps done: 6646\n",
      "SV: [ 0.29533878 -0.05011423  0.5854281 ]\n",
      "Reward for action 20: -122.52735323423121\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 868\n",
      "Steps done: 6647\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 8: -35.93649836257708\n",
      "[0, 1, 8]\n",
      "Steps done: 6648\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 6: -33.19500069135222\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 6649\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 7: -36.653145285101516\n",
      "[0, 1, 8, 6, 7]\n",
      "Steps done: 6650\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 2: -32.88368517852558\n",
      "[0, 1, 8, 6, 7, 2]\n",
      "Steps done: 6651\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 17: -36.43984955954951\n",
      "[0, 1, 8, 6, 2]\n",
      "Steps done: 6652\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 10: -34.10569526502572\n",
      "[1, 8, 6, 2]\n",
      "Steps done: 6653\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 3: -36.72499022620614\n",
      "[1, 8, 6, 2, 3]\n",
      "Steps done: 6654\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 4: -45.13065735747877\n",
      "[1, 8, 6, 2, 3, 4]\n",
      "Steps done: 6655\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 14: -36.72499022620614\n",
      "[1, 8, 6, 2, 3]\n",
      "Steps done: 6656\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 13: -34.10569526502572\n",
      "[1, 8, 6, 2]\n",
      "Steps done: 6657\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 3: -36.72499022620614\n",
      "[1, 8, 6, 2, 3]\n",
      "Steps done: 6658\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 13: -34.10569526502572\n",
      "[1, 8, 6, 2]\n",
      "Steps done: 6659\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 9: -29.05011272802698\n",
      "[1, 8, 6, 2, 9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6660\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 3: -27.86230447669608\n",
      "[1, 8, 6, 2, 9, 3]\n",
      "Steps done: 6661\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 13: -29.05011272802698\n",
      "[1, 8, 6, 2, 9]\n",
      "Steps done: 6662\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 18: -32.44063526057579\n",
      "[1, 6, 2, 9]\n",
      "Steps done: 6663\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 5: -31.462832022678597\n",
      "[1, 6, 2, 9, 5]\n",
      "Steps done: 6664\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 3: -32.00163548017981\n",
      "[1, 6, 2, 9, 5, 3]\n",
      "Steps done: 6665\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 16: -29.81318088193801\n",
      "[1, 2, 9, 5, 3]\n",
      "Steps done: 6666\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 13: -37.94999777967823\n",
      "[1, 2, 9, 5]\n",
      "Steps done: 6667\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 8: -36.25048906768312\n",
      "[1, 2, 9, 5, 8]\n",
      "Steps done: 6668\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 3: -29.82818403517087\n",
      "[1, 2, 9, 5, 8, 3]\n",
      "Steps done: 6669\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 13: -36.25048906768312\n",
      "[1, 2, 9, 5, 8]\n",
      "Steps done: 6670\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 6: -32.75198311757843\n",
      "[1, 2, 9, 5, 8, 6]\n",
      "Steps done: 6671\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 0: -28.906775875944632\n",
      "[1, 2, 9, 5, 8, 6, 0]\n",
      "Steps done: 6672\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Reward for action 10: -32.75198311757843\n",
      "[1, 2, 9, 5, 8, 6]\n",
      "Steps done: 6673\n",
      "SV: [0.44796148 0.55025494 0.9068349 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -28.751983117578433\n",
      "[1, 2, 9, 5, 8, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 869\n",
      "Steps done: 6674\n",
      "SV: [0.06885469 0.03084184 0.4730997 ]\n",
      "Reward for action 3: -19.179634710189717\n",
      "[0, 1, 3]\n",
      "Steps done: 6675\n",
      "SV: [0.06885469 0.03084184 0.4730997 ]\n",
      "Reward for action 20: -15.179634710189717\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 870\n",
      "Steps done: 6676\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 8: -71.94848633400414\n",
      "[0, 1, 8]\n",
      "Steps done: 6677\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 3: -43.73608249517176\n",
      "[0, 1, 8, 3]\n",
      "Steps done: 6678\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 10: -72.34585898072277\n",
      "[1, 8, 3]\n",
      "Steps done: 6679\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 4: -31.909668958178614\n",
      "[1, 8, 3, 4]\n",
      "Steps done: 6680\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 11: -87.52747211980527\n",
      "[8, 3, 4]\n",
      "Steps done: 6681\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 7: -22.398749413583364\n",
      "[8, 3, 4, 7]\n",
      "Steps done: 6682\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 6: -9.65898094796669\n",
      "[8, 3, 4, 7, 6]\n",
      "Steps done: 6683\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 17: -9.783640902018508\n",
      "[8, 3, 4, 6]\n",
      "Steps done: 6684\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 1: -8.425165577835834\n",
      "[8, 3, 4, 6, 1]\n",
      "Steps done: 6685\n",
      "SV: [ 0.04647003  0.01124945 -0.48226482]\n",
      "Reward for action 20: -4.425165577835834\n",
      "[8, 3, 4, 6, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 871\n",
      "Steps done: 6686\n",
      "SV: [ 0.0768351  -0.08867934 -0.70444673]\n",
      "Reward for action 20: -43.070493821938584\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 872\n",
      "Steps done: 6687\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 8: -79.85235723299995\n",
      "[0, 1, 8]\n",
      "Steps done: 6688\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 9: -3.211423578466491\n",
      "[0, 1, 8, 9]\n",
      "Steps done: 6689\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 10: -11.168926371983682\n",
      "[1, 8, 9]\n",
      "Steps done: 6690\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 2: -17.963332450576136\n",
      "[1, 8, 9, 2]\n",
      "Steps done: 6691\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 6: -10.881872224807777\n",
      "[1, 8, 9, 2, 6]\n",
      "Steps done: 6692\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 7: -126.05006515824007\n",
      "[1, 8, 9, 2, 6, 7]\n",
      "Steps done: 6693\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 17: -10.881872224807777\n",
      "[1, 8, 9, 2, 6]\n",
      "Steps done: 6694\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 12: -17.903643824911157\n",
      "[1, 8, 9, 6]\n",
      "Steps done: 6695\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 18: -85.83108086386119\n",
      "[1, 9, 6]\n",
      "Steps done: 6696\n",
      "SV: [ 0.07664342 -0.04859559  0.49814913]\n",
      "Reward for action 20: -81.83108086386119\n",
      "[1, 9, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 873\n",
      "Steps done: 6697\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 6: -367.5410476846923\n",
      "[0, 1, 6]\n",
      "Steps done: 6698\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 16: -487.86362424089293\n",
      "[0, 1]\n",
      "Steps done: 6699\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 2: -437.28435857889804\n",
      "[0, 1, 2]\n",
      "Did target update\n",
      "Steps done: 6700\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 12: -487.86362424089293\n",
      "[0, 1]\n",
      "Steps done: 6701\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 6: -367.5410476846923\n",
      "[0, 1, 6]\n",
      "Steps done: 6702\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 9: -266.46429809601625\n",
      "[0, 1, 6, 9]\n",
      "Steps done: 6703\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 2: -251.36227351398847\n",
      "[0, 1, 6, 9, 2]\n",
      "Steps done: 6704\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 8: -228.23569057553112\n",
      "[0, 1, 6, 9, 2, 8]\n",
      "Steps done: 6705\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 16: -220.13783008839562\n",
      "[0, 1, 9, 2, 8]\n",
      "Steps done: 6706\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 7: -232.71519622798883\n",
      "[0, 1, 9, 2, 8, 7]\n",
      "Steps done: 6707\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 10: -200.16659142970835\n",
      "[1, 9, 2, 8, 7]\n",
      "Steps done: 6708\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 5: -146.31136074676596\n",
      "[1, 9, 2, 8, 7, 5]\n",
      "Steps done: 6709\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 3: -147.8061618264925\n",
      "[1, 9, 2, 8, 7, 5, 3]\n",
      "Steps done: 6710\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 19: -148.0342168536935\n",
      "[1, 2, 8, 7, 5, 3]\n",
      "Steps done: 6711\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 6: -143.61591791065834\n",
      "[1, 2, 8, 7, 5, 3, 6]\n",
      "Steps done: 6712\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 9: -144.31967852440087\n",
      "[1, 2, 8, 7, 5, 3, 6, 9]\n",
      "Steps done: 6713\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 17: -109.65335057184348\n",
      "[1, 2, 8, 5, 3, 6, 9]\n",
      "Steps done: 6714\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 12: -97.91999576162146\n",
      "[1, 8, 5, 3, 6, 9]\n",
      "Steps done: 6715\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 18: -183.00641591322415\n",
      "[1, 5, 3, 6, 9]\n",
      "Steps done: 6716\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 8: -97.91999576162144\n",
      "[1, 5, 3, 6, 9, 8]\n",
      "Steps done: 6717\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 13: -82.16402339520957\n",
      "[1, 5, 6, 9, 8]\n",
      "Steps done: 6718\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 3: -97.91999576162151\n",
      "[1, 5, 6, 9, 8, 3]\n",
      "Steps done: 6719\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 18: -183.0064159132241\n",
      "[1, 5, 6, 9, 3]\n",
      "Steps done: 6720\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 2: -147.4441582121856\n",
      "[1, 5, 6, 9, 3, 2]\n",
      "Steps done: 6721\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 4: -88.24099748801498\n",
      "[1, 5, 6, 9, 3, 2, 4]\n",
      "Steps done: 6722\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Reward for action 8: -61.04320519396386\n",
      "[1, 5, 6, 9, 3, 2, 4, 8]\n",
      "Steps done: 6723\n",
      "SV: [ 0.30651474  0.41167435 -0.9459793 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -57.04320519396386\n",
      "[1, 5, 6, 9, 3, 2, 4, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 874\n",
      "Steps done: 6724\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 5: -1.5342121241380018\n",
      "[0, 1, 5]\n",
      "Steps done: 6725\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 15: -4.868811183376316\n",
      "[0, 1]\n",
      "Steps done: 6726\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 3: -132.060821680179\n",
      "[0, 1, 3]\n",
      "Steps done: 6727\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 7: -5.242137764356284\n",
      "[0, 1, 3, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6728\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 10: -19.267742982497396\n",
      "[1, 3, 7]\n",
      "Steps done: 6729\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 6: -5.196522376940829\n",
      "[1, 3, 7, 6]\n",
      "Steps done: 6730\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 11: -30.096206131945646\n",
      "[3, 7, 6]\n",
      "Steps done: 6731\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 16: -192.91790507248243\n",
      "[3, 7]\n",
      "Steps done: 6732\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 6: -30.096206131945646\n",
      "[3, 7, 6]\n",
      "Steps done: 6733\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 17: -159.70965633877347\n",
      "[3, 6]\n",
      "Steps done: 6734\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 7: -30.096206131945646\n",
      "[3, 6, 7]\n",
      "Steps done: 6735\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 16: -192.91790507248243\n",
      "[3, 7]\n",
      "Steps done: 6736\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 6: -30.096206131945646\n",
      "[3, 7, 6]\n",
      "Steps done: 6737\n",
      "SV: [-0.09221026  0.04167697  0.06594027]\n",
      "Reward for action 20: -26.096206131945646\n",
      "[3, 7, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 875\n",
      "Steps done: 6738\n",
      "SV: [0.26539525 0.30629024 1.5875102 ]\n",
      "Reward for action 5: -330.2245473654969\n",
      "[0, 1, 5]\n",
      "Steps done: 6739\n",
      "SV: [0.26539525 0.30629024 1.5875102 ]\n",
      "Reward for action 3: -352.795316481084\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 6740\n",
      "SV: [0.26539525 0.30629024 1.5875102 ]\n",
      "Reward for action 20: -348.795316481084\n",
      "[0, 1, 5, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 876\n",
      "Steps done: 6741\n",
      "SV: [ 0.89486235 -0.3865115  -0.20042813]\n",
      "Reward for action 20: -76.45350992597011\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 877\n",
      "Steps done: 6742\n",
      "SV: [-0.10936236  0.2990077   0.7335401 ]\n",
      "Reward for action 20: 3.3212154255264146\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 878\n",
      "Steps done: 6743\n",
      "SV: [-0.08396129  0.09770935  0.8661559 ]\n",
      "Reward for action 20: -190.85637082579177\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 879\n",
      "Steps done: 6744\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 3: -105.93032789508442\n",
      "[0, 1, 3]\n",
      "Steps done: 6745\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 2: -120.04666027906362\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 6746\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 9: -92.45518330255513\n",
      "[0, 1, 3, 2, 9]\n",
      "Steps done: 6747\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 8: -78.99484824927526\n",
      "[0, 1, 3, 2, 9, 8]\n",
      "Steps done: 6748\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 7: -81.61143738068725\n",
      "[0, 1, 3, 2, 9, 8, 7]\n",
      "Steps done: 6749\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 18: -89.73337388533612\n",
      "[0, 1, 3, 2, 9, 7]\n",
      "Steps done: 6750\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 11: -91.60466641330167\n",
      "[0, 3, 2, 9, 7]\n",
      "Steps done: 6751\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 17: -88.49451313346546\n",
      "[0, 3, 2, 9]\n",
      "Steps done: 6752\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 5: -70.15676589318556\n",
      "[0, 3, 2, 9, 5]\n",
      "Steps done: 6753\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 13: -68.78156989179298\n",
      "[0, 2, 9, 5]\n",
      "Steps done: 6754\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 19: -82.2852124903301\n",
      "[0, 2, 5]\n",
      "Steps done: 6755\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 9: -68.78156989179297\n",
      "[0, 2, 5, 9]\n",
      "Steps done: 6756\n",
      "SV: [-0.13754377  0.13105808 -0.6130326 ]\n",
      "Reward for action 20: -64.78156989179297\n",
      "[0, 2, 5, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 880\n",
      "Steps done: 6757\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 2: -110.6283226351728\n",
      "[0, 1, 2]\n",
      "Steps done: 6758\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 12: -812.8349116420447\n",
      "[0, 1]\n",
      "Steps done: 6759\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 2: -110.6283226351728\n",
      "[0, 1, 2]\n",
      "Steps done: 6760\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 3: -70.04071468807129\n",
      "[0, 1, 2, 3]\n",
      "Steps done: 6761\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 13: -110.6283226351728\n",
      "[0, 1, 2]\n",
      "Steps done: 6762\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 10: -137.5048708040316\n",
      "[1, 2]\n",
      "Steps done: 6763\n",
      "SV: [-0.22639036 -0.89606297 -0.72872305]\n",
      "Reward for action 20: -133.5048708040316\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 881\n",
      "Steps done: 6764\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 3: -33.40661800190856\n",
      "[0, 1, 3]\n",
      "Steps done: 6765\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 7: -27.255075205290296\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 6766\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 6: -23.575236368060782\n",
      "[0, 1, 3, 7, 6]\n",
      "Steps done: 6767\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 13: -24.0084234218673\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 6768\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 10: -15.195851342054425\n",
      "[1, 7, 6]\n",
      "Steps done: 6769\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 3: -18.569842522771534\n",
      "[1, 7, 6, 3]\n",
      "Steps done: 6770\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 13: -15.195851342054425\n",
      "[1, 7, 6]\n",
      "Steps done: 6771\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 17: -21.429418887065523\n",
      "[1, 6]\n",
      "Steps done: 6772\n",
      "SV: [-0.13276158  0.01071344 -0.45682994]\n",
      "Reward for action 20: -17.429418887065523\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 882\n",
      "Steps done: 6773\n",
      "SV: [ 0.3246174  -0.15093422  1.369923  ]\n",
      "Reward for action 20: -485.42187317315694\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 883\n",
      "Steps done: 6774\n",
      "SV: [ 0.12273052 -0.46026203 -0.70741904]\n",
      "Reward for action 20: 0.09960249173296631\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 884\n",
      "Steps done: 6775\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -201.07018398870804\n",
      "[0, 1, 6]\n",
      "Steps done: 6776\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 11: -832.0702912732633\n",
      "[0, 6]\n",
      "Steps done: 6777\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 4: -529.3327368118004\n",
      "[0, 6, 4]\n",
      "Steps done: 6778\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -124.81736611111435\n",
      "[0, 4]\n",
      "Steps done: 6779\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 5: -61.35587905339439\n",
      "[0, 4, 5]\n",
      "Steps done: 6780\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -328.0815241730309\n",
      "[0, 4, 5, 6]\n",
      "Steps done: 6781\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 10: -563.6149563729441\n",
      "[4, 5, 6]\n",
      "Steps done: 6782\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -203.2175532312469\n",
      "[4, 5]\n",
      "Steps done: 6783\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -563.6149563729441\n",
      "[4, 5, 6]\n",
      "Steps done: 6784\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 7: -259.48694123884445\n",
      "[4, 5, 6, 7]\n",
      "Steps done: 6785\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -15.584627458689754\n",
      "[4, 5, 7]\n",
      "Steps done: 6786\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 14: -125.73593712305421\n",
      "[5, 7]\n",
      "Steps done: 6787\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 4: -15.584627458689752\n",
      "[5, 7, 4]\n",
      "Steps done: 6788\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 14: -125.73593712305421\n",
      "[5, 7]\n",
      "Steps done: 6789\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -509.8221830087338\n",
      "[5, 7, 6]\n",
      "Steps done: 6790\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -125.73593712305421\n",
      "[5, 7]\n",
      "Steps done: 6791\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -509.8221830087338\n",
      "[5, 7, 6]\n",
      "Steps done: 6792\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -125.73593712305421\n",
      "[5, 7]\n",
      "Steps done: 6793\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -509.8221830087338\n",
      "[5, 7, 6]\n",
      "Steps done: 6794\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -125.73593712305421\n",
      "[5, 7]\n",
      "Steps done: 6795\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 6: -509.8221830087338\n",
      "[5, 7, 6]\n",
      "Steps done: 6796\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 3: -316.11590417185346\n",
      "[5, 7, 6, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6797\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 0: -194.5635681935023\n",
      "[5, 7, 6, 3, 0]\n",
      "Steps done: 6798\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 16: -21.216647989722453\n",
      "[5, 7, 3, 0]\n",
      "Steps done: 6799\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 1: -4.652961560665753\n",
      "[5, 7, 3, 0, 1]\n",
      "Did target update\n",
      "Steps done: 6800\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Reward for action 15: -29.45103963462713\n",
      "[7, 3, 0, 1]\n",
      "Steps done: 6801\n",
      "SV: [ 0.45121247  0.35882658 -1.2207445 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -25.45103963462713\n",
      "[7, 3, 0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 885\n",
      "Steps done: 6802\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 5: -1.9351538907172738\n",
      "[0, 1, 5]\n",
      "Steps done: 6803\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 11: -0.5064233453787577\n",
      "[0, 5]\n",
      "Steps done: 6804\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 6: -0.4839783980605071\n",
      "[0, 5, 6]\n",
      "Steps done: 6805\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 15: -0.29504933690436946\n",
      "[0, 6]\n",
      "Steps done: 6806\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 4: -88.21376804428508\n",
      "[0, 6, 4]\n",
      "Steps done: 6807\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 5: -26.434715844472436\n",
      "[0, 6, 4, 5]\n",
      "Steps done: 6808\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 14: -0.4839783980605071\n",
      "[0, 6, 5]\n",
      "Steps done: 6809\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 15: -0.29504933690436946\n",
      "[0, 6]\n",
      "Steps done: 6810\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 5: -0.4839783980605071\n",
      "[0, 6, 5]\n",
      "Steps done: 6811\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 10: -1.1324662013488584\n",
      "[6, 5]\n",
      "Steps done: 6812\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 2: -2.554983919204742\n",
      "[6, 5, 2]\n",
      "Steps done: 6813\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 3: -28.042686064682023\n",
      "[6, 5, 2, 3]\n",
      "Steps done: 6814\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 12: -7.060790729434904\n",
      "[6, 5, 3]\n",
      "Steps done: 6815\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 4: -23.675325064725087\n",
      "[6, 5, 3, 4]\n",
      "Steps done: 6816\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 15: -56.19359586879787\n",
      "[6, 3, 4]\n",
      "Steps done: 6817\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 1: -5.396226169636272\n",
      "[6, 3, 4, 1]\n",
      "Steps done: 6818\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 14: -1.6758531022580139\n",
      "[6, 3, 1]\n",
      "Steps done: 6819\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 4: -5.39622616963627\n",
      "[6, 3, 1, 4]\n",
      "Steps done: 6820\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 13: -12.465572380857157\n",
      "[6, 1, 4]\n",
      "Steps done: 6821\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 3: -5.396226169636268\n",
      "[6, 1, 4, 3]\n",
      "Steps done: 6822\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 11: -56.19359586879787\n",
      "[6, 4, 3]\n",
      "Steps done: 6823\n",
      "SV: [ 0.13376103  0.11357852 -0.26320198]\n",
      "Reward for action 20: -52.19359586879787\n",
      "[6, 4, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 886\n",
      "Steps done: 6824\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 2: -13.57577119046189\n",
      "[0, 1, 2]\n",
      "Steps done: 6825\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 10: -30.988171097881963\n",
      "[1, 2]\n",
      "Steps done: 6826\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 4: -6.6747121570132\n",
      "[1, 2, 4]\n",
      "Steps done: 6827\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 14: -30.988171097881963\n",
      "[1, 2]\n",
      "Steps done: 6828\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 4: -6.6747121570132\n",
      "[1, 2, 4]\n",
      "Steps done: 6829\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 14: -30.988171097881963\n",
      "[1, 2]\n",
      "Steps done: 6830\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 4: -6.6747121570132\n",
      "[1, 2, 4]\n",
      "Steps done: 6831\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 0: -10.28123165296735\n",
      "[1, 2, 4, 0]\n",
      "Steps done: 6832\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 12: -53.01551678626622\n",
      "[1, 4, 0]\n",
      "Steps done: 6833\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 2: -10.28123165296735\n",
      "[1, 4, 0, 2]\n",
      "Steps done: 6834\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 10: -6.674712157013199\n",
      "[1, 4, 2]\n",
      "Steps done: 6835\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 11: -5.465763307052827\n",
      "[4, 2]\n",
      "Steps done: 6836\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 0: -1.9537565104205323\n",
      "[4, 2, 0]\n",
      "Steps done: 6837\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 3: -5.234085957662758\n",
      "[4, 2, 0, 3]\n",
      "Steps done: 6838\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 10: -15.595671411516692\n",
      "[4, 2, 3]\n",
      "Steps done: 6839\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 1: -2.19803197517846\n",
      "[4, 2, 3, 1]\n",
      "Steps done: 6840\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 5: -0.52746944063084\n",
      "[4, 2, 3, 1, 5]\n",
      "Steps done: 6841\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 12: -0.9706058087843775\n",
      "[4, 3, 1, 5]\n",
      "Steps done: 6842\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 11: -22.212977326376482\n",
      "[4, 3, 5]\n",
      "Steps done: 6843\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 2: -4.7985921201799915\n",
      "[4, 3, 5, 2]\n",
      "Steps done: 6844\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 13: -0.9664220960932126\n",
      "[4, 5, 2]\n",
      "Steps done: 6845\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 14: -1.6489208687155945\n",
      "[5, 2]\n",
      "Steps done: 6846\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 3: -0.31642885944541127\n",
      "[5, 2, 3]\n",
      "Steps done: 6847\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 1: -0.22451502968204987\n",
      "[5, 2, 3, 1]\n",
      "Steps done: 6848\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 13: -22.9587155561138\n",
      "[5, 2, 1]\n",
      "Steps done: 6849\n",
      "SV: [ 0.09007575 -0.02383473 -0.15815285]\n",
      "Reward for action 20: -18.9587155561138\n",
      "[5, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 887\n",
      "Steps done: 6850\n",
      "SV: [ 0.49000522 -0.69879484 -0.5605877 ]\n",
      "Reward for action 20: -75.76423046725444\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 888\n",
      "Steps done: 6851\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 5: -76.50639701218009\n",
      "[0, 1, 5]\n",
      "Steps done: 6852\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 11: -127.04078118190021\n",
      "[0, 5]\n",
      "Steps done: 6853\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 7: -76.83081092303503\n",
      "[0, 5, 7]\n",
      "Steps done: 6854\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 1: -58.2554044717376\n",
      "[0, 5, 7, 1]\n",
      "Steps done: 6855\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 11: -76.83081092303503\n",
      "[0, 5, 7]\n",
      "Steps done: 6856\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 6: -45.93896742238911\n",
      "[0, 5, 7, 6]\n",
      "Steps done: 6857\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 10: -52.91450237534066\n",
      "[5, 7, 6]\n",
      "Steps done: 6858\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 3: -47.935598576188234\n",
      "[5, 7, 6, 3]\n",
      "Steps done: 6859\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 15: -59.048016550555154\n",
      "[7, 6, 3]\n",
      "Steps done: 6860\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 1: -59.191475168310575\n",
      "[7, 6, 3, 1]\n",
      "Steps done: 6861\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 17: -71.40362058191685\n",
      "[6, 3, 1]\n",
      "Steps done: 6862\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 8: -53.9491457245829\n",
      "[6, 3, 1, 8]\n",
      "Steps done: 6863\n",
      "SV: [-0.13362513 -0.1666991   0.6727528 ]\n",
      "Reward for action 20: -49.9491457245829\n",
      "[6, 3, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 889\n",
      "Steps done: 6864\n",
      "SV: [ 3.3012924  -0.18402559 -1.3262063 ]\n",
      "Reward for action 20: -74.57798409341751\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 891\n",
      "Steps done: 6865\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 8: -362.2357523113739\n",
      "[0, 1, 8]\n",
      "Steps done: 6866\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 6: -133.20983432663306\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 6867\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 7: -114.12200967748187\n",
      "[0, 1, 8, 6, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6868\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 2: -77.18590515149293\n",
      "[0, 1, 8, 6, 7, 2]\n",
      "Steps done: 6869\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 16: -107.13635531678977\n",
      "[0, 1, 8, 7, 2]\n",
      "Steps done: 6870\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 17: -143.8872332188916\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 6871\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 6: -78.08906241638884\n",
      "[0, 1, 8, 2, 6]\n",
      "Steps done: 6872\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 12: -133.20983432663306\n",
      "[0, 1, 8, 6]\n",
      "Steps done: 6873\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 16: -362.2357523113739\n",
      "[0, 1, 8]\n",
      "Steps done: 6874\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 18: -1011.7199939277019\n",
      "[0, 1]\n",
      "Steps done: 6875\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 2: -205.2726081960874\n",
      "[0, 1, 2]\n",
      "Steps done: 6876\n",
      "SV: [-0.01500376  0.02036246  0.5069327 ]\n",
      "Reward for action 20: -201.2726081960874\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 892\n",
      "Steps done: 6877\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 2: -13.861229527289847\n",
      "[0, 1, 2]\n",
      "Steps done: 6878\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 4: -36.69124453581338\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 6879\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 6: -0.5569936603801435\n",
      "[0, 1, 2, 4, 6]\n",
      "Steps done: 6880\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 10: -1.291677066313981\n",
      "[1, 2, 4, 6]\n",
      "Steps done: 6881\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 12: -33.04428641352693\n",
      "[1, 4, 6]\n",
      "Steps done: 6882\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 0: -3.9008683133732265\n",
      "[1, 4, 6, 0]\n",
      "Steps done: 6883\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 16: -17.191023002118268\n",
      "[1, 4, 0]\n",
      "Steps done: 6884\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 11: -444.0979926600932\n",
      "[4, 0]\n",
      "Steps done: 6885\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 1: -17.191023002118268\n",
      "[4, 0, 1]\n",
      "Steps done: 6886\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 11: -444.0979926600932\n",
      "[4, 0]\n",
      "Steps done: 6887\n",
      "SV: [ 0.04089608  0.26191998 -0.65584797]\n",
      "Reward for action 20: -440.0979926600932\n",
      "[4, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 893\n",
      "Steps done: 6888\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 4: -18.240114805231542\n",
      "[0, 1, 4]\n",
      "Steps done: 6889\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 5: -0.38259249163904874\n",
      "[0, 1, 4, 5]\n",
      "Steps done: 6890\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 10: -8.324740266991551\n",
      "[1, 4, 5]\n",
      "Steps done: 6891\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 6: -0.8144006636715817\n",
      "[1, 4, 5, 6]\n",
      "Steps done: 6892\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 16: -8.324740266991551\n",
      "[1, 4, 5]\n",
      "Steps done: 6893\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 15: -83.68389502251034\n",
      "[1, 4]\n",
      "Steps done: 6894\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 0: -18.24011480523155\n",
      "[1, 4, 0]\n",
      "Steps done: 6895\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 5: -0.3825924916390494\n",
      "[1, 4, 0, 5]\n",
      "Steps done: 6896\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 6: -1.745393020262204\n",
      "[1, 4, 0, 5, 6]\n",
      "Steps done: 6897\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 16: -0.3825924916390494\n",
      "[1, 4, 0, 5]\n",
      "Steps done: 6898\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 6: -1.745393020262204\n",
      "[1, 4, 0, 5, 6]\n",
      "Steps done: 6899\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 15: -11.470838806363759\n",
      "[1, 4, 0, 6]\n",
      "Did target update\n",
      "Steps done: 6900\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 11: -13.349635875047229\n",
      "[4, 0, 6]\n",
      "Steps done: 6901\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 3: -8.627143325617157\n",
      "[4, 0, 6, 3]\n",
      "Steps done: 6902\n",
      "SV: [ 0.053549   -0.30818745  0.30065593]\n",
      "Reward for action 20: -4.627143325617157\n",
      "[4, 0, 6, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 894\n",
      "Steps done: 6903\n",
      "SV: [0.69646436 1.223654   1.1780472 ]\n",
      "Reward for action 20: -1051.9982466029226\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 895\n",
      "Steps done: 6904\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 9: -463.65013700574235\n",
      "[0, 1, 9]\n",
      "Steps done: 6905\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 11: -591.939216317532\n",
      "[0, 9]\n",
      "Steps done: 6906\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 5: -657.2211131997817\n",
      "[0, 9, 5]\n",
      "Steps done: 6907\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 19: -798.9873266140278\n",
      "[0, 5]\n",
      "Steps done: 6908\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 3: -9.456445567665295\n",
      "[0, 5, 3]\n",
      "Steps done: 6909\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 7: -204.59984456638676\n",
      "[0, 5, 3, 7]\n",
      "Steps done: 6910\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 8: -207.5766521885463\n",
      "[0, 5, 3, 7, 8]\n",
      "Steps done: 6911\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 13: -714.751017661889\n",
      "[0, 5, 7, 8]\n",
      "Steps done: 6912\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 10: -645.3656788452395\n",
      "[5, 7, 8]\n",
      "Steps done: 6913\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 2: -14.149315406866943\n",
      "[5, 7, 8, 2]\n",
      "Steps done: 6914\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 0: -172.76328683135623\n",
      "[5, 7, 8, 2, 0]\n",
      "Steps done: 6915\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 18: -177.4904309292549\n",
      "[5, 7, 2, 0]\n",
      "Steps done: 6916\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 8: -172.76328683135623\n",
      "[5, 7, 2, 0, 8]\n",
      "Steps done: 6917\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 3: -33.667933661832635\n",
      "[5, 7, 2, 0, 8, 3]\n",
      "Steps done: 6918\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 13: -172.76328683135623\n",
      "[5, 7, 2, 0, 8]\n",
      "Steps done: 6919\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 9: -284.09181171367305\n",
      "[5, 7, 2, 0, 8, 9]\n",
      "Steps done: 6920\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 1: -357.9140390714871\n",
      "[5, 7, 2, 0, 8, 9, 1]\n",
      "Steps done: 6921\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 3: -151.03415123558074\n",
      "[5, 7, 2, 0, 8, 9, 1, 3]\n",
      "Steps done: 6922\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 13: -357.9140390714871\n",
      "[5, 7, 2, 0, 8, 9, 1]\n",
      "Steps done: 6923\n",
      "SV: [-1.0758775   0.47946125  2.9149797 ]\n",
      "Reward for action 20: -353.9140390714871\n",
      "[5, 7, 2, 0, 8, 9, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 896\n",
      "Steps done: 6924\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 4: -36.97381312544046\n",
      "[0, 1, 4]\n",
      "Steps done: 6925\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 10: -45.53100289451209\n",
      "[1, 4]\n",
      "Steps done: 6926\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 3: -3.3289079965350394\n",
      "[1, 4, 3]\n",
      "Steps done: 6927\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 13: -45.53100289451209\n",
      "[1, 4]\n",
      "Steps done: 6928\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 0: -36.973813125440444\n",
      "[1, 4, 0]\n",
      "Steps done: 6929\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 2: -37.71446508335765\n",
      "[1, 4, 0, 2]\n",
      "Steps done: 6930\n",
      "SV: [-0.17731035  0.4641547   0.55995375]\n",
      "Reward for action 20: -33.71446508335765\n",
      "[1, 4, 0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 897\n",
      "Steps done: 6931\n",
      "SV: [ 0.37296614  0.39075264 -0.71006614]\n",
      "Reward for action 20: -38.664600816318554\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 898\n",
      "Steps done: 6932\n",
      "SV: [-0.39086506  0.20310941 -0.06594756]\n",
      "Reward for action 2: -96.10020397839364\n",
      "[0, 1, 2]\n",
      "Steps done: 6933\n",
      "SV: [-0.39086506  0.20310941 -0.06594756]\n",
      "Reward for action 10: -77.45024674856161\n",
      "[1, 2]\n",
      "Steps done: 6934\n",
      "SV: [-0.39086506  0.20310941 -0.06594756]\n",
      "Reward for action 0: -96.10020397839364\n",
      "[1, 2, 0]\n",
      "Steps done: 6935\n",
      "SV: [-0.39086506  0.20310941 -0.06594756]\n",
      "Reward for action 20: -92.10020397839364\n",
      "[1, 2, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 899\n",
      "Steps done: 6936\n",
      "SV: [0.37564442 1.0413922  3.414175  ]\n",
      "Reward for action 20: -0.4061660249462964\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 6937\n",
      "SV: [ 0.12389203 -0.04987592  0.46595508]\n",
      "Reward for action 20: -11.679385095622733\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 901\n",
      "Steps done: 6938\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 4: -44.22238099020623\n",
      "[0, 1, 4]\n",
      "Steps done: 6939\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 14: -13.677475587457133\n",
      "[0, 1]\n",
      "Steps done: 6940\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 3: -9.971008635382727\n",
      "[0, 1, 3]\n",
      "Steps done: 6941\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 4: -21.703207433221138\n",
      "[0, 1, 3, 4]\n",
      "Steps done: 6942\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 14: -9.971008635382727\n",
      "[0, 1, 3]\n",
      "Steps done: 6943\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 10: -7.857307357197058\n",
      "[1, 3]\n",
      "Steps done: 6944\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 4: -18.959943310697238\n",
      "[1, 3, 4]\n",
      "Steps done: 6945\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 5: -9.43584444182589\n",
      "[1, 3, 4, 5]\n",
      "Steps done: 6946\n",
      "SV: [-0.1414222  -0.06702787 -0.25997478]\n",
      "Reward for action 20: -5.435844441825891\n",
      "[1, 3, 4, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 902\n",
      "Steps done: 6947\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 3: -18.890526670380414\n",
      "[0, 1, 3]\n",
      "Steps done: 6948\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 11: -22.339626870002917\n",
      "[0, 3]\n",
      "Steps done: 6949\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 2: -94.70502926146877\n",
      "[0, 3, 2]\n",
      "Steps done: 6950\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 4: -58.29095176483089\n",
      "[0, 3, 2, 4]\n",
      "Steps done: 6951\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 12: -23.34761229028864\n",
      "[0, 3, 4]\n",
      "Steps done: 6952\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 1: -20.070061578570083\n",
      "[0, 3, 4, 1]\n",
      "Steps done: 6953\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 11: -23.34761229028864\n",
      "[0, 3, 4]\n",
      "Steps done: 6954\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 14: -22.339626870002917\n",
      "[0, 3]\n",
      "Steps done: 6955\n",
      "SV: [-0.01684546  0.07007056 -0.5357627 ]\n",
      "Reward for action 20: -18.339626870002917\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 903\n",
      "Steps done: 6956\n",
      "SV: [0.54280347 0.77657676 0.03845575]\n",
      "Reward for action 2: -8.31714336413162\n",
      "[0, 1, 2]\n",
      "Steps done: 6957\n",
      "SV: [0.54280347 0.77657676 0.03845575]\n",
      "Reward for action 20: -4.317143364131621\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 904\n",
      "Steps done: 6958\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 5: -5.848227352534091\n",
      "[0, 1, 5]\n",
      "Steps done: 6959\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 10: -5.250921351369811\n",
      "[1, 5]\n",
      "Steps done: 6960\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 7: -174.74978536039225\n",
      "[1, 5, 7]\n",
      "Steps done: 6961\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 8: -18.940343262585465\n",
      "[1, 5, 7, 8]\n",
      "Steps done: 6962\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 17: -15.20300115380888\n",
      "[1, 5, 8]\n",
      "Steps done: 6963\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 4: -100.59293555335985\n",
      "[1, 5, 8, 4]\n",
      "Steps done: 6964\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 18: -79.66554327677736\n",
      "[1, 5, 4]\n",
      "Steps done: 6965\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 3: -4.821747680712923\n",
      "[1, 5, 4, 3]\n",
      "Steps done: 6966\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 0: -22.255459212702267\n",
      "[1, 5, 4, 3, 0]\n",
      "Steps done: 6967\n",
      "SV: [ 0.5073082  -0.04337624  0.5056302 ]\n",
      "Reward for action 20: -18.255459212702267\n",
      "[1, 5, 4, 3, 0]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 905\n",
      "Steps done: 6968\n",
      "SV: [0.08093765 0.36123684 1.5093836 ]\n",
      "Reward for action 20: -1.4237952141471855\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 906\n",
      "Steps done: 6969\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 6: -19.364010930692054\n",
      "[0, 1, 6]\n",
      "Steps done: 6970\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 8: -23.10766190584487\n",
      "[0, 1, 6, 8]\n",
      "Steps done: 6971\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 4: -20.24264688537579\n",
      "[0, 1, 6, 8, 4]\n",
      "Steps done: 6972\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 18: -17.879639077363905\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 6973\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 9: -12.999070522425278\n",
      "[0, 1, 6, 4, 9]\n",
      "Steps done: 6974\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 3: -5.531153599760799\n",
      "[0, 1, 6, 4, 9, 3]\n",
      "Steps done: 6975\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 13: -12.999070522425278\n",
      "[0, 1, 6, 4, 9]\n",
      "Steps done: 6976\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 10: -16.855699586662652\n",
      "[1, 6, 4, 9]\n",
      "Steps done: 6977\n",
      "SV: [-0.2589444   0.14023395  0.297774  ]\n",
      "Reward for action 20: -12.855699586662652\n",
      "[1, 6, 4, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 907\n",
      "Steps done: 6978\n",
      "SV: [0.2855106 0.5990426 2.5696213]\n",
      "Reward for action 20: -519.3535219872481\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 908\n",
      "Steps done: 6979\n",
      "SV: [-0.02421457  0.05532271 -0.84443325]\n",
      "Reward for action 3: -141.62985316771233\n",
      "[0, 1, 3]\n",
      "Steps done: 6980\n",
      "SV: [-0.02421457  0.05532271 -0.84443325]\n",
      "Reward for action 20: -137.62985316771233\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 909\n",
      "Steps done: 6981\n",
      "SV: [-0.62230533  0.12961954 -0.6982629 ]\n",
      "Reward for action 20: -121.09409846634475\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 910\n",
      "Steps done: 6982\n",
      "SV: [-1.6598412 -0.735948   1.0505726]\n",
      "Reward for action 20: -251.10618365000266\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 911\n",
      "Steps done: 6983\n",
      "SV: [ 0.1978857  -0.28018877  0.03621912]\n",
      "Reward for action 3: -7.149407358089089\n",
      "[0, 1, 3]\n",
      "Steps done: 6984\n",
      "SV: [ 0.1978857  -0.28018877  0.03621912]\n",
      "Reward for action 10: -6.703738583709295\n",
      "[1, 3]\n",
      "Steps done: 6985\n",
      "SV: [ 0.1978857  -0.28018877  0.03621912]\n",
      "Reward for action 20: -2.7037385837092947\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 912\n",
      "Steps done: 6986\n",
      "SV: [-0.43263775 -0.7964146  -0.789034  ]\n",
      "Reward for action 3: -89.97649302236258\n",
      "[0, 1, 3]\n",
      "Steps done: 6987\n",
      "SV: [-0.43263775 -0.7964146  -0.789034  ]\n",
      "Reward for action 13: -99.41880831756535\n",
      "[0, 1]\n",
      "Steps done: 6988\n",
      "SV: [-0.43263775 -0.7964146  -0.789034  ]\n",
      "Reward for action 2: -80.49785666409895\n",
      "[0, 1, 2]\n",
      "Steps done: 6989\n",
      "SV: [-0.43263775 -0.7964146  -0.789034  ]\n",
      "Reward for action 11: -120.77846977661798\n",
      "[0, 2]\n",
      "Steps done: 6990\n",
      "SV: [-0.43263775 -0.7964146  -0.789034  ]\n",
      "Reward for action 1: -80.49785666409895\n",
      "[0, 2, 1]\n",
      "Steps done: 6991\n",
      "SV: [-0.43263775 -0.7964146  -0.789034  ]\n",
      "Reward for action 20: -76.49785666409895\n",
      "[0, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 913\n",
      "Steps done: 6992\n",
      "SV: [ 0.4671277   0.38225207 -0.47504583]\n",
      "Reward for action 6: -22.335004220796474\n",
      "[0, 1, 6]\n",
      "Steps done: 6993\n",
      "SV: [ 0.4671277   0.38225207 -0.47504583]\n",
      "Reward for action 11: -1.18383840444576\n",
      "[0, 6]\n",
      "Steps done: 6994\n",
      "SV: [ 0.4671277   0.38225207 -0.47504583]\n",
      "Reward for action 20: 2.81616159555424\n",
      "[0, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 914\n",
      "Steps done: 6995\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 4: -1.6812628475921818\n",
      "[0, 1, 4]\n",
      "Steps done: 6996\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 3: -1.339613380116844\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 6997\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 10: -1.3880480604813965\n",
      "[1, 4, 3]\n",
      "Steps done: 6998\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 7: -1.3485677996059033\n",
      "[1, 4, 3, 7]\n",
      "Steps done: 6999\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 9: -1.3205684648852904\n",
      "[1, 4, 3, 7, 9]\n",
      "Did target update\n",
      "Steps done: 7000\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 6: -1.2956960356760612\n",
      "[1, 4, 3, 7, 9, 6]\n",
      "Steps done: 7001\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 16: -1.3205684648852904\n",
      "[1, 4, 3, 7, 9]\n",
      "Steps done: 7002\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 8: -1.296342949486084\n",
      "[1, 4, 3, 7, 9, 8]\n",
      "Steps done: 7003\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 14: -1.2846022717140702\n",
      "[1, 3, 7, 9, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7004\n",
      "SV: [-0.04950128 -0.01924641  0.12042432]\n",
      "Reward for action 20: 2.7153977282859296\n",
      "[1, 3, 7, 9, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 915\n",
      "Steps done: 7005\n",
      "SV: [ 0.08397532 -0.47781798 -1.8323103 ]\n",
      "Reward for action 3: -99.89301414470476\n",
      "[0, 1, 3]\n",
      "Steps done: 7006\n",
      "SV: [ 0.08397532 -0.47781798 -1.8323103 ]\n",
      "Reward for action 20: -95.89301414470476\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 916\n",
      "Steps done: 7007\n",
      "SV: [-2.4865448  1.2498348 -7.483178 ]\n",
      "Reward for action 20: -214.61723173067793\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 917\n",
      "Steps done: 7008\n",
      "SV: [-0.5859055   0.00567601 -1.1526935 ]\n",
      "Reward for action 20: -87.79061430567468\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 918\n",
      "Steps done: 7009\n",
      "SV: [ 1.8669951  -0.15181762  8.837151  ]\n",
      "Reward for action 20: -458.02251055692915\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 919\n",
      "Steps done: 7010\n",
      "SV: [ 0.12746432 -0.01967176  0.7515769 ]\n",
      "Reward for action 2: -31.979418785088978\n",
      "[0, 1, 2]\n",
      "Steps done: 7011\n",
      "SV: [ 0.12746432 -0.01967176  0.7515769 ]\n",
      "Reward for action 10: -44.464341895079336\n",
      "[1, 2]\n",
      "Steps done: 7012\n",
      "SV: [ 0.12746432 -0.01967176  0.7515769 ]\n",
      "Reward for action 7: -48.34041246491112\n",
      "[1, 2, 7]\n",
      "Steps done: 7013\n",
      "SV: [ 0.12746432 -0.01967176  0.7515769 ]\n",
      "Reward for action 5: -48.90117614158007\n",
      "[1, 2, 7, 5]\n",
      "Steps done: 7014\n",
      "SV: [ 0.12746432 -0.01967176  0.7515769 ]\n",
      "Reward for action 20: -44.90117614158007\n",
      "[1, 2, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 920\n",
      "Steps done: 7015\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 8: -6.676319221246447\n",
      "[0, 1, 8]\n",
      "Steps done: 7016\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 11: -19.869536179569252\n",
      "[0, 8]\n",
      "Steps done: 7017\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 6: -2.4588602459404902\n",
      "[0, 8, 6]\n",
      "Steps done: 7018\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 7: -15.184419891395128\n",
      "[0, 8, 6, 7]\n",
      "Steps done: 7019\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 5: -12.385895462631964\n",
      "[0, 8, 6, 7, 5]\n",
      "Steps done: 7020\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 18: -40.64916820408676\n",
      "[0, 6, 7, 5]\n",
      "Steps done: 7021\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 4: -35.78460651868222\n",
      "[0, 6, 7, 5, 4]\n",
      "Steps done: 7022\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 17: -67.10860361638338\n",
      "[0, 6, 5, 4]\n",
      "Steps done: 7023\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 1: -30.427839024922164\n",
      "[0, 6, 5, 4, 1]\n",
      "Steps done: 7024\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 8: -11.449158492927573\n",
      "[0, 6, 5, 4, 1, 8]\n",
      "Steps done: 7025\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 16: -10.957478388870058\n",
      "[0, 5, 4, 1, 8]\n",
      "Steps done: 7026\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 15: -10.92423773743813\n",
      "[0, 4, 1, 8]\n",
      "Steps done: 7027\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 11: -17.269991544291486\n",
      "[0, 4, 8]\n",
      "Steps done: 7028\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 5: -13.070917791836775\n",
      "[0, 4, 8, 5]\n",
      "Steps done: 7029\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 14: -10.094607117626067\n",
      "[0, 8, 5]\n",
      "Steps done: 7030\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 2: -13.775693899734854\n",
      "[0, 8, 5, 2]\n",
      "Steps done: 7031\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 7: -10.211904783060973\n",
      "[0, 8, 5, 2, 7]\n",
      "Steps done: 7032\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 6: -6.30929872020548\n",
      "[0, 8, 5, 2, 7, 6]\n",
      "Steps done: 7033\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 12: -12.385895462631968\n",
      "[0, 8, 5, 7, 6]\n",
      "Steps done: 7034\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 15: -15.184419891395125\n",
      "[0, 8, 7, 6]\n",
      "Steps done: 7035\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 9: -6.262096078816386\n",
      "[0, 8, 7, 6, 9]\n",
      "Steps done: 7036\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 5: -7.758053858126083\n",
      "[0, 8, 7, 6, 9, 5]\n",
      "Steps done: 7037\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 18: -15.482280927301069\n",
      "[0, 7, 6, 9, 5]\n",
      "Steps done: 7038\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 2: -7.6645077482911175\n",
      "[0, 7, 6, 9, 5, 2]\n",
      "Steps done: 7039\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 12: -15.482280927301069\n",
      "[0, 7, 6, 9, 5]\n",
      "Steps done: 7040\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Reward for action 17: -16.99955440331919\n",
      "[0, 6, 9, 5]\n",
      "Steps done: 7041\n",
      "SV: [-0.03884508 -0.474567    0.10646176]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -12.999554403319191\n",
      "[0, 6, 9, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 921\n",
      "Steps done: 7042\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 8: -54.77591625812732\n",
      "[0, 1, 8]\n",
      "Steps done: 7043\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 3: -22.51077693909998\n",
      "[0, 1, 8, 3]\n",
      "Steps done: 7044\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 2: -15.262930951634479\n",
      "[0, 1, 8, 3, 2]\n",
      "Steps done: 7045\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 9: -15.730371410399355\n",
      "[0, 1, 8, 3, 2, 9]\n",
      "Steps done: 7046\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 18: -30.304801066745796\n",
      "[0, 1, 3, 2, 9]\n",
      "Steps done: 7047\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 11: -51.94287595380724\n",
      "[0, 3, 2, 9]\n",
      "Steps done: 7048\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 8: -26.376033013385932\n",
      "[0, 3, 2, 9, 8]\n",
      "Steps done: 7049\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 18: -51.94287595380724\n",
      "[0, 3, 2, 9]\n",
      "Steps done: 7050\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 4: -21.671630383831918\n",
      "[0, 3, 2, 9, 4]\n",
      "Steps done: 7051\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 12: -18.68389239009379\n",
      "[0, 3, 9, 4]\n",
      "Steps done: 7052\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 13: -14.709614065601603\n",
      "[0, 9, 4]\n",
      "Steps done: 7053\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 19: -14.407788048920878\n",
      "[0, 4]\n",
      "Steps done: 7054\n",
      "SV: [-0.0978178   0.33841145 -0.2353581 ]\n",
      "Reward for action 20: -10.407788048920878\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 922\n",
      "Steps done: 7055\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 6: -31.571869053754437\n",
      "[0, 1, 6]\n",
      "Steps done: 7056\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 11: -22.05859604700482\n",
      "[0, 6]\n",
      "Steps done: 7057\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 7: -24.3798645659887\n",
      "[0, 6, 7]\n",
      "Steps done: 7058\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 5: -21.596584344766484\n",
      "[0, 6, 7, 5]\n",
      "Steps done: 7059\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 8: -19.38974927290046\n",
      "[0, 6, 7, 5, 8]\n",
      "Steps done: 7060\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 10: -32.78128663243253\n",
      "[6, 7, 5, 8]\n",
      "Steps done: 7061\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 18: -23.844997256719825\n",
      "[6, 7, 5]\n",
      "Steps done: 7062\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 3: -31.173424572205143\n",
      "[6, 7, 5, 3]\n",
      "Steps done: 7063\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 4: -28.984061281257137\n",
      "[6, 7, 5, 3, 4]\n",
      "Steps done: 7064\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 16: -35.52582070485443\n",
      "[7, 5, 3, 4]\n",
      "Steps done: 7065\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 9: -34.702157260282576\n",
      "[7, 5, 3, 4, 9]\n",
      "Steps done: 7066\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 6: -29.07150126320555\n",
      "[7, 5, 3, 4, 9, 6]\n",
      "Steps done: 7067\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 16: -34.702157260282576\n",
      "[7, 5, 3, 4, 9]\n",
      "Steps done: 7068\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 6: -29.07150126320555\n",
      "[7, 5, 3, 4, 9, 6]\n",
      "Steps done: 7069\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 17: -23.360436809016022\n",
      "[5, 3, 4, 9, 6]\n",
      "Steps done: 7070\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 0: -21.812858787858346\n",
      "[5, 3, 4, 9, 6, 0]\n",
      "Steps done: 7071\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 15: -23.627943793979682\n",
      "[3, 4, 9, 6, 0]\n",
      "Steps done: 7072\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 8: -25.575057104925076\n",
      "[3, 4, 9, 6, 0, 8]\n",
      "Steps done: 7073\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 1: -21.66872903709134\n",
      "[3, 4, 9, 6, 0, 8, 1]\n",
      "Steps done: 7074\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 18: -23.547384210303537\n",
      "[3, 4, 9, 6, 0, 1]\n",
      "Steps done: 7075\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 8: -21.66872903709133\n",
      "[3, 4, 9, 6, 0, 1, 8]\n",
      "Steps done: 7076\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 11: -25.575057104925076\n",
      "[3, 4, 9, 6, 0, 8]\n",
      "Steps done: 7077\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 16: -25.337486516704548\n",
      "[3, 4, 9, 0, 8]\n",
      "Steps done: 7078\n",
      "SV: [-0.29553977  0.09021762 -0.42544577]\n",
      "Reward for action 20: -21.337486516704548\n",
      "[3, 4, 9, 0, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 923\n",
      "Steps done: 7079\n",
      "SV: [-0.13448016 -0.06555202 -0.08290254]\n",
      "Reward for action 4: -1.4128472742476599\n",
      "[0, 1, 4]\n",
      "Steps done: 7080\n",
      "SV: [-0.13448016 -0.06555202 -0.08290254]\n",
      "Reward for action 14: -3.652185219601894\n",
      "[0, 1]\n",
      "Steps done: 7081\n",
      "SV: [-0.13448016 -0.06555202 -0.08290254]\n",
      "Reward for action 4: -1.4128472742476599\n",
      "[0, 1, 4]\n",
      "Steps done: 7082\n",
      "SV: [-0.13448016 -0.06555202 -0.08290254]\n",
      "Reward for action 20: 2.58715272575234\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 924\n",
      "Steps done: 7083\n",
      "SV: [-0.8591365  -0.23923369 -1.4577271 ]\n",
      "Reward for action 2: -28.879618345679106\n",
      "[0, 1, 2]\n",
      "Steps done: 7084\n",
      "SV: [-0.8591365  -0.23923369 -1.4577271 ]\n",
      "Reward for action 12: -40.6040102026062\n",
      "[0, 1]\n",
      "Steps done: 7085\n",
      "SV: [-0.8591365  -0.23923369 -1.4577271 ]\n",
      "Reward for action 20: -36.6040102026062\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 925\n",
      "Steps done: 7086\n",
      "SV: [-0.14437453 -1.0876602   2.7734542 ]\n",
      "Reward for action 2: -480.39215284529536\n",
      "[0, 1, 2]\n",
      "Steps done: 7087\n",
      "SV: [-0.14437453 -1.0876602   2.7734542 ]\n",
      "Reward for action 10: -1170.0866026230271\n",
      "[1, 2]\n",
      "Steps done: 7088\n",
      "SV: [-0.14437453 -1.0876602   2.7734542 ]\n",
      "Reward for action 20: -1166.0866026230271\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 926\n",
      "Steps done: 7089\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 6: -175.4479634588027\n",
      "[0, 1, 6]\n",
      "Steps done: 7090\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 7: -45.92107441290709\n",
      "[0, 1, 6, 7]\n",
      "Steps done: 7091\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 4: -50.358483683074866\n",
      "[0, 1, 6, 7, 4]\n",
      "Steps done: 7092\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 5: -64.6323764727642\n",
      "[0, 1, 6, 7, 4, 5]\n",
      "Steps done: 7093\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 15: -50.358483683074866\n",
      "[0, 1, 6, 7, 4]\n",
      "Steps done: 7094\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 11: -62.69934414708199\n",
      "[0, 6, 7, 4]\n",
      "Steps done: 7095\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 1: -50.358483683074866\n",
      "[0, 6, 7, 4, 1]\n",
      "Steps done: 7096\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 17: -149.03523240005347\n",
      "[0, 6, 4, 1]\n",
      "Steps done: 7097\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 16: -65.21332419028563\n",
      "[0, 4, 1]\n",
      "Steps done: 7098\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 11: -68.45715418261281\n",
      "[0, 4]\n",
      "Steps done: 7099\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 6: -199.06006585200325\n",
      "[0, 4, 6]\n",
      "Did target update\n",
      "Steps done: 7100\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 16: -68.45715418261281\n",
      "[0, 4]\n",
      "Steps done: 7101\n",
      "SV: [-0.03830555  0.3016611   0.14702298]\n",
      "Reward for action 20: -64.45715418261281\n",
      "[0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 927\n",
      "Steps done: 7102\n",
      "SV: [ 0.02263737  0.01446202 -0.47226247]\n",
      "Reward for action 6: -117.45822562701649\n",
      "[0, 1, 6]\n",
      "Steps done: 7103\n",
      "SV: [ 0.02263737  0.01446202 -0.47226247]\n",
      "Reward for action 10: -213.72608083955885\n",
      "[1, 6]\n",
      "Steps done: 7104\n",
      "SV: [ 0.02263737  0.01446202 -0.47226247]\n",
      "Reward for action 5: -129.6561918004312\n",
      "[1, 6, 5]\n",
      "Steps done: 7105\n",
      "SV: [ 0.02263737  0.01446202 -0.47226247]\n",
      "Reward for action 15: -213.72608083955885\n",
      "[1, 6]\n",
      "Steps done: 7106\n",
      "SV: [ 0.02263737  0.01446202 -0.47226247]\n",
      "Reward for action 20: -209.72608083955885\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 928\n",
      "Steps done: 7107\n",
      "SV: [-0.48849455 -0.5053426  -0.6494784 ]\n",
      "Reward for action 20: -53.635523506877895\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 929\n",
      "Steps done: 7108\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 4: -15.219495806104206\n",
      "[0, 1, 4]\n",
      "Steps done: 7109\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 3: -21.123059497873815\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 7110\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 11: -21.07863031579059\n",
      "[0, 4, 3]\n",
      "Steps done: 7111\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 14: -60.218249723233235\n",
      "[0, 3]\n",
      "Steps done: 7112\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 4: -21.07863031579059\n",
      "[0, 3, 4]\n",
      "Steps done: 7113\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 10: -269.5703962693168\n",
      "[3, 4]\n",
      "Steps done: 7114\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 1: -66.98309021071108\n",
      "[3, 4, 1]\n",
      "Steps done: 7115\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 11: -269.5703962693168\n",
      "[3, 4]\n",
      "Steps done: 7116\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 1: -66.98309021071108\n",
      "[3, 4, 1]\n",
      "Steps done: 7117\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 13: -169.00845437845845\n",
      "[4, 1]\n",
      "Steps done: 7118\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 3: -66.98309021071105\n",
      "[4, 1, 3]\n",
      "Steps done: 7119\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 11: -269.5703962693168\n",
      "[4, 3]\n",
      "Steps done: 7120\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 2: -59.19241260174117\n",
      "[4, 3, 2]\n",
      "Steps done: 7121\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 14: -126.5617043453208\n",
      "[3, 2]\n",
      "Steps done: 7122\n",
      "SV: [-0.02104072  0.04233881  0.47782618]\n",
      "Reward for action 20: -122.5617043453208\n",
      "[3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 930\n",
      "Steps done: 7123\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 2: -150.12514082471188\n",
      "[0, 1, 2]\n",
      "Steps done: 7124\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 10: -159.2585358026662\n",
      "[1, 2]\n",
      "Steps done: 7125\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 0: -150.1251408247119\n",
      "[1, 2, 0]\n",
      "Steps done: 7126\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 10: -159.2585358026662\n",
      "[1, 2]\n",
      "Steps done: 7127\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 0: -150.1251408247119\n",
      "[1, 2, 0]\n",
      "Steps done: 7128\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 4: -133.31660090529266\n",
      "[1, 2, 0, 4]\n",
      "Steps done: 7129\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 12: -238.3892217198337\n",
      "[1, 0, 4]\n",
      "Steps done: 7130\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 14: -236.53895698132803\n",
      "[1, 0]\n",
      "Steps done: 7131\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 4: -238.3892217198337\n",
      "[1, 0, 4]\n",
      "Steps done: 7132\n",
      "SV: [-0.01170479 -0.1390626  -1.7097876 ]\n",
      "Reward for action 20: -234.3892217198337\n",
      "[1, 0, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 931\n",
      "Steps done: 7133\n",
      "SV: [-0.39270887  0.0251823  -1.2674879 ]\n",
      "Reward for action 3: -1.0035766911752404\n",
      "[0, 1, 3]\n",
      "Steps done: 7134\n",
      "SV: [-0.39270887  0.0251823  -1.2674879 ]\n",
      "Reward for action 11: -1.0497895571776825\n",
      "[0, 3]\n",
      "Steps done: 7135\n",
      "SV: [-0.39270887  0.0251823  -1.2674879 ]\n",
      "Reward for action 2: -14.572437270074763\n",
      "[0, 3, 2]\n",
      "Steps done: 7136\n",
      "SV: [-0.39270887  0.0251823  -1.2674879 ]\n",
      "Reward for action 13: -110.36388174602271\n",
      "[0, 2]\n",
      "Steps done: 7137\n",
      "SV: [-0.39270887  0.0251823  -1.2674879 ]\n",
      "Reward for action 1: -47.594012339492444\n",
      "[0, 2, 1]\n",
      "Steps done: 7138\n",
      "SV: [-0.39270887  0.0251823  -1.2674879 ]\n",
      "Reward for action 20: -43.594012339492444\n",
      "[0, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 932\n",
      "Steps done: 7139\n",
      "SV: [1.3293822 0.9724293 4.038538 ]\n",
      "Reward for action 20: -532.4133714684712\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 933\n",
      "Steps done: 7140\n",
      "SV: [-0.01025127  0.00838604 -0.39036533]\n",
      "Reward for action 4: -5.507779720966855\n",
      "[0, 1, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7141\n",
      "SV: [-0.01025127  0.00838604 -0.39036533]\n",
      "Reward for action 9: -5.849406148343894\n",
      "[0, 1, 4, 9]\n",
      "Steps done: 7142\n",
      "SV: [-0.01025127  0.00838604 -0.39036533]\n",
      "Reward for action 20: -1.8494061483438937\n",
      "[0, 1, 4, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 934\n",
      "Steps done: 7143\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 6: -88.28183304544294\n",
      "[0, 1, 6]\n",
      "Steps done: 7144\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 11: -87.62567370369169\n",
      "[0, 6]\n",
      "Steps done: 7145\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 1: -88.28183304544294\n",
      "[0, 6, 1]\n",
      "Steps done: 7146\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 11: -87.62567370369169\n",
      "[0, 6]\n",
      "Steps done: 7147\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 7: -109.42933056071583\n",
      "[0, 6, 7]\n",
      "Steps done: 7148\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 10: -111.96999894204795\n",
      "[6, 7]\n",
      "Steps done: 7149\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 9: -92.1450921870691\n",
      "[6, 7, 9]\n",
      "Steps done: 7150\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 17: -98.61332309884021\n",
      "[6, 9]\n",
      "Steps done: 7151\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 1: -91.04502494429867\n",
      "[6, 9, 1]\n",
      "Steps done: 7152\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 19: -87.45034624314894\n",
      "[6, 1]\n",
      "Steps done: 7153\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 4: -88.16423009646823\n",
      "[6, 1, 4]\n",
      "Steps done: 7154\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 14: -87.45034624314894\n",
      "[6, 1]\n",
      "Steps done: 7155\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 9: -91.04502494429867\n",
      "[6, 1, 9]\n",
      "Steps done: 7156\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 11: -98.61332309884021\n",
      "[6, 9]\n",
      "Steps done: 7157\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 8: -246.9525123753027\n",
      "[6, 9, 8]\n",
      "Steps done: 7158\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 18: -98.61332309884021\n",
      "[6, 9]\n",
      "Steps done: 7159\n",
      "SV: [-0.18227002 -0.13148582  1.0333371 ]\n",
      "Reward for action 20: -94.61332309884021\n",
      "[6, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 935\n",
      "Steps done: 7160\n",
      "SV: [-0.58892655 -0.5712137  -1.0738858 ]\n",
      "Reward for action 3: -69.5176380441959\n",
      "[0, 1, 3]\n",
      "Steps done: 7161\n",
      "SV: [-0.58892655 -0.5712137  -1.0738858 ]\n",
      "Reward for action 9: -98.81263617847443\n",
      "[0, 1, 3, 9]\n",
      "Steps done: 7162\n",
      "SV: [-0.58892655 -0.5712137  -1.0738858 ]\n",
      "Reward for action 20: -94.81263617847443\n",
      "[0, 1, 3, 9]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 936\n",
      "Steps done: 7163\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 4: -8.671604643184299\n",
      "[0, 1, 4]\n",
      "Steps done: 7164\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 8: -8.746655813420574\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 7165\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 7: -22.126422796855522\n",
      "[0, 1, 4, 8, 7]\n",
      "Steps done: 7166\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 11: -26.956472413701302\n",
      "[0, 4, 8, 7]\n",
      "Steps done: 7167\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 1: -22.126422796855525\n",
      "[0, 4, 8, 7, 1]\n",
      "Steps done: 7168\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 11: -26.956472413701302\n",
      "[0, 4, 8, 7]\n",
      "Steps done: 7169\n",
      "SV: [ 0.00921092  0.00817207 -0.32855374]\n",
      "Reward for action 20: -22.956472413701302\n",
      "[0, 4, 8, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 937\n",
      "Steps done: 7170\n",
      "SV: [ 0.43450066  1.056382   -0.8704969 ]\n",
      "Reward for action 2: -172.86852767080097\n",
      "[0, 1, 2]\n",
      "Steps done: 7171\n",
      "SV: [ 0.43450066  1.056382   -0.8704969 ]\n",
      "Reward for action 10: -161.23162557301117\n",
      "[1, 2]\n",
      "Steps done: 7172\n",
      "SV: [ 0.43450066  1.056382   -0.8704969 ]\n",
      "Reward for action 20: -157.23162557301117\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 938\n",
      "Steps done: 7173\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 4: -17.225318025626354\n",
      "[0, 1, 4]\n",
      "Steps done: 7174\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 10: -14.88452820253323\n",
      "[1, 4]\n",
      "Steps done: 7175\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 2: -9.797836895012518\n",
      "[1, 4, 2]\n",
      "Steps done: 7176\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 14: -9.70797477407973\n",
      "[1, 2]\n",
      "Steps done: 7177\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 7: -9.724306160513116\n",
      "[1, 2, 7]\n",
      "Steps done: 7178\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 11: -9.345876432642843\n",
      "[2, 7]\n",
      "Steps done: 7179\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 3: -53.57589888742231\n",
      "[2, 7, 3]\n",
      "Steps done: 7180\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 5: -12.878441385464688\n",
      "[2, 7, 3, 5]\n",
      "Steps done: 7181\n",
      "SV: [-0.17786336 -0.05016889 -0.5268435 ]\n",
      "Reward for action 20: -8.878441385464688\n",
      "[2, 7, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 939\n",
      "Steps done: 7182\n",
      "SV: [-0.05433841  0.9780418   0.376285  ]\n",
      "Reward for action 2: -39.953878518106\n",
      "[0, 1, 2]\n",
      "Steps done: 7183\n",
      "SV: [-0.05433841  0.9780418   0.376285  ]\n",
      "Reward for action 4: -48.06295702460898\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7184\n",
      "SV: [-0.05433841  0.9780418   0.376285  ]\n",
      "Reward for action 20: -44.06295702460898\n",
      "[0, 1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 940\n",
      "Steps done: 7185\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 4: -85.03788722519795\n",
      "[0, 1, 4]\n",
      "Steps done: 7186\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 3: -45.473688728537454\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 7187\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 5: -92.20628246472394\n",
      "[0, 1, 4, 3, 5]\n",
      "Steps done: 7188\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 2: -51.66614278655952\n",
      "[0, 1, 4, 3, 5, 2]\n",
      "Steps done: 7189\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 14: -3.4547764121932114\n",
      "[0, 1, 3, 5, 2]\n",
      "Steps done: 7190\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 11: -14.684696513814078\n",
      "[0, 3, 5, 2]\n",
      "Steps done: 7191\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 15: -115.68924704890371\n",
      "[0, 3, 2]\n",
      "Steps done: 7192\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 5: -14.684696513814078\n",
      "[0, 3, 2, 5]\n",
      "Steps done: 7193\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 13: -33.440376750426985\n",
      "[0, 2, 5]\n",
      "Steps done: 7194\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 15: -355.5995041236086\n",
      "[0, 2]\n",
      "Steps done: 7195\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 5: -33.440376750426985\n",
      "[0, 2, 5]\n",
      "Steps done: 7196\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 15: -355.5995041236086\n",
      "[0, 2]\n",
      "Steps done: 7197\n",
      "SV: [ 0.29960915 -0.11617309 -0.7526452 ]\n",
      "Reward for action 20: -351.5995041236086\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 941\n",
      "Steps done: 7198\n",
      "SV: [-0.0784392   0.00133914  0.966014  ]\n",
      "Reward for action 8: -61.56598737641535\n",
      "[0, 1, 8]\n",
      "Steps done: 7199\n",
      "SV: [-0.0784392   0.00133914  0.966014  ]\n",
      "Reward for action 18: -60.777865316429015\n",
      "[0, 1]\n",
      "Did target update\n",
      "Steps done: 7200\n",
      "SV: [-0.0784392   0.00133914  0.966014  ]\n",
      "Reward for action 20: -56.777865316429015\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 942\n",
      "Steps done: 7201\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 2: -25.772659662238695\n",
      "[0, 1, 2]\n",
      "Steps done: 7202\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 12: -45.53621322809542\n",
      "[0, 1]\n",
      "Steps done: 7203\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 3: -87.70500324360185\n",
      "[0, 1, 3]\n",
      "Steps done: 7204\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 13: -45.53621322809542\n",
      "[0, 1]\n",
      "Steps done: 7205\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 2: -25.772659662238695\n",
      "[0, 1, 2]\n",
      "Steps done: 7206\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 10: -55.782061829389995\n",
      "[1, 2]\n",
      "Steps done: 7207\n",
      "SV: [-1.1870629   0.62169915 -1.7339811 ]\n",
      "Reward for action 20: -51.782061829389995\n",
      "[1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 943\n",
      "Steps done: 7208\n",
      "SV: [ 0.23117478 -0.03742113 -0.7862059 ]\n",
      "Reward for action 20: -38.463296317058834\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 944\n",
      "Steps done: 7209\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 5: -1.2851126544082656\n",
      "[0, 1, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7210\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 4: -2.6416457463425678\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 7211\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 11: -1.2461272034207078\n",
      "[0, 5, 4]\n",
      "Steps done: 7212\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 7: -4.342327789986012\n",
      "[0, 5, 4, 7]\n",
      "Steps done: 7213\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 6: -3.0883700001439944\n",
      "[0, 5, 4, 7, 6]\n",
      "Steps done: 7214\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 8: -1.49737684328718\n",
      "[0, 5, 4, 7, 6, 8]\n",
      "Steps done: 7215\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 16: -1.423403379440583\n",
      "[0, 5, 4, 7, 8]\n",
      "Steps done: 7216\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 10: -1.7013382323572603\n",
      "[5, 4, 7, 8]\n",
      "Steps done: 7217\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 18: -12.024581872489847\n",
      "[5, 4, 7]\n",
      "Steps done: 7218\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 1: -7.616725263628282\n",
      "[5, 4, 7, 1]\n",
      "Steps done: 7219\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 9: -2.7160852250523604\n",
      "[5, 4, 7, 1, 9]\n",
      "Steps done: 7220\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 11: -1.7260426319125852\n",
      "[5, 4, 7, 9]\n",
      "Steps done: 7221\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 19: -12.024581872489847\n",
      "[5, 4, 7]\n",
      "Steps done: 7222\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 3: -2.6585390780476104\n",
      "[5, 4, 7, 3]\n",
      "Steps done: 7223\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 15: -9.398753281094475\n",
      "[4, 7, 3]\n",
      "Steps done: 7224\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 13: -41.88398699121052\n",
      "[4, 7]\n",
      "Steps done: 7225\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 9: -3.621741151694242\n",
      "[4, 7, 9]\n",
      "Steps done: 7226\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 17: -2.457694045712063\n",
      "[4, 9]\n",
      "Steps done: 7227\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 3: -2.953525720016771\n",
      "[4, 9, 3]\n",
      "Steps done: 7228\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 8: -5.073715553379986\n",
      "[4, 9, 3, 8]\n",
      "Steps done: 7229\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 7: -4.9251664421674715\n",
      "[4, 9, 3, 8, 7]\n",
      "Steps done: 7230\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 5: -2.599414473537609\n",
      "[4, 9, 3, 8, 7, 5]\n",
      "Steps done: 7231\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 13: -4.018237056959093\n",
      "[4, 9, 8, 7, 5]\n",
      "Steps done: 7232\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 19: -1.7013382323572608\n",
      "[4, 8, 7, 5]\n",
      "Steps done: 7233\n",
      "SV: [0.17664258 0.0338435  0.15768072]\n",
      "Reward for action 20: 2.298661767642739\n",
      "[4, 8, 7, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 945\n",
      "Steps done: 7234\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 8: -73.469245252185\n",
      "[0, 1, 8]\n",
      "Steps done: 7235\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 4: -128.93587279647278\n",
      "[0, 1, 8, 4]\n",
      "Steps done: 7236\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 18: -285.2468641407614\n",
      "[0, 1, 4]\n",
      "Steps done: 7237\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 3: -59.384016990869306\n",
      "[0, 1, 4, 3]\n",
      "Steps done: 7238\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 10: -16.193918957211903\n",
      "[1, 4, 3]\n",
      "Steps done: 7239\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 14: -179.46185444431336\n",
      "[1, 3]\n",
      "Steps done: 7240\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 2: -42.736817781007204\n",
      "[1, 3, 2]\n",
      "Steps done: 7241\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 0: -0.9664036115124119\n",
      "[1, 3, 2, 0]\n",
      "Steps done: 7242\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 13: -27.116296011899344\n",
      "[1, 2, 0]\n",
      "Steps done: 7243\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 12: -390.55495465217757\n",
      "[1, 0]\n",
      "Steps done: 7244\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 8: -73.46924525218503\n",
      "[1, 0, 8]\n",
      "Steps done: 7245\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 10: -33.75042113693205\n",
      "[1, 8]\n",
      "Steps done: 7246\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 2: -11.585122972067817\n",
      "[1, 8, 2]\n",
      "Steps done: 7247\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 4: -11.954019605409115\n",
      "[1, 8, 2, 4]\n",
      "Steps done: 7248\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 12: -71.26573409872232\n",
      "[1, 8, 4]\n",
      "Steps done: 7249\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 18: -379.774952501808\n",
      "[1, 4]\n",
      "Steps done: 7250\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 2: -26.482745571935226\n",
      "[1, 4, 2]\n",
      "Steps done: 7251\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 11: -1.5290010452562177\n",
      "[4, 2]\n",
      "Steps done: 7252\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 1: -26.482745571935226\n",
      "[4, 2, 1]\n",
      "Steps done: 7253\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 11: -1.5290010452562177\n",
      "[4, 2]\n",
      "Steps done: 7254\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 6: -10.805310955088842\n",
      "[4, 2, 6]\n",
      "Steps done: 7255\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 3: -3.003843456437594\n",
      "[4, 2, 6, 3]\n",
      "Steps done: 7256\n",
      "SV: [-0.6822668   0.10610793  1.092245  ]\n",
      "Reward for action 20: 0.9961565435624058\n",
      "[4, 2, 6, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 946\n",
      "Steps done: 7257\n",
      "SV: [ 1.4858187 -0.4902285  3.8880768]\n",
      "Reward for action 6: -217.1241269560374\n",
      "[0, 1, 6]\n",
      "Steps done: 7258\n",
      "SV: [ 1.4858187 -0.4902285  3.8880768]\n",
      "Reward for action 9: -347.7280244600981\n",
      "[0, 1, 6, 9]\n",
      "Steps done: 7259\n",
      "SV: [ 1.4858187 -0.4902285  3.8880768]\n",
      "Reward for action 3: -240.60782264060435\n",
      "[0, 1, 6, 9, 3]\n",
      "Steps done: 7260\n",
      "SV: [ 1.4858187 -0.4902285  3.8880768]\n",
      "Reward for action 10: -108.96036709630908\n",
      "[1, 6, 9, 3]\n",
      "Steps done: 7261\n",
      "SV: [ 1.4858187 -0.4902285  3.8880768]\n",
      "Reward for action 2: -58.692724905060274\n",
      "[1, 6, 9, 3, 2]\n",
      "Steps done: 7262\n",
      "SV: [ 1.4858187 -0.4902285  3.8880768]\n",
      "Reward for action 20: -54.692724905060274\n",
      "[1, 6, 9, 3, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 947\n",
      "Steps done: 7263\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 8: -138.0820483420862\n",
      "[0, 1, 8]\n",
      "Steps done: 7264\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 11: -145.725374172545\n",
      "[0, 8]\n",
      "Steps done: 7265\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 9: -126.41517360742318\n",
      "[0, 8, 9]\n",
      "Steps done: 7266\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 6: -74.7879677526758\n",
      "[0, 8, 9, 6]\n",
      "Steps done: 7267\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 18: -40.495332869119196\n",
      "[0, 9, 6]\n",
      "Steps done: 7268\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 5: -51.45898125363187\n",
      "[0, 9, 6, 5]\n",
      "Steps done: 7269\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 1: -63.2652744234894\n",
      "[0, 9, 6, 5, 1]\n",
      "Steps done: 7270\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 16: -88.89806770092561\n",
      "[0, 9, 5, 1]\n",
      "Steps done: 7271\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 11: -70.04456859527916\n",
      "[0, 9, 5]\n",
      "Steps done: 7272\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 4: -88.99895110589023\n",
      "[0, 9, 5, 4]\n",
      "Steps done: 7273\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 1: -97.64125316321294\n",
      "[0, 9, 5, 4, 1]\n",
      "Steps done: 7274\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 19: -89.5299960604748\n",
      "[0, 5, 4, 1]\n",
      "Steps done: 7275\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 11: -70.92514462921855\n",
      "[0, 5, 4]\n",
      "Steps done: 7276\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 8: -109.76935603153062\n",
      "[0, 5, 4, 8]\n",
      "Steps done: 7277\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 15: -127.6672639090649\n",
      "[0, 4, 8]\n",
      "Steps done: 7278\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 5: -109.76935603153062\n",
      "[0, 4, 8, 5]\n",
      "Steps done: 7279\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 18: -70.92514462921855\n",
      "[0, 4, 5]\n",
      "Steps done: 7280\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 6: -33.40540516754554\n",
      "[0, 4, 5, 6]\n",
      "Steps done: 7281\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 3: -46.22275803856373\n",
      "[0, 4, 5, 6, 3]\n",
      "Steps done: 7282\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 8: -73.24426269884036\n",
      "[0, 4, 5, 6, 3, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7283\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 18: -46.22275803856373\n",
      "[0, 4, 5, 6, 3]\n",
      "Steps done: 7284\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 8: -73.24426269884036\n",
      "[0, 4, 5, 6, 3, 8]\n",
      "Steps done: 7285\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 14: -89.38997750021723\n",
      "[0, 5, 6, 3, 8]\n",
      "Steps done: 7286\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 18: -56.917101304155835\n",
      "[0, 5, 6, 3]\n",
      "Steps done: 7287\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 7: -70.38715432201391\n",
      "[0, 5, 6, 3, 7]\n",
      "Steps done: 7288\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Reward for action 8: -89.271896146404\n",
      "[0, 5, 6, 3, 7, 8]\n",
      "Steps done: 7289\n",
      "SV: [ 0.04429105 -0.47402558  1.0724949 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -85.271896146404\n",
      "[0, 5, 6, 3, 7, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 948\n",
      "Steps done: 7290\n",
      "SV: [ 0.07745195  0.32587028 -0.71331894]\n",
      "Reward for action 20: -23.989902324323378\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 949\n",
      "Steps done: 7291\n",
      "SV: [-0.0355944  -0.73995054  1.3356    ]\n",
      "Reward for action 4: -3.546496878174781\n",
      "[0, 1, 4]\n",
      "Steps done: 7292\n",
      "SV: [-0.0355944  -0.73995054  1.3356    ]\n",
      "Reward for action 8: -27.988311401126982\n",
      "[0, 1, 4, 8]\n",
      "Steps done: 7293\n",
      "SV: [-0.0355944  -0.73995054  1.3356    ]\n",
      "Reward for action 2: -47.823833271803586\n",
      "[0, 1, 4, 8, 2]\n",
      "Steps done: 7294\n",
      "SV: [-0.0355944  -0.73995054  1.3356    ]\n",
      "Reward for action 20: -43.823833271803586\n",
      "[0, 1, 4, 8, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 950\n",
      "Steps done: 7295\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 2: -30.901939370322747\n",
      "[0, 1, 2]\n",
      "Steps done: 7296\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 4: -33.415409508788265\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7297\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 11: -45.96838830028199\n",
      "[0, 2, 4]\n",
      "Steps done: 7298\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 12: -21.150766999589564\n",
      "[0, 4]\n",
      "Steps done: 7299\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 2: -45.96838830028199\n",
      "[0, 4, 2]\n",
      "Did target update\n",
      "Steps done: 7300\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 1: -33.41540950878826\n",
      "[0, 4, 2, 1]\n",
      "Steps done: 7301\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 11: -45.96838830028199\n",
      "[0, 4, 2]\n",
      "Steps done: 7302\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 12: -21.150766999589564\n",
      "[0, 4]\n",
      "Steps done: 7303\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 1: -22.49629760814653\n",
      "[0, 4, 1]\n",
      "Steps done: 7304\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 5: -19.882305810479345\n",
      "[0, 4, 1, 5]\n",
      "Steps done: 7305\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 11: -24.271392166106267\n",
      "[0, 4, 5]\n",
      "Steps done: 7306\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 10: -19.033872926646005\n",
      "[4, 5]\n",
      "Steps done: 7307\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 6: -25.18415510832887\n",
      "[4, 5, 6]\n",
      "Steps done: 7308\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 16: -19.033872926646005\n",
      "[4, 5]\n",
      "Steps done: 7309\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 2: -36.72220321210699\n",
      "[4, 5, 2]\n",
      "Steps done: 7310\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 1: -20.814212684320665\n",
      "[4, 5, 2, 1]\n",
      "Steps done: 7311\n",
      "SV: [ 0.26450464 -0.53367513 -0.05713763]\n",
      "Reward for action 20: -16.814212684320665\n",
      "[4, 5, 2, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 951\n",
      "Steps done: 7312\n",
      "SV: [ 0.02488271  0.01463503 -0.23914753]\n",
      "Reward for action 3: -5.651218066713075\n",
      "[0, 1, 3]\n",
      "Steps done: 7313\n",
      "SV: [ 0.02488271  0.01463503 -0.23914753]\n",
      "Reward for action 20: -1.6512180667130751\n",
      "[0, 1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 952\n",
      "Steps done: 7314\n",
      "SV: [ 0.01631626  0.05146768 -0.27217832]\n",
      "Reward for action 20: -5.346763324175761\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 953\n",
      "Steps done: 7315\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 7: -121.38061545649862\n",
      "[0, 1, 7]\n",
      "Steps done: 7316\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 6: -93.01445189879186\n",
      "[0, 1, 7, 6]\n",
      "Steps done: 7317\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 11: -56.22544712754536\n",
      "[0, 7, 6]\n",
      "Steps done: 7318\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 5: -71.41584037869978\n",
      "[0, 7, 6, 5]\n",
      "Steps done: 7319\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 17: -108.64504603457499\n",
      "[0, 6, 5]\n",
      "Steps done: 7320\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 2: -76.69373092037554\n",
      "[0, 6, 5, 2]\n",
      "Steps done: 7321\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 8: -88.61713868492359\n",
      "[0, 6, 5, 2, 8]\n",
      "Steps done: 7322\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 10: -117.79254806167734\n",
      "[6, 5, 2, 8]\n",
      "Steps done: 7323\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 7: -82.2971760724296\n",
      "[6, 5, 2, 8, 7]\n",
      "Steps done: 7324\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 4: -87.72282903504372\n",
      "[6, 5, 2, 8, 7, 4]\n",
      "Steps done: 7325\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 14: -82.2971760724296\n",
      "[6, 5, 2, 8, 7]\n",
      "Steps done: 7326\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 18: -69.7402717962529\n",
      "[6, 5, 2, 7]\n",
      "Steps done: 7327\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 8: -82.29717607242961\n",
      "[6, 5, 2, 7, 8]\n",
      "Steps done: 7328\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 17: -117.79254806167734\n",
      "[6, 5, 2, 8]\n",
      "Steps done: 7329\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 4: -106.59718797201637\n",
      "[6, 5, 2, 8, 4]\n",
      "Steps done: 7330\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 3: -85.71769772069729\n",
      "[6, 5, 2, 8, 4, 3]\n",
      "Steps done: 7331\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 12: -96.12196668043008\n",
      "[6, 5, 8, 4, 3]\n",
      "Steps done: 7332\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 7: -80.41137396134462\n",
      "[6, 5, 8, 4, 3, 7]\n",
      "Steps done: 7333\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 1: -104.81101664988984\n",
      "[6, 5, 8, 4, 3, 7, 1]\n",
      "Steps done: 7334\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 15: -106.88169152066928\n",
      "[6, 8, 4, 3, 7, 1]\n",
      "Steps done: 7335\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 0: -91.56656474489387\n",
      "[6, 8, 4, 3, 7, 1, 0]\n",
      "Steps done: 7336\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 5: -93.5595464619706\n",
      "[6, 8, 4, 3, 7, 1, 0, 5]\n",
      "Steps done: 7337\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 16: -94.1769125752439\n",
      "[8, 4, 3, 7, 1, 0, 5]\n",
      "Steps done: 7338\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 2: -81.14681228298892\n",
      "[8, 4, 3, 7, 1, 0, 5, 2]\n",
      "Steps done: 7339\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 13: -102.12145493908486\n",
      "[8, 4, 7, 1, 0, 5, 2]\n",
      "Steps done: 7340\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Reward for action 3: -81.1468122829889\n",
      "[8, 4, 7, 1, 0, 5, 2, 3]\n",
      "Steps done: 7341\n",
      "SV: [-0.61883384 -0.4105494  -0.8337542 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -77.1468122829889\n",
      "[8, 4, 7, 1, 0, 5, 2, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 954\n",
      "Steps done: 7342\n",
      "SV: [ 0.43187162 -2.325713    1.2466922 ]\n",
      "Reward for action 20: -64.64310056766877\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 955\n",
      "Steps done: 7343\n",
      "SV: [-1.8463598  1.6771513 -7.3111696]\n",
      "Reward for action 4: -238.08629925090636\n",
      "[0, 1, 4]\n",
      "Steps done: 7344\n",
      "SV: [-1.8463598  1.6771513 -7.3111696]\n",
      "Reward for action 20: -234.08629925090636\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 956\n",
      "Steps done: 7345\n",
      "SV: [ 0.02167802  0.02498541 -0.7361555 ]\n",
      "Reward for action 2: -45.28224267365872\n",
      "[0, 1, 2]\n",
      "Steps done: 7346\n",
      "SV: [ 0.02167802  0.02498541 -0.7361555 ]\n",
      "Reward for action 12: -41.57974903951506\n",
      "[0, 1]\n",
      "Steps done: 7347\n",
      "SV: [ 0.02167802  0.02498541 -0.7361555 ]\n",
      "Reward for action 20: -37.57974903951506\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 957\n",
      "Steps done: 7348\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 2: -11.458603096300223\n",
      "[0, 1, 2]\n",
      "Steps done: 7349\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 12: -808.2269996625939\n",
      "[0, 1]\n",
      "Steps done: 7350\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 2: -11.458603096300223\n",
      "[0, 1, 2]\n",
      "Steps done: 7351\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 12: -808.2269996625939\n",
      "[0, 1]\n",
      "Steps done: 7352\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 4: -1007.6065852481828\n",
      "[0, 1, 4]\n",
      "Steps done: 7353\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 11: -1117.5355743264743\n",
      "[0, 4]\n",
      "Steps done: 7354\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 1: -1007.6065852481833\n",
      "[0, 4, 1]\n",
      "Steps done: 7355\n",
      "SV: [-0.542939   -0.81121254  3.782325  ]\n",
      "Reward for action 20: -1003.6065852481833\n",
      "[0, 4, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 958\n",
      "Steps done: 7356\n",
      "SV: [-0.5693155   0.9281665   0.62238723]\n",
      "Reward for action 3: -67.87407534192697\n",
      "[0, 1, 3]\n",
      "Steps done: 7357\n",
      "SV: [-0.5693155   0.9281665   0.62238723]\n",
      "Reward for action 10: -61.02361714402681\n",
      "[1, 3]\n",
      "Steps done: 7358\n",
      "SV: [-0.5693155   0.9281665   0.62238723]\n",
      "Reward for action 20: -57.02361714402681\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 959\n",
      "Steps done: 7359\n",
      "SV: [-0.07058575 -0.5715492  -1.9316491 ]\n",
      "Reward for action 20: -37.97762765862217\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 960\n",
      "Steps done: 7360\n",
      "SV: [ 0.14298706  0.43272376 -0.26686546]\n",
      "Reward for action 20: -2.409677390172268\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 961\n",
      "Steps done: 7361\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 6: -17.13410776341047\n",
      "[0, 1, 6]\n",
      "Steps done: 7362\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 2: -8.47406399325016\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 7363\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 4: -5.380484076000877\n",
      "[0, 1, 6, 2, 4]\n",
      "Steps done: 7364\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 16: -8.625318979350721\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7365\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 6: -5.380484076000874\n",
      "[0, 1, 2, 4, 6]\n",
      "Steps done: 7366\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 7: -2.752282881549454\n",
      "[0, 1, 2, 4, 6, 7]\n",
      "Steps done: 7367\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 16: -2.8843731203294833\n",
      "[0, 1, 2, 4, 7]\n",
      "Steps done: 7368\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 3: -2.371699225521384\n",
      "[0, 1, 2, 4, 7, 3]\n",
      "Steps done: 7369\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 12: -2.1226579862466215\n",
      "[0, 1, 4, 7, 3]\n",
      "Steps done: 7370\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 5: -2.0555784410707205\n",
      "[0, 1, 4, 7, 3, 5]\n",
      "Steps done: 7371\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 2: -2.368448795978082\n",
      "[0, 1, 4, 7, 3, 5, 2]\n",
      "Steps done: 7372\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 17: -2.75646896477975\n",
      "[0, 1, 4, 3, 5, 2]\n",
      "Steps done: 7373\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 14: -3.7367234242509992\n",
      "[0, 1, 3, 5, 2]\n",
      "Steps done: 7374\n",
      "SV: [ 0.17191592 -0.12255722 -0.01373418]\n",
      "Reward for action 20: 0.26327657574900076\n",
      "[0, 1, 3, 5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 962\n",
      "Steps done: 7375\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 8: -73.31106382171942\n",
      "[0, 1, 8]\n",
      "Steps done: 7376\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 9: -63.408358078781575\n",
      "[0, 1, 8, 9]\n",
      "Steps done: 7377\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 11: -70.0529247582972\n",
      "[0, 8, 9]\n",
      "Steps done: 7378\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 1: -63.408358078781575\n",
      "[0, 8, 9, 1]\n",
      "Steps done: 7379\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 11: -70.0529247582972\n",
      "[0, 8, 9]\n",
      "Steps done: 7380\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 6: -70.31450690878086\n",
      "[0, 8, 9, 6]\n",
      "Steps done: 7381\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 3: -63.79237384061909\n",
      "[0, 8, 9, 6, 3]\n",
      "Steps done: 7382\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 19: -66.81191939990498\n",
      "[0, 8, 6, 3]\n",
      "Steps done: 7383\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 16: -66.5792576694964\n",
      "[0, 8, 3]\n",
      "Steps done: 7384\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 6: -66.81191939990498\n",
      "[0, 8, 3, 6]\n",
      "Steps done: 7385\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 18: -62.25045327726628\n",
      "[0, 3, 6]\n",
      "Steps done: 7386\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 4: -55.396331567604484\n",
      "[0, 3, 6, 4]\n",
      "Steps done: 7387\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 9: -54.597237969371776\n",
      "[0, 3, 6, 4, 9]\n",
      "Steps done: 7388\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 2: -57.22829826844326\n",
      "[0, 3, 6, 4, 9, 2]\n",
      "Steps done: 7389\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 8: -60.296535626147396\n",
      "[0, 3, 6, 4, 9, 2, 8]\n",
      "Steps done: 7390\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 13: -63.09506153877049\n",
      "[0, 6, 4, 9, 2, 8]\n",
      "Steps done: 7391\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 19: -63.70623056690587\n",
      "[0, 6, 4, 2, 8]\n",
      "Steps done: 7392\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 5: -37.98637008878698\n",
      "[0, 6, 4, 2, 8, 5]\n",
      "Steps done: 7393\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 7: -42.4003113788953\n",
      "[0, 6, 4, 2, 8, 5, 7]\n",
      "Steps done: 7394\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 18: -52.84584766104129\n",
      "[0, 6, 4, 2, 5, 7]\n",
      "Steps done: 7395\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 15: -44.80287929912855\n",
      "[0, 6, 4, 2, 7]\n",
      "Steps done: 7396\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 16: -49.23732374487678\n",
      "[0, 4, 2, 7]\n",
      "Steps done: 7397\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 14: -67.164855975888\n",
      "[0, 2, 7]\n",
      "Steps done: 7398\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 9: -46.348162124264256\n",
      "[0, 2, 7, 9]\n",
      "Steps done: 7399\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 17: -69.80876525402843\n",
      "[0, 2, 9]\n",
      "Did target update\n",
      "Steps done: 7400\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Reward for action 3: -59.72109557115828\n",
      "[0, 2, 9, 3]\n",
      "Steps done: 7401\n",
      "SV: [-0.21554616  0.06614134 -0.8975131 ]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -55.72109557115828\n",
      "[0, 2, 9, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 964\n",
      "Steps done: 7402\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 8: -0.15780612462342758\n",
      "[0, 1, 8]\n",
      "Steps done: 7403\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 2: -0.44094182903266504\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 7404\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 3: -44.702994899769266\n",
      "[0, 1, 8, 2, 3]\n",
      "Steps done: 7405\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 13: -0.44094182903266504\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 7406\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 4: -59.66663305806655\n",
      "[0, 1, 8, 2, 4]\n",
      "Steps done: 7407\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 14: -0.44094182903266504\n",
      "[0, 1, 8, 2]\n",
      "Steps done: 7408\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 10: -0.3678349302554855\n",
      "[1, 8, 2]\n",
      "Steps done: 7409\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 18: -1.375220319136337\n",
      "[1, 2]\n",
      "Steps done: 7410\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 5: -6.149924829496237\n",
      "[1, 2, 5]\n",
      "Steps done: 7411\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 3: -48.386325435753136\n",
      "[1, 2, 5, 3]\n",
      "Steps done: 7412\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 11: -104.9270830269825\n",
      "[2, 5, 3]\n",
      "Steps done: 7413\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 1: -48.386325435753136\n",
      "[2, 5, 3, 1]\n",
      "Steps done: 7414\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 0: -47.584047603549934\n",
      "[2, 5, 3, 1, 0]\n",
      "Steps done: 7415\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 13: -7.5001243609401165\n",
      "[2, 5, 1, 0]\n",
      "Steps done: 7416\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 8: -4.000592930194729\n",
      "[2, 5, 1, 0, 8]\n",
      "Steps done: 7417\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 6: -1.8738024113469436\n",
      "[2, 5, 1, 0, 8, 6]\n",
      "Steps done: 7418\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 12: -1.6253687759040845\n",
      "[5, 1, 0, 8, 6]\n",
      "Steps done: 7419\n",
      "SV: [-0.13843916 -0.0775997   0.29291308]\n",
      "Reward for action 20: 2.3746312240959155\n",
      "[5, 1, 0, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 965\n",
      "Steps done: 7420\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 6: -25.202991853002807\n",
      "[0, 1, 6]\n",
      "Steps done: 7421\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 2: -26.241098778081785\n",
      "[0, 1, 6, 2]\n",
      "Steps done: 7422\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 11: -27.38556802175538\n",
      "[0, 6, 2]\n",
      "Steps done: 7423\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 5: -27.474471427943655\n",
      "[0, 6, 2, 5]\n",
      "Steps done: 7424\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 15: -27.38556802175538\n",
      "[0, 6, 2]\n",
      "Steps done: 7425\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 16: -74.73569400700448\n",
      "[0, 2]\n",
      "Steps done: 7426\n",
      "SV: [-0.24289088 -0.34338152 -0.87324876]\n",
      "Reward for action 20: -70.73569400700448\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 966\n",
      "Steps done: 7427\n",
      "SV: [-0.13945362  0.02635313 -0.17921983]\n",
      "Reward for action 2: -242.32407192302014\n",
      "[0, 1, 2]\n",
      "Steps done: 7428\n",
      "SV: [-0.13945362  0.02635313 -0.17921983]\n",
      "Reward for action 20: -238.32407192302014\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 967\n",
      "Steps done: 7429\n",
      "SV: [-0.42542312  0.13241552 -0.9307084 ]\n",
      "Reward for action 6: -69.62518341394238\n",
      "[0, 1, 6]\n",
      "Steps done: 7430\n",
      "SV: [-0.42542312  0.13241552 -0.9307084 ]\n",
      "Reward for action 20: -65.62518341394238\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 969\n",
      "Steps done: 7431\n",
      "SV: [ 1.6361357 -1.0483444 -6.891848 ]\n",
      "Reward for action 20: -1272.122144949545\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 970\n",
      "Steps done: 7432\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 6: -11.464374266817206\n",
      "[0, 1, 6]\n",
      "Steps done: 7433\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 5: -6.772096171948567\n",
      "[0, 1, 6, 5]\n",
      "Steps done: 7434\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 10: -6.92225537832294\n",
      "[1, 6, 5]\n",
      "Steps done: 7435\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 3: -7.604495759822772\n",
      "[1, 6, 5, 3]\n",
      "Steps done: 7436\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 13: -6.92225537832294\n",
      "[1, 6, 5]\n",
      "Steps done: 7437\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 9: -13.301467038054358\n",
      "[1, 6, 5, 9]\n",
      "Steps done: 7438\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 3: -10.865349688147948\n",
      "[1, 6, 5, 9, 3]\n",
      "Steps done: 7439\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 19: -7.604495759822772\n",
      "[1, 6, 5, 3]\n",
      "Steps done: 7440\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 8: -7.220890835426401\n",
      "[1, 6, 5, 3, 8]\n",
      "Steps done: 7441\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 18: -7.604495759822772\n",
      "[1, 6, 5, 3]\n",
      "Steps done: 7442\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 13: -6.92225537832294\n",
      "[1, 6, 5]\n",
      "Steps done: 7443\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 7: -7.2845070879447\n",
      "[1, 6, 5, 7]\n",
      "Steps done: 7444\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 3: -7.888505233209393\n",
      "[1, 6, 5, 7, 3]\n",
      "Steps done: 7445\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 8: -8.112981651594367\n",
      "[1, 6, 5, 7, 3, 8]\n",
      "Steps done: 7446\n",
      "SV: [ 0.26631084 -0.03811419 -0.26144716]\n",
      "Reward for action 20: -4.112981651594367\n",
      "[1, 6, 5, 7, 3, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 971\n",
      "Steps done: 7447\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 2: -57.36338409442823\n",
      "[0, 1, 2]\n",
      "Steps done: 7448\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 11: -18.660584214935284\n",
      "[0, 2]\n",
      "Steps done: 7449\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 1: -57.36338409442824\n",
      "[0, 2, 1]\n",
      "Steps done: 7450\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 11: -18.660584214935284\n",
      "[0, 2]\n",
      "Steps done: 7451\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 1: -57.36338409442824\n",
      "[0, 2, 1]\n",
      "Steps done: 7452\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 11: -18.660584214935284\n",
      "[0, 2]\n",
      "Steps done: 7453\n",
      "SV: [0.07804601 0.48595387 0.29895553]\n",
      "Reward for action 20: -14.660584214935284\n",
      "[0, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 972\n",
      "Steps done: 7454\n",
      "SV: [0.15664043 0.06667195 0.9313631 ]\n",
      "Reward for action 3: -79.17820359030658\n",
      "[0, 1, 3]\n",
      "Steps done: 7455\n",
      "SV: [0.15664043 0.06667195 0.9313631 ]\n",
      "Reward for action 10: -148.55656191942975\n",
      "[1, 3]\n",
      "Steps done: 7456\n",
      "SV: [0.15664043 0.06667195 0.9313631 ]\n",
      "Reward for action 5: -75.89293032028121\n",
      "[1, 3, 5]\n",
      "Steps done: 7457\n",
      "SV: [0.15664043 0.06667195 0.9313631 ]\n",
      "Reward for action 20: -71.89293032028121\n",
      "[1, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 973\n",
      "Steps done: 7458\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 5: -52.60148055835159\n",
      "[0, 1, 5]\n",
      "Steps done: 7459\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 2: -36.107295417319676\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 7460\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 12: -52.60148055835159\n",
      "[0, 1, 5]\n",
      "Steps done: 7461\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 8: -44.30325513945544\n",
      "[0, 1, 5, 8]\n",
      "Steps done: 7462\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 18: -52.60148055835159\n",
      "[0, 1, 5]\n",
      "Steps done: 7463\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 9: -36.43702685481654\n",
      "[0, 1, 5, 9]\n",
      "Steps done: 7464\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 4: -33.5663453339327\n",
      "[0, 1, 5, 9, 4]\n",
      "Steps done: 7465\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 7: -32.747015022081726\n",
      "[0, 1, 5, 9, 4, 7]\n",
      "Steps done: 7466\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 17: -33.5663453339327\n",
      "[0, 1, 5, 9, 4]\n",
      "Steps done: 7467\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 7: -32.747015022081726\n",
      "[0, 1, 5, 9, 4, 7]\n",
      "Steps done: 7468\n",
      "SV: [-0.18673153  0.3693703   0.52273154]\n",
      "Reward for action 20: -28.747015022081726\n",
      "[0, 1, 5, 9, 4, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 974\n",
      "Steps done: 7469\n",
      "SV: [-0.05857592 -1.8689823  -7.5972667 ]\n",
      "Reward for action 4: -281.5853217372147\n",
      "[0, 1, 4]\n",
      "Steps done: 7470\n",
      "SV: [-0.05857592 -1.8689823  -7.5972667 ]\n",
      "Reward for action 11: -256.57346753883087\n",
      "[0, 4]\n",
      "Steps done: 7471\n",
      "SV: [-0.05857592 -1.8689823  -7.5972667 ]\n",
      "Reward for action 2: -8.819027765787363\n",
      "[0, 4, 2]\n",
      "Steps done: 7472\n",
      "SV: [-0.05857592 -1.8689823  -7.5972667 ]\n",
      "Reward for action 14: -450.7011280331438\n",
      "[0, 2]\n",
      "Steps done: 7473\n",
      "SV: [-0.05857592 -1.8689823  -7.5972667 ]\n",
      "Reward for action 4: -8.819027765787363\n",
      "[0, 2, 4]\n",
      "Steps done: 7474\n",
      "SV: [-0.05857592 -1.8689823  -7.5972667 ]\n",
      "Reward for action 20: -4.819027765787363\n",
      "[0, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 975\n",
      "Steps done: 7475\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 2: -0.12131192811868262\n",
      "[0, 1, 2]\n",
      "Steps done: 7476\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 12: -0.09246431287561525\n",
      "[0, 1]\n",
      "Steps done: 7477\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 5: -0.40590872489022534\n",
      "[0, 1, 5]\n",
      "Steps done: 7478\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 3: -0.2564575884511755\n",
      "[0, 1, 5, 3]\n",
      "Steps done: 7479\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 4: -0.06045587551344343\n",
      "[0, 1, 5, 3, 4]\n",
      "Steps done: 7480\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 11: -0.07564870960642144\n",
      "[0, 5, 3, 4]\n",
      "Steps done: 7481\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 15: -0.08679419887779512\n",
      "[0, 3, 4]\n",
      "Steps done: 7482\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 5: -0.07564870960642145\n",
      "[0, 3, 4, 5]\n",
      "Steps done: 7483\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 14: -0.11259503374395836\n",
      "[0, 3, 5]\n",
      "Steps done: 7484\n",
      "SV: [ 0.00150366 -0.00088386  0.04793633]\n",
      "Reward for action 20: 3.8874049662560415\n",
      "[0, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 976\n",
      "Steps done: 7485\n",
      "SV: [-0.04856041 -0.02635895  0.05390885]\n",
      "Reward for action 20: -4.414725297409962\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 977\n",
      "Steps done: 7486\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 3: -147.98965508970844\n",
      "[0, 1, 3]\n",
      "Steps done: 7487\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 4: -188.1538929596547\n",
      "[0, 1, 3, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7488\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 14: -147.98965508970844\n",
      "[0, 1, 3]\n",
      "Steps done: 7489\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 11: -112.88541119816023\n",
      "[0, 3]\n",
      "Steps done: 7490\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 2: -513.5751054107059\n",
      "[0, 3, 2]\n",
      "Steps done: 7491\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 10: -793.8223672566811\n",
      "[3, 2]\n",
      "Steps done: 7492\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 1: -468.61041568122585\n",
      "[3, 2, 1]\n",
      "Steps done: 7493\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 4: -372.5688278545766\n",
      "[3, 2, 1, 4]\n",
      "Steps done: 7494\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 14: -468.61041568122585\n",
      "[3, 2, 1]\n",
      "Steps done: 7495\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 13: -981.7995760906795\n",
      "[2, 1]\n",
      "Steps done: 7496\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 0: -645.2279575687032\n",
      "[2, 1, 0]\n",
      "Steps done: 7497\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 3: -388.42081003729925\n",
      "[2, 1, 0, 3]\n",
      "Steps done: 7498\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 12: -147.9896550897084\n",
      "[1, 0, 3]\n",
      "Steps done: 7499\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 10: -104.14271303071965\n",
      "[1, 3]\n",
      "Did target update\n",
      "Steps done: 7500\n",
      "SV: [0.5068713  0.98598737 1.4284757 ]\n",
      "Reward for action 20: -100.14271303071965\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 978\n",
      "Steps done: 7501\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 3: -32.6299529484548\n",
      "[0, 1, 3]\n",
      "Steps done: 7502\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 7: -24.785556743462536\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 7503\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 9: -55.711453347877686\n",
      "[0, 1, 3, 7, 9]\n",
      "Steps done: 7504\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 4: -49.43405497580003\n",
      "[0, 1, 3, 7, 9, 4]\n",
      "Steps done: 7505\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 19: -25.80230329508328\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 7506\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 14: -24.785556743462536\n",
      "[0, 1, 3, 7]\n",
      "Steps done: 7507\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 4: -25.80230329508328\n",
      "[0, 1, 3, 7, 4]\n",
      "Steps done: 7508\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 6: -29.370559372821198\n",
      "[0, 1, 3, 7, 4, 6]\n",
      "Steps done: 7509\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 14: -28.568164801984572\n",
      "[0, 1, 3, 7, 6]\n",
      "Steps done: 7510\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 9: -35.28762879941758\n",
      "[0, 1, 3, 7, 6, 9]\n",
      "Steps done: 7511\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 5: -34.88927788102737\n",
      "[0, 1, 3, 7, 6, 9, 5]\n",
      "Steps done: 7512\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 17: -37.74735900403393\n",
      "[0, 1, 3, 6, 9, 5]\n",
      "Steps done: 7513\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 7: -34.88927788102737\n",
      "[0, 1, 3, 6, 9, 5, 7]\n",
      "Steps done: 7514\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 15: -35.28762879941759\n",
      "[0, 1, 3, 6, 9, 7]\n",
      "Steps done: 7515\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 11: -32.88889560804247\n",
      "[0, 3, 6, 9, 7]\n",
      "Steps done: 7516\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 10: -24.77258070296739\n",
      "[3, 6, 9, 7]\n",
      "Steps done: 7517\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 16: -81.39782903124048\n",
      "[3, 9, 7]\n",
      "Steps done: 7518\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 5: -64.75064620482843\n",
      "[3, 9, 7, 5]\n",
      "Steps done: 7519\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 8: -51.58361608733452\n",
      "[3, 9, 7, 5, 8]\n",
      "Steps done: 7520\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 2: -44.53930212291199\n",
      "[3, 9, 7, 5, 8, 2]\n",
      "Steps done: 7521\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 15: -50.28264358500015\n",
      "[3, 9, 7, 8, 2]\n",
      "Steps done: 7522\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 18: -60.94078968532654\n",
      "[3, 9, 7, 2]\n",
      "Steps done: 7523\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 4: -51.17997073626433\n",
      "[3, 9, 7, 2, 4]\n",
      "Steps done: 7524\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 5: -45.49706634064186\n",
      "[3, 9, 7, 2, 4, 5]\n",
      "Steps done: 7525\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 13: -52.04105566809119\n",
      "[9, 7, 2, 4, 5]\n",
      "Steps done: 7526\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Reward for action 14: -67.52321259981517\n",
      "[9, 7, 2, 5]\n",
      "Steps done: 7527\n",
      "SV: [-0.10378069 -0.00587843  0.51155937]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -63.52321259981517\n",
      "[9, 7, 2, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 979\n",
      "Steps done: 7528\n",
      "SV: [ 0.47721544  0.2737251  -0.698018  ]\n",
      "Reward for action 20: -53.71804774753973\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 980\n",
      "Steps done: 7529\n",
      "SV: [ 0.20177443 -0.00443426 -1.0607203 ]\n",
      "Reward for action 8: -54.46848495943357\n",
      "[0, 1, 8]\n",
      "Steps done: 7530\n",
      "SV: [ 0.20177443 -0.00443426 -1.0607203 ]\n",
      "Reward for action 20: -50.46848495943357\n",
      "[0, 1, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 981\n",
      "Steps done: 7531\n",
      "SV: [-0.42483625  0.8375554   1.588432  ]\n",
      "Reward for action 20: -536.6365825431894\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 982\n",
      "Steps done: 7532\n",
      "SV: [-0.26279116 -0.4447877  -0.36380562]\n",
      "Reward for action 7: -46.36280075886272\n",
      "[0, 1, 7]\n",
      "Steps done: 7533\n",
      "SV: [-0.26279116 -0.4447877  -0.36380562]\n",
      "Reward for action 20: -42.36280075886272\n",
      "[0, 1, 7]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 983\n",
      "Steps done: 7534\n",
      "SV: [ 1.5099381  0.8483022 -1.7743431]\n",
      "Reward for action 20: -185.9756789606515\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 984\n",
      "Steps done: 7535\n",
      "SV: [ 0.39560097 -0.50367343  0.0873877 ]\n",
      "Reward for action 3: -40.49675354107078\n",
      "[0, 1, 3]\n",
      "Steps done: 7536\n",
      "SV: [ 0.39560097 -0.50367343  0.0873877 ]\n",
      "Reward for action 11: -34.3781185472158\n",
      "[0, 3]\n",
      "Steps done: 7537\n",
      "SV: [ 0.39560097 -0.50367343  0.0873877 ]\n",
      "Reward for action 20: -30.3781185472158\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 985\n",
      "Steps done: 7538\n",
      "SV: [ 0.52651644 -0.09892493 -1.6576614 ]\n",
      "Reward for action 6: -305.4304721567851\n",
      "[0, 1, 6]\n",
      "Steps done: 7539\n",
      "SV: [ 0.52651644 -0.09892493 -1.6576614 ]\n",
      "Reward for action 10: -804.4729858894691\n",
      "[1, 6]\n",
      "Steps done: 7540\n",
      "SV: [ 0.52651644 -0.09892493 -1.6576614 ]\n",
      "Reward for action 20: -800.4729858894691\n",
      "[1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 986\n",
      "Steps done: 7541\n",
      "SV: [ 0.02077686  0.00837796 -0.72553486]\n",
      "Reward for action 20: -8.417522946383976\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 987\n",
      "Steps done: 7542\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 6: -750.0235497979343\n",
      "[0, 1, 6]\n",
      "Steps done: 7543\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 4: -406.16031832673684\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 7544\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 5: -122.32759346760291\n",
      "[0, 1, 6, 4, 5]\n",
      "Steps done: 7545\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 15: -406.16031832673684\n",
      "[0, 1, 6, 4]\n",
      "Steps done: 7546\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 5: -122.32759346760291\n",
      "[0, 1, 6, 4, 5]\n",
      "Steps done: 7547\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 11: -27.83224764379042\n",
      "[0, 6, 4, 5]\n",
      "Steps done: 7548\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 14: -16.55393761080713\n",
      "[0, 6, 5]\n",
      "Steps done: 7549\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 15: -778.8554482345057\n",
      "[0, 6]\n",
      "Steps done: 7550\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 5: -16.55393761080713\n",
      "[0, 6, 5]\n",
      "Steps done: 7551\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 15: -778.8554482345057\n",
      "[0, 6]\n",
      "Steps done: 7552\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 2: -777.5322520652355\n",
      "[0, 6, 2]\n",
      "Steps done: 7553\n",
      "SV: [-1.0246975   0.21151951 -3.4036005 ]\n",
      "Reward for action 20: -773.5322520652355\n",
      "[0, 6, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 988\n",
      "Steps done: 7554\n",
      "SV: [-0.01123539  0.13985741  0.1641812 ]\n",
      "Reward for action 20: -17.110616962206542\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 989\n",
      "Steps done: 7555\n",
      "SV: [0.00197906 0.30459943 0.05889479]\n",
      "Reward for action 6: -4.847085601735571\n",
      "[0, 1, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7556\n",
      "SV: [0.00197906 0.30459943 0.05889479]\n",
      "Reward for action 16: -1.6234738147626455\n",
      "[0, 1]\n",
      "Steps done: 7557\n",
      "SV: [0.00197906 0.30459943 0.05889479]\n",
      "Reward for action 4: -7.781562798088046\n",
      "[0, 1, 4]\n",
      "Steps done: 7558\n",
      "SV: [0.00197906 0.30459943 0.05889479]\n",
      "Reward for action 20: -3.7815627980880464\n",
      "[0, 1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 990\n",
      "Steps done: 7559\n",
      "SV: [0.5466742 0.6791486 2.70021  ]\n",
      "Reward for action 20: -699.2090495691928\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 991\n",
      "Steps done: 7560\n",
      "SV: [ 0.80794424 -0.2873624  -0.25888368]\n",
      "Reward for action 20: -197.77968341114808\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 992\n",
      "Steps done: 7561\n",
      "SV: [0.4135503  0.07185375 0.04371176]\n",
      "Reward for action 20: -12.140162439912814\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 993\n",
      "Steps done: 7562\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 3: -98.81404140525275\n",
      "[0, 1, 3]\n",
      "Steps done: 7563\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 10: -113.52716657569793\n",
      "[1, 3]\n",
      "Steps done: 7564\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 0: -98.81404140525275\n",
      "[1, 3, 0]\n",
      "Steps done: 7565\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 11: -71.56531035002577\n",
      "[3, 0]\n",
      "Steps done: 7566\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 1: -98.81404140525275\n",
      "[3, 0, 1]\n",
      "Steps done: 7567\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 10: -113.52716657569793\n",
      "[3, 1]\n",
      "Steps done: 7568\n",
      "SV: [ 0.33564195  0.27121767 -1.1336075 ]\n",
      "Reward for action 20: -109.52716657569793\n",
      "[3, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 994\n",
      "Steps done: 7569\n",
      "SV: [ 0.31595746  0.00073818 -0.33854407]\n",
      "Reward for action 2: -18.924922479772704\n",
      "[0, 1, 2]\n",
      "Steps done: 7570\n",
      "SV: [ 0.31595746  0.00073818 -0.33854407]\n",
      "Reward for action 8: -14.478008453802008\n",
      "[0, 1, 2, 8]\n",
      "Steps done: 7571\n",
      "SV: [ 0.31595746  0.00073818 -0.33854407]\n",
      "Reward for action 10: -22.9248818138934\n",
      "[1, 2, 8]\n",
      "Steps done: 7572\n",
      "SV: [ 0.31595746  0.00073818 -0.33854407]\n",
      "Reward for action 20: -18.9248818138934\n",
      "[1, 2, 8]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 995\n",
      "Steps done: 7573\n",
      "SV: [-0.4296088  -0.23220976  0.6245821 ]\n",
      "Reward for action 20: -11.097467523305372\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 996\n",
      "Steps done: 7574\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 2: -83.63169248244263\n",
      "[0, 1, 2]\n",
      "Steps done: 7575\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 6: -39.451625023652774\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 7576\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 16: -83.63169248244263\n",
      "[0, 1, 2]\n",
      "Steps done: 7577\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 4: -82.34236872457208\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7578\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 6: -43.11552495232597\n",
      "[0, 1, 2, 4, 6]\n",
      "Steps done: 7579\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 16: -82.34236872457208\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7580\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 14: -83.63169248244263\n",
      "[0, 1, 2]\n",
      "Steps done: 7581\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 4: -82.34236872457208\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7582\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 14: -83.63169248244263\n",
      "[0, 1, 2]\n",
      "Steps done: 7583\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 11: -89.04920505970279\n",
      "[0, 2]\n",
      "Steps done: 7584\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 6: -38.146241950580745\n",
      "[0, 2, 6]\n",
      "Steps done: 7585\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 5: -34.3313013464505\n",
      "[0, 2, 6, 5]\n",
      "Steps done: 7586\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 10: -34.03508637829616\n",
      "[2, 6, 5]\n",
      "Steps done: 7587\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 0: -34.33130134645051\n",
      "[2, 6, 5, 0]\n",
      "Steps done: 7588\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 15: -38.146241950580745\n",
      "[2, 6, 0]\n",
      "Steps done: 7589\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 10: -35.9225273349591\n",
      "[2, 6]\n",
      "Steps done: 7590\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 3: -36.66690444601309\n",
      "[2, 6, 3]\n",
      "Steps done: 7591\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 16: -86.44907539278033\n",
      "[2, 3]\n",
      "Steps done: 7592\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 4: -85.33325867144448\n",
      "[2, 3, 4]\n",
      "Steps done: 7593\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 13: -79.9450212417922\n",
      "[2, 4]\n",
      "Steps done: 7594\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 6: -37.06325990422387\n",
      "[2, 4, 6]\n",
      "Steps done: 7595\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 1: -38.65825683732727\n",
      "[2, 4, 6, 1]\n",
      "Steps done: 7596\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 16: -81.83351259955802\n",
      "[2, 4, 1]\n",
      "Steps done: 7597\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 14: -81.40800017402913\n",
      "[2, 1]\n",
      "Steps done: 7598\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 3: -75.4820027904869\n",
      "[2, 1, 3]\n",
      "Steps done: 7599\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Reward for action 12: -61.28912565399726\n",
      "[1, 3]\n",
      "Did target update\n",
      "Steps done: 7600\n",
      "SV: [-0.4827636  -0.31348604  0.81498396]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -57.28912565399726\n",
      "[1, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 997\n",
      "Steps done: 7601\n",
      "SV: [ 0.03592826  0.11008773 -0.041718  ]\n",
      "Reward for action 6: -9.77548883899385\n",
      "[0, 1, 6]\n",
      "Steps done: 7602\n",
      "SV: [ 0.03592826  0.11008773 -0.041718  ]\n",
      "Reward for action 20: -5.77548883899385\n",
      "[0, 1, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 998\n",
      "Steps done: 7603\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 3: -54.99015241882708\n",
      "[0, 1, 3]\n",
      "Steps done: 7604\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 13: -76.73861260774652\n",
      "[0, 1]\n",
      "Steps done: 7605\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 4: -15.589648171834895\n",
      "[0, 1, 4]\n",
      "Steps done: 7606\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 14: -76.73861260774652\n",
      "[0, 1]\n",
      "Steps done: 7607\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 3: -54.99015241882708\n",
      "[0, 1, 3]\n",
      "Steps done: 7608\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 11: -16.827590086951997\n",
      "[0, 3]\n",
      "Steps done: 7609\n",
      "SV: [ 0.3919801  0.1256133 -1.3898848]\n",
      "Reward for action 20: -12.827590086951997\n",
      "[0, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 999\n",
      "Steps done: 7610\n",
      "SV: [0.33068222 1.154007   4.6882553 ]\n",
      "Reward for action 20: -785.7826985845434\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1000\n",
      "Steps done: 7611\n",
      "SV: [ 0.03893443 -0.00245329  0.46560565]\n",
      "Reward for action 20: -134.26087417937103\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1001\n",
      "Steps done: 7612\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 6: -64.71119821770645\n",
      "[0, 1, 6]\n",
      "Steps done: 7613\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 16: -18.34640409228521\n",
      "[0, 1]\n",
      "Steps done: 7614\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 5: -32.52961112802513\n",
      "[0, 1, 5]\n",
      "Steps done: 7615\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 2: -20.358226791179124\n",
      "[0, 1, 5, 2]\n",
      "Steps done: 7616\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 10: -39.1273916195513\n",
      "[1, 5, 2]\n",
      "Steps done: 7617\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 3: -31.851922061879137\n",
      "[1, 5, 2, 3]\n",
      "Steps done: 7618\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 0: -21.979076540300102\n",
      "[1, 5, 2, 3, 0]\n",
      "Steps done: 7619\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 13: -20.358226791179124\n",
      "[1, 5, 2, 0]\n",
      "Steps done: 7620\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 6: -45.40191153471507\n",
      "[1, 5, 2, 0, 6]\n",
      "Steps done: 7621\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 10: -67.5800940380441\n",
      "[1, 5, 2, 6]\n",
      "Steps done: 7622\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 15: -55.86363948402993\n",
      "[1, 2, 6]\n",
      "Steps done: 7623\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 3: -57.760293191594286\n",
      "[1, 2, 6, 3]\n",
      "Steps done: 7624\n",
      "SV: [-0.20957687 -0.5018671  -0.24738081]\n",
      "Reward for action 20: -53.760293191594286\n",
      "[1, 2, 6, 3]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps done: 7625\n",
      "SV: [-0.48737952  0.49172845  3.236216  ]\n",
      "Reward for action 5: -66.6897549340154\n",
      "[0, 1, 5]\n",
      "Steps done: 7626\n",
      "SV: [-0.48737952  0.49172845  3.236216  ]\n",
      "Reward for action 4: -87.2809120949796\n",
      "[0, 1, 5, 4]\n",
      "Steps done: 7627\n",
      "SV: [-0.48737952  0.49172845  3.236216  ]\n",
      "Reward for action 20: -83.2809120949796\n",
      "[0, 1, 5, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1003\n",
      "Steps done: 7628\n",
      "SV: [-0.41496643  0.07515403  0.62416285]\n",
      "Reward for action 2: -25.81533643101867\n",
      "[0, 1, 2]\n",
      "Steps done: 7629\n",
      "SV: [-0.41496643  0.07515403  0.62416285]\n",
      "Reward for action 6: -36.24976831553843\n",
      "[0, 1, 2, 6]\n",
      "Steps done: 7630\n",
      "SV: [-0.41496643  0.07515403  0.62416285]\n",
      "Reward for action 20: -32.24976831553843\n",
      "[0, 1, 2, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1004\n",
      "Steps done: 7631\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 3: -5.227581208513903\n",
      "[0, 1, 3]\n",
      "Steps done: 7632\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 10: -6.671384024578072\n",
      "[1, 3]\n",
      "Steps done: 7633\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 4: -17.25378951377888\n",
      "[1, 3, 4]\n",
      "Steps done: 7634\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 14: -6.671384024578072\n",
      "[1, 3]\n",
      "Steps done: 7635\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 5: -18.314444950116794\n",
      "[1, 3, 5]\n",
      "Steps done: 7636\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 11: -28.781662239622086\n",
      "[3, 5]\n",
      "Steps done: 7637\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 2: -15.396603143223615\n",
      "[3, 5, 2]\n",
      "Steps done: 7638\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 13: -7.4167241014781915\n",
      "[5, 2]\n",
      "Steps done: 7639\n",
      "SV: [-0.544881    0.19677937 -0.08305584]\n",
      "Reward for action 20: -3.4167241014781915\n",
      "[5, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1005\n",
      "Steps done: 7640\n",
      "SV: [-0.7997558  0.0104178 -1.1271013]\n",
      "Reward for action 20: -262.03397855501754\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1006\n",
      "Steps done: 7641\n",
      "SV: [-0.03565295 -0.07192012 -0.90214294]\n",
      "Reward for action 4: -75.33062445070655\n",
      "[0, 1, 4]\n",
      "Steps done: 7642\n",
      "SV: [-0.03565295 -0.07192012 -0.90214294]\n",
      "Reward for action 14: -68.52757987076595\n",
      "[0, 1]\n",
      "Steps done: 7643\n",
      "SV: [-0.03565295 -0.07192012 -0.90214294]\n",
      "Reward for action 20: -64.52757987076595\n",
      "[0, 1]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1007\n",
      "Steps done: 7644\n",
      "SV: [-0.02618834 -0.04256494 -0.19002442]\n",
      "Reward for action 2: -90.23414419419606\n",
      "[0, 1, 2]\n",
      "Steps done: 7645\n",
      "SV: [-0.02618834 -0.04256494 -0.19002442]\n",
      "Reward for action 20: -86.23414419419606\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1008\n",
      "Steps done: 7646\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 2: -68.89274668697412\n",
      "[0, 1, 2]\n",
      "Steps done: 7647\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 4: -55.35288188388248\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7648\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 11: -42.7543532961392\n",
      "[0, 2, 4]\n",
      "Steps done: 7649\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 1: -55.35288188388249\n",
      "[0, 2, 4, 1]\n",
      "Steps done: 7650\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 3: -18.02115447377392\n",
      "[0, 2, 4, 1, 3]\n",
      "Steps done: 7651\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 11: -5.04955280299267\n",
      "[0, 2, 4, 3]\n",
      "Steps done: 7652\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 1: -18.021154473773926\n",
      "[0, 2, 4, 3, 1]\n",
      "Steps done: 7653\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 12: -21.29890924221541\n",
      "[0, 4, 3, 1]\n",
      "Steps done: 7654\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 11: -2.3020246283847774\n",
      "[0, 4, 3]\n",
      "Steps done: 7655\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 10: -9.50786279788355\n",
      "[4, 3]\n",
      "Steps done: 7656\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 8: -6.971997550201413\n",
      "[4, 3, 8]\n",
      "Steps done: 7657\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 18: -9.50786279788355\n",
      "[4, 3]\n",
      "Steps done: 7658\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 6: -0.9793344260091943\n",
      "[4, 3, 6]\n",
      "Steps done: 7659\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 8: -4.118040790144853\n",
      "[4, 3, 6, 8]\n",
      "Steps done: 7660\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 13: -4.162592839305867\n",
      "[4, 6, 8]\n",
      "Steps done: 7661\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 18: -0.8762092977507423\n",
      "[4, 6]\n",
      "Steps done: 7662\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 8: -4.162592839305867\n",
      "[4, 6, 8]\n",
      "Steps done: 7663\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 2: -1.935075846068911\n",
      "[4, 6, 8, 2]\n",
      "Steps done: 7664\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 18: -5.162308053722118\n",
      "[4, 6, 2]\n",
      "Steps done: 7665\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 16: -44.29295754263511\n",
      "[4, 2]\n",
      "Steps done: 7666\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 8: -2.8250667911396534\n",
      "[4, 2, 8]\n",
      "Steps done: 7667\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 18: -44.29295754263511\n",
      "[4, 2]\n",
      "Steps done: 7668\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 6: -5.162308053722118\n",
      "[4, 2, 6]\n",
      "Steps done: 7669\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 3: -0.9006912522608022\n",
      "[4, 2, 6, 3]\n",
      "Steps done: 7670\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 13: -5.162308053722118\n",
      "[4, 2, 6]\n",
      "Steps done: 7671\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Reward for action 16: -44.29295754263511\n",
      "[4, 2]\n",
      "Steps done: 7672\n",
      "SV: [-0.07567633  0.521666   -0.29962242]\n",
      "Episode will have ended forcefully\n",
      "Reward for action 20: -40.29295754263511\n",
      "[4, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1009\n",
      "Steps done: 7673\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 5: -0.8407807072639312\n",
      "[0, 1, 5]\n",
      "Steps done: 7674\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 9: -0.9659929480325418\n",
      "[0, 1, 5, 9]\n",
      "Steps done: 7675\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 15: -1.005147796250276\n",
      "[0, 1, 9]\n",
      "Steps done: 7676\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 2: -0.8536437823033093\n",
      "[0, 1, 9, 2]\n",
      "Steps done: 7677\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 6: -1.1399330701441044\n",
      "[0, 1, 9, 2, 6]\n",
      "Steps done: 7678\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 16: -0.8536437823033093\n",
      "[0, 1, 9, 2]\n",
      "Steps done: 7679\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 19: -0.6855969546661002\n",
      "[0, 1, 2]\n",
      "Steps done: 7680\n",
      "SV: [ 0.0608655  -0.05597265 -0.10788367]\n",
      "Reward for action 20: 3.3144030453338997\n",
      "[0, 1, 2]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1010\n",
      "Steps done: 7681\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 6: -3.5723938851717425\n",
      "[0, 1, 6]\n",
      "Steps done: 7682\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 11: -4.156300097044654\n",
      "[0, 6]\n",
      "Steps done: 7683\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 4: -5.396745386129998\n",
      "[0, 6, 4]\n",
      "Steps done: 7684\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 14: -4.156300097044654\n",
      "[0, 6]\n",
      "Steps done: 7685\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 3: -3.6921375067495035\n",
      "[0, 6, 3]\n",
      "Steps done: 7686\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 2: -3.504368465664178\n",
      "[0, 6, 3, 2]\n",
      "Steps done: 7687\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 13: -5.60748457442878\n",
      "[0, 6, 2]\n",
      "Steps done: 7688\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 4: -3.807677342864451\n",
      "[0, 6, 2, 4]\n",
      "Steps done: 7689\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 16: -4.316634040314472\n",
      "[0, 2, 4]\n",
      "Steps done: 7690\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 14: -34.15524648254831\n",
      "[0, 2]\n",
      "Steps done: 7691\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 3: -5.012694160578127\n",
      "[0, 2, 3]\n",
      "Steps done: 7692\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 10: -3.1279332095389534\n",
      "[2, 3]\n",
      "Steps done: 7693\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 4: -10.584218823931428\n",
      "[2, 3, 4]\n",
      "Steps done: 7694\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 14: -3.1279332095389534\n",
      "[2, 3]\n",
      "Steps done: 7695\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for action 0: -5.012694160578127\n",
      "[2, 3, 0]\n",
      "Steps done: 7696\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 10: -3.1279332095389534\n",
      "[2, 3]\n",
      "Steps done: 7697\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 6: -3.1789643972982073\n",
      "[2, 3, 6]\n",
      "Steps done: 7698\n",
      "SV: [-0.03680566  0.13860929 -0.13753511]\n",
      "Reward for action 20: 0.8210356027017927\n",
      "[2, 3, 6]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1011\n",
      "Steps done: 7699\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 6: -3.830746884391728\n",
      "[0, 1, 6]\n",
      "Did target update\n",
      "Steps done: 7700\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 16: -610.1135406141095\n",
      "[0, 1]\n",
      "Steps done: 7701\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 6: -3.830746884391728\n",
      "[0, 1, 6]\n",
      "Steps done: 7702\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 3: -135.53538873814844\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 7703\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 16: -58.272591198312455\n",
      "[0, 1, 3]\n",
      "Steps done: 7704\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 13: -610.1135406141095\n",
      "[0, 1]\n",
      "Steps done: 7705\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 3: -58.272591198312455\n",
      "[0, 1, 3]\n",
      "Steps done: 7706\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 2: -0.24509081878562453\n",
      "[0, 1, 3, 2]\n",
      "Steps done: 7707\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 13: -190.61448324401937\n",
      "[0, 1, 2]\n",
      "Steps done: 7708\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 4: -87.88212799398772\n",
      "[0, 1, 2, 4]\n",
      "Steps done: 7709\n",
      "SV: [ 0.2138255  -0.02150291 -0.8313722 ]\n",
      "Reward for action 20: -83.88212799398772\n",
      "[0, 1, 2, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1012\n",
      "Steps done: 7710\n",
      "SV: [0.04648754 0.34568232 0.21432836]\n",
      "Reward for action 6: -8.124591351692114\n",
      "[0, 1, 6]\n",
      "Steps done: 7711\n",
      "SV: [0.04648754 0.34568232 0.21432836]\n",
      "Reward for action 3: -12.413841422643559\n",
      "[0, 1, 6, 3]\n",
      "Steps done: 7712\n",
      "SV: [0.04648754 0.34568232 0.21432836]\n",
      "Reward for action 16: -14.175007167318245\n",
      "[0, 1, 3]\n",
      "Steps done: 7713\n",
      "SV: [0.04648754 0.34568232 0.21432836]\n",
      "Reward for action 5: -14.016649399173433\n",
      "[0, 1, 3, 5]\n",
      "Steps done: 7714\n",
      "SV: [0.04648754 0.34568232 0.21432836]\n",
      "Reward for action 11: -13.957733451938953\n",
      "[0, 3, 5]\n",
      "Steps done: 7715\n",
      "SV: [0.04648754 0.34568232 0.21432836]\n",
      "Reward for action 20: -9.957733451938953\n",
      "[0, 3, 5]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Currently at event: 1013\n",
      "Steps done: 7716\n",
      "SV: [ 0.14621973 -0.99457496  1.1096114 ]\n",
      "Reward for action 4: -15.78415908003097\n",
      "[0, 1, 4]\n",
      "Steps done: 7717\n",
      "SV: [ 0.14621973 -0.99457496  1.1096114 ]\n",
      "Reward for action 10: -237.10962010796862\n",
      "[1, 4]\n",
      "Steps done: 7718\n",
      "SV: [ 0.14621973 -0.99457496  1.1096114 ]\n",
      "Reward for action 0: -15.784159080030962\n",
      "[1, 4, 0]\n",
      "Steps done: 7719\n",
      "SV: [ 0.14621973 -0.99457496  1.1096114 ]\n",
      "Reward for action 10: -237.10962010796862\n",
      "[1, 4]\n",
      "Steps done: 7720\n",
      "SV: [ 0.14621973 -0.99457496  1.1096114 ]\n",
      "Reward for action 20: -233.10962010796862\n",
      "[1, 4]\n",
      "Done flag\n",
      "Episode ended naturally\n",
      "Reached required number of episodes\n",
      "Should I do testing?\n",
      "Yes!\n",
      "Started testing\n",
      "Predicted value of all actions: tensor([ -81.6229,  -63.8853,  -92.0756,  -60.5240,  -42.8722,  -46.6100,\n",
      "         -89.4055,  -82.2106,  -42.7096,  -88.7032,  -86.8073, -151.7392,\n",
      "         -85.8354, -120.4234, -113.8015,  -92.3036,  -71.6248,  -79.6674,\n",
      "         -58.0689, -129.9399,  -72.0989], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-55.3144, -59.6154, -56.4962, -56.9645, -62.2561, -54.1457, -58.0628,\n",
      "        -60.7613, -58.3485, -60.3888, -56.5180, -60.2902, -50.5493, -57.1995,\n",
      "        -57.3656, -56.7746, -61.6161, -57.0319, -56.7572, -54.0837, -37.7125],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-30.2285, -48.3693, -39.3775, -39.6340, -38.5191, -25.6059, -36.1131,\n",
      "        -30.4050, -61.3005, -35.6926, -36.5524, -37.5787, -58.4559, -50.5320,\n",
      "        -32.2882, -40.1717, -33.7063, -25.3131, -65.3675, -61.6821, -54.2236],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.6999,  -6.2413, -11.0656, -17.6946,  -7.5678,  -5.5435, -12.7845,\n",
      "         -5.9100,  -9.8887,  -8.9465,  -4.6884,  -5.5522, -14.4088, -26.4506,\n",
      "         -1.7219, -10.9108,   0.0939,  -6.9578,  -8.5244,  -1.3619,  -2.5722],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -1.6887, -46.4233, -15.0052, -10.7627,   1.6306, -16.7892,  -9.8518,\n",
      "        -38.9808, -18.4803,  -1.0672, -24.1819, -19.2632, -15.4989, -24.9648,\n",
      "         -4.4557, -21.9050, -32.1533,  -6.6015, -15.1741, -26.4480,  -6.5861],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-50.7807, -54.7043, -54.0616, -49.1302, -50.0808, -38.7940, -51.6995,\n",
      "        -47.8147, -52.6270, -50.2260, -51.9249, -47.7195, -40.6634, -56.4814,\n",
      "        -58.2411, -55.4380, -46.6613, -54.0294, -44.9932, -45.0768, -32.5261],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -63.4344,  -84.1814,  -77.3334,  -67.1866,  -80.2622,  -64.6456,\n",
      "         -75.5324,  -59.1770,  -94.3472,  -87.1654,  -60.3265,  -82.3864,\n",
      "        -109.0070,  -83.0002,  -41.5377,  -54.5506,  -85.2675,  -40.4441,\n",
      "        -118.9248, -104.2471,  -92.1693], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-73.5719, -75.5337, -68.9684, -88.4740, -88.2238, -77.2523, -81.4848,\n",
      "        -91.6912, -78.0480, -70.5092, -77.9945, -88.6856, -78.0406, -69.9903,\n",
      "        -84.1740, -93.2919, -77.4427, -80.9085, -80.5699, -91.2403, -65.0409],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-41.5272, -45.6358, -46.0699, -36.4444, -40.5767, -29.8020, -39.7238,\n",
      "        -34.9998, -43.2959, -45.7217, -42.4276, -35.4453, -31.5314, -47.4543,\n",
      "        -45.1190, -40.8031, -38.9986, -42.7693, -35.5467, -32.8824, -22.9185],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -33.9812,  -39.0772,  -36.5510,  -29.2184,  -42.8188,  -30.6639,\n",
      "         -29.7683,  -27.2844,  -36.9670,  -61.5022,  -68.2854,  -43.9097,\n",
      "         -49.4097,  -75.8390,  -69.3033,  -73.7775,  -58.6203,  -62.7388,\n",
      "         -41.1556, -104.3460,  -50.1881], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.0268, -15.1169, -17.5692,  -9.6686,  -6.3979,  -6.8641, -17.5678,\n",
      "         -4.1389, -10.6531, -10.4830, -19.3021,  -6.2389, -10.3620, -20.1344,\n",
      "        -35.2532, -34.1612, -11.8990, -22.2582, -10.7924, -25.8563, -16.7868],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 4 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.7393, -29.4942, -34.2314, -11.8706, -39.9681, -55.2715, -42.3969,\n",
      "        -35.0945, -76.3536, -70.3272, -48.1216, -60.8321, -30.5547, -63.0692,\n",
      "        -58.6178, -44.0580, -98.6678, -45.0629, -45.5351, -77.3319, -35.0562],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-358.3193, -390.6003, -374.4026, -329.9320, -379.1734, -326.2565,\n",
      "        -358.9428, -328.9684, -373.8245, -411.7258, -397.9610, -336.1510,\n",
      "        -326.4873, -392.9645, -437.3720, -437.7034, -382.2595, -429.3989,\n",
      "        -363.5125, -429.2151, -307.0268], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-118.7203, -176.6454, -146.2030,  -57.2325, -147.1882, -134.7092,\n",
      "        -172.5560,  -83.3554, -444.3507, -230.2321, -275.5029, -250.9285,\n",
      "         -89.3238, -241.7348, -156.2076, -201.6272, -218.9443,  -86.4690,\n",
      "        -193.2408, -292.9184, -206.1218], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-64.0477, -69.1239, -63.8438, -67.3618, -82.3921, -70.8120, -63.1064,\n",
      "        -74.8681, -70.5287, -81.7551, -66.0330, -73.3967, -65.8397, -61.3340,\n",
      "        -54.9982, -57.3100, -78.8916, -62.8430, -72.0059, -65.1492, -43.7411],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([   7.7862,  -69.8342,   27.5964,   -2.8126,  -64.8598, -283.4919,\n",
      "        -143.6487, -271.9812,   10.9717, -189.0427, -215.2316, -502.9182,\n",
      "         -78.2338, -505.1623,  -95.8395,   -5.1846, -621.6580, -129.8017,\n",
      "        -174.2160, -445.5039,  -40.4334], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-188.1871, -204.7657, -191.2918, -185.3655, -219.1669, -191.6098,\n",
      "        -188.7291, -199.0185, -201.7987, -225.8706, -199.7294, -198.9996,\n",
      "        -179.7084, -193.9669, -193.6786, -195.6615, -217.6257, -205.8461,\n",
      "        -201.6076, -205.1209, -144.6475], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.2139, -25.6378, -25.2536, -22.1263, -25.5810, -22.7516, -24.5261,\n",
      "        -24.2984, -23.9293, -27.4032, -23.0451, -24.5988, -19.7437, -24.8599,\n",
      "        -22.7902, -20.4720, -28.5220, -21.2798, -23.2868, -18.5875, -11.4141],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ 1.1191e+00,  2.9970e-01,  1.9844e+01, -4.9025e+01, -6.8941e+01,\n",
      "        -1.4119e+02, -5.7090e+01, -1.0100e+02, -2.4886e+01, -1.4032e+02,\n",
      "        -9.2327e+01, -1.7318e+02, -1.0101e+02, -9.7200e+01, -1.0024e+02,\n",
      "        -1.5068e+02, -2.9115e+02, -1.8480e+02, -1.0939e+02, -3.4088e+02,\n",
      "        -1.2737e+02], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -58.5087,  -67.9044,  -60.8041,  -52.2795,  -76.0654,  -77.1828,\n",
      "         -61.7154,  -65.1629,  -62.0434,  -95.3942,  -82.1786,  -78.3942,\n",
      "         -69.3277,  -85.8548,  -75.7782,  -75.8090, -106.0148,  -77.7021,\n",
      "         -77.1995, -109.4298,  -58.8015], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-145.4441, -146.5029, -131.9720, -168.5825, -219.2097, -179.0629,\n",
      "        -138.0421, -190.8340, -172.8292, -196.0962, -153.4845, -174.9965,\n",
      "        -189.1501, -110.3091, -106.6888, -134.2132, -178.3938, -148.2884,\n",
      "        -196.6761, -180.3072, -138.3554], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -204.9486,  -160.0865,  -280.8345,  -492.5172,  -268.0918,   -64.1336,\n",
      "         -650.5800,  -494.1944,  -605.0163,  -494.0763,  -589.6525,  -607.8264,\n",
      "          -13.5436,  -504.6074,  -828.7188,  -375.9049,  -999.3480, -1259.3032,\n",
      "          -77.1177,  -280.8329,     5.5212], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -3.8002,  -4.0842,  -4.1687,  -1.6525, -14.7942, -18.5369,  -2.9047,\n",
      "         -0.6334,  -3.1583, -23.5733, -14.7338,  -2.4625, -20.1312,  -0.3239,\n",
      "        -15.1028, -22.3272, -22.1170, -11.1751, -17.3790, -38.0704, -19.8865],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -14.3874,  -12.3961,    3.9613,  -46.4817,  -43.3777,  -65.4202,\n",
      "         -44.8761,  -50.2465,   -9.5313,  -39.5782, -114.5201,  -93.6527,\n",
      "        -100.8961, -105.7130, -154.0856, -195.9801, -102.9761, -111.9864,\n",
      "         -70.3002, -293.3937, -145.1558], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -99.9621,  -80.0773, -119.1297,  -53.0697,  -71.3306,  -74.7088,\n",
      "         -88.1239,  -75.2141,  -56.6962, -149.4495,  -83.5221, -138.8996,\n",
      "        -106.9180,  -84.1674,  -92.7186,  -68.9935,  -87.5111,  -73.3229,\n",
      "         -82.8416, -111.6723,  -75.1005], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-140.5320, -143.9972, -144.0150, -142.3952, -126.9750,  -94.2174,\n",
      "        -147.5139, -111.8758, -139.5184, -116.8819, -170.4699, -112.9363,\n",
      "        -132.9322, -159.8332, -235.8888, -251.6688, -100.6728, -196.2730,\n",
      "        -126.3062, -216.7592, -158.8660], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -2.8409,  -2.8071,  -3.6781,   2.2490, -10.2690,  -1.4375,  -2.6585,\n",
      "         -4.5008,  -5.8873,   2.0241,  -2.0851, -23.2969,  -8.5455, -18.3861,\n",
      "         -7.7976, -10.2491,  -6.0797,   0.6187,  -4.6594,  -9.8420,  -2.1227],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  -1.1290,  -18.1890,  -11.6437,   13.3740,  -59.0120,  -49.3096,\n",
      "         -27.7343,  -32.6237,  -38.8318,  -59.8586, -134.0988,  -17.3944,\n",
      "        -144.7818,  -81.1793, -123.3393, -102.2843,  -68.9080,  -49.1683,\n",
      "         -38.7213, -155.4330,  -44.8819], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-24.5765, -27.1524, -25.9993, -23.1528, -33.3948, -30.6803, -24.1595,\n",
      "        -25.9520, -26.1818, -38.0916, -27.3712, -26.4977, -27.8932, -23.6785,\n",
      "        -22.8161, -23.7428, -37.1389, -23.9938, -30.5712, -30.2438, -17.8708],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  0.3915,   0.1886,   3.4360,  -5.6359,  -9.0067, -11.0827,  -4.3582,\n",
      "         -5.5992,   1.1798, -15.4114, -33.9766, -21.8533, -26.7877, -35.5069,\n",
      "        -40.6621, -51.2235, -29.4161, -28.0411, -14.5820, -85.6967, -35.7293],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -37.8857,  -54.8408,  -34.8933,  -33.9756,  -68.3571, -114.9237,\n",
      "         -67.1226,  -80.8415,  -36.3445, -104.9120,  -97.4079, -124.1824,\n",
      "         -76.1613, -121.6830,  -86.5902,  -83.5478, -181.3375,  -84.9710,\n",
      "         -95.2563, -184.3893,  -75.2132], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.8579, -19.2006, -19.4825, -17.9184, -18.4467, -15.6422, -19.5365,\n",
      "        -18.0225, -17.8625, -18.4301, -18.1645, -18.1114, -15.2482, -19.3447,\n",
      "        -20.7329, -19.5064, -19.5261, -16.9390, -16.8311, -15.7429,  -9.6803],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-27.6076, -26.1902, -30.5892, -23.9227, -20.3603, -24.7621, -34.0323,\n",
      "        -27.7450, -19.9947, -25.9723, -26.1835, -36.4458, -24.6362, -29.0357,\n",
      "        -38.1400, -33.7985, -29.1843, -26.8995, -25.2275, -32.0613, -21.5232],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-47.6503, -57.0480, -50.3513, -39.2329, -42.7064, -44.4713, -49.2723,\n",
      "        -55.5053, -49.4716, -65.5010, -64.8341, -83.0777, -37.3109, -73.9382,\n",
      "        -43.6051, -40.4440, -66.8073, -48.9121, -49.2083, -62.6627, -29.4862],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-267.7754, -303.7938, -273.4358, -250.3450, -295.8114, -267.4096,\n",
      "        -287.1295, -262.6651, -277.4784, -371.3155, -449.6507, -356.7137,\n",
      "        -323.6124, -507.0470, -501.9869, -519.5688, -418.2645, -457.0927,\n",
      "        -314.8385, -665.7770, -362.6300], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -911.2520,  -952.7667,  -952.5616,  -884.7046,  -898.2585,  -511.1340,\n",
      "         -800.2321,  -726.3985,  -983.8364,  -888.0530,  -995.9169,  -701.4938,\n",
      "         -776.2764, -1011.5496, -1111.7383, -1145.3391,  -574.9528, -1079.5337,\n",
      "         -735.0027,  -949.5508,  -725.5350], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-121.4975, -129.6355, -125.3018, -122.2829, -123.7254,  -95.2439,\n",
      "        -122.6613, -119.5821, -128.0688, -117.9865, -124.9979, -117.9374,\n",
      "        -101.7089, -131.9857, -137.5210, -135.6731, -107.9933, -133.4279,\n",
      "        -110.3460, -115.2565,  -86.2304], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-504.4135, -526.2558, -519.2581, -494.8012, -478.1408, -359.3505,\n",
      "        -510.9243, -412.2722, -512.9903, -467.3707, -595.4055, -416.0092,\n",
      "        -467.9873, -568.4365, -768.4977, -809.2667, -394.9777, -676.8270,\n",
      "        -462.5516, -716.6881, -527.8106], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.0830, -19.4701, -14.6184, -10.6407,  -9.0728, -53.2345, -44.1942,\n",
      "        -28.7953,  -3.0191, -13.7056, -19.1334, -36.7113, -14.6696, -19.2127,\n",
      "        -45.7413, -40.3528, -64.6375, -26.9300, -36.5864, -47.5056, -28.8987],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-58.7349, -59.1217, -66.0351, -47.2788, -43.5605, -38.5442, -62.7082,\n",
      "        -56.8127, -48.5383, -64.5389, -66.8542, -83.9252, -49.3354, -92.5411,\n",
      "        -77.1763, -63.2342, -63.0742, -64.4199, -46.4316, -73.9121, -39.9692],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-185.1864, -190.5465, -177.9796, -209.3941, -240.7423, -164.0933,\n",
      "        -160.7146, -203.5287, -212.5241, -218.4590, -197.4866, -190.7191,\n",
      "        -200.7337, -171.5269, -164.9961, -191.1008, -165.5689, -192.1251,\n",
      "        -189.5646, -202.0093, -144.1847], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-12.0024, -10.1915,  -7.9028, -14.0074, -16.4066,  -5.2456,  -4.2420,\n",
      "        -12.9278, -12.3133, -10.9176,  -9.9271,  -2.5447,  -6.2650, -19.4071,\n",
      "        -26.0085, -13.0352,  -3.7918, -23.2411, -22.4660, -12.4064,  -8.5603],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -51.8552,  -55.4322,  -47.5711,  -57.0284,  -86.2921,  -80.5506,\n",
      "         -49.9085,  -60.9391,  -57.7275,  -97.2198,  -87.0430,  -70.4148,\n",
      "         -90.5839,  -64.8661,  -80.0383,  -99.8180,  -96.8163,  -80.6475,\n",
      "         -83.4378, -148.0589,  -83.6172], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-31.4659, -27.6036, -40.2452, -21.7048,  -7.1981, -12.6902, -32.7719,\n",
      "        -41.2722, -27.7797, -22.7932, -14.6622, -33.2065,  -8.2792, -21.8325,\n",
      "        -20.1175,  -6.9484, -54.0382, -15.8597, -13.6393,   5.5936,  -1.3412],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-66.4177, -73.1393, -66.3541, -71.2966, -70.2122, -72.7358, -80.6827,\n",
      "        -84.3464, -67.5641, -64.1499, -68.7430, -88.5174, -56.9678, -79.8046,\n",
      "        -75.4139, -70.4751, -86.7701, -71.6238, -70.8297, -69.1679, -45.5664],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-30.1676, -31.3395, -30.6278, -33.4515, -33.3096, -26.0342, -31.7304,\n",
      "        -32.5470, -31.5151, -29.2261, -31.4836, -31.6207, -28.7008, -30.7668,\n",
      "        -34.7922, -36.1235, -28.6496, -31.0756, -29.1007, -31.3665, -21.4365],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -4.5333,  -7.5502,  -6.3269,  -2.9873,  -8.3375,  -3.8740,  -4.7565,\n",
      "         -8.3561,  -5.6272,  -9.2776, -14.0204, -12.2616,  -3.8182, -25.5941,\n",
      "        -12.0158,  -6.3937, -15.0626,  -9.0806,  -5.8326, -11.6745,   0.9945],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -97.2568,  -74.1178, -108.8638,  -66.7482,  -85.0577,  -85.7733,\n",
      "        -120.3736,  -60.8342,  -41.2713, -130.3954,  -68.3539, -128.7158,\n",
      "        -130.4756,  -88.0370,  -72.1757,  -84.6102, -119.9169,  -58.4326,\n",
      "        -109.1097, -143.2595, -112.0848], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-183.4135, -152.7383, -153.7823, -244.0557, -200.6848, -151.1678,\n",
      "        -205.9693, -115.8453, -167.7940, -136.1344, -341.2530, -115.7166,\n",
      "        -331.3477, -178.2402, -560.3077, -707.4021,  -95.3814, -410.9364,\n",
      "        -231.5216, -706.7695, -491.1037], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.1697, -25.1292, -16.3566, -14.0280, -14.9488, -10.4008, -10.7455,\n",
      "        -20.6517, -16.0093, -19.1966, -18.6060, -14.2021, -16.6656, -17.7297,\n",
      "        -13.1255, -22.3318, -18.9296, -18.7767, -10.5757, -23.7533,  -9.2938],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -39.1358,  -36.5253,  -49.5871,  -59.7745,  -28.4079,  -16.1631,\n",
      "         -47.6085,  -24.9257,  -47.1252,  -47.2803,  -32.5459,  -33.7512,\n",
      "         -46.4392, -131.9517,  -12.7107,  -34.4425,  -12.8924,  -44.1389,\n",
      "         -31.1421,  -12.6382,  -10.5732], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-106.1494, -108.2519,  -49.0821, -213.6728, -179.0580, -275.2184,\n",
      "        -227.7742, -276.2455,  -94.3714, -113.6987, -289.0764, -364.9727,\n",
      "        -271.8701, -288.5491, -378.8680, -459.3199, -348.2691, -297.2188,\n",
      "        -251.1980, -659.3806, -346.8662], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -30.7776,  -43.1784,  -30.2775,  -34.5141,  -31.9382,  -82.0215,\n",
      "         -81.7857,  -74.4979,  -23.3809,  -25.3202,  -46.4734,  -86.0694,\n",
      "         -25.8185,  -58.6238,  -70.0250,  -36.2434, -107.5339,  -43.6035,\n",
      "         -54.9991,  -46.0063,  -22.4517], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-29.3871, -29.5575, -29.2953, -34.3937, -35.9675, -21.0906, -25.9034,\n",
      "        -31.0481, -32.9897, -30.5592, -30.3751, -28.5644, -30.1052, -27.8288,\n",
      "        -27.7508, -31.1422, -21.6796, -27.6373, -26.4451, -27.2590, -18.5276],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-103.8805, -105.9361, -110.7227,  -94.3789,  -87.3881,  -91.9193,\n",
      "        -121.4703, -104.4248,  -91.1091,  -95.1137, -100.1506, -119.0028,\n",
      "         -84.9337, -107.4898, -132.8779, -118.7102, -102.7892, -110.4780,\n",
      "         -95.7772, -103.3882,  -79.3508], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -59.0002,  -64.9939,  -59.1197,  -59.1323,  -79.7438,  -72.0368,\n",
      "         -67.4050,  -63.7010,  -59.2307,  -87.4662,  -73.4907,  -78.1971,\n",
      "         -74.2142,  -84.8819,  -60.2736,  -68.7742, -103.0515,  -66.6706,\n",
      "         -78.3083, -102.9430,  -60.1341], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -5.6366,  -5.5334,  -5.3164,  -4.7028, -19.0696, -21.3507,  -3.9350,\n",
      "         -2.9241,  -5.5788, -27.1157, -18.0741,  -4.4094, -24.9509,  -0.9359,\n",
      "        -17.8384, -27.2329, -24.0516, -14.2417, -20.7341, -44.9937, -24.3738],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-125.3649, -131.2632, -131.1949, -122.6585, -119.5284,  -80.0230,\n",
      "        -121.1337, -103.0608, -130.3051, -116.8256, -140.0138, -101.9861,\n",
      "        -108.6146, -140.8567, -170.3428, -174.9509,  -90.8224, -153.7938,\n",
      "        -106.7367, -146.3964, -108.6511], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.2491, -16.7290, -22.5952, -21.4882, -12.1123,  -8.3148, -20.2395,\n",
      "         -9.0244, -18.7038, -19.2247, -11.3907,  -9.5741, -15.9327, -40.7897,\n",
      "         -7.9142, -13.1282,  -6.6068, -14.8845, -12.9656,   0.7341,  -2.1549],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-29.7046, -39.7757, -35.7614, -21.5645, -29.1470, -18.8178, -29.4365,\n",
      "        -12.7494, -37.2099, -41.2520, -33.3773, -26.0346, -30.0081, -35.6183,\n",
      "        -13.9546, -23.3569, -30.9249, -25.6194, -39.9094, -33.9326, -27.2788],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-118.9929, -109.3632, -116.0036, -118.7636, -176.7502, -111.9442,\n",
      "        -170.7012, -141.3909, -159.9958, -109.8371, -183.5538,  -84.0582,\n",
      "        -186.1718,  -71.8141, -218.0893, -145.9534,  -98.6455, -179.3016,\n",
      "        -188.6594, -123.5867, -178.1630], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-28.5803, -30.2618, -29.4083, -30.6348, -31.3555, -26.8948, -31.1391,\n",
      "        -31.2843, -29.4007, -28.9073, -29.5786, -30.9362, -26.5800, -29.5708,\n",
      "        -32.7416, -32.9094, -30.6333, -29.1452, -28.6346, -29.2317, -19.5950],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -34.7507,  -40.7915,  -31.1897,  -31.0295,  -58.7928, -109.1192,\n",
      "         -62.1461,  -49.2226,  -25.7166,  -76.1684,  -64.9952,  -59.7747,\n",
      "         -73.8513,  -29.9226,  -89.6877, -105.2179, -122.5154,  -71.7465,\n",
      "         -90.7330, -152.5509,  -93.1284], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-39.0128, -43.4812, -40.8892, -38.0243, -40.4850, -42.6776, -46.4533,\n",
      "        -43.6649, -38.6306, -42.5151, -43.2309, -47.1226, -34.6500, -47.8537,\n",
      "        -49.7650, -46.7363, -53.5506, -44.4203, -42.4263, -46.9970, -29.5399],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-64.6426, -65.6028, -64.0088, -77.0192, -83.1955, -60.0679, -67.4159,\n",
      "        -79.0756, -93.2887, -64.5035, -67.9062, -73.9332, -65.3563, -65.8370,\n",
      "        -71.7116, -73.2595, -60.0132, -63.0330, -64.7288, -64.4435, -48.4218],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-32.3243, -31.6688, -33.7520, -40.9805, -48.9781, -33.8512, -37.0734,\n",
      "        -43.9789, -68.8785, -33.6675, -36.4217, -40.0070, -35.3842, -35.7752,\n",
      "        -43.9476, -41.0692, -33.1328, -27.7323, -34.0407, -33.2368, -25.4570],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -86.5695,  -95.8826,  -80.0151, -101.8526,  -88.2461, -139.9484,\n",
      "        -139.4858, -137.2927,  -77.1306,  -65.4553,  -84.8721, -143.0806,\n",
      "         -78.3317,  -90.2624, -121.7177, -116.6493, -150.9197, -101.4219,\n",
      "        -116.8017, -114.9230,  -82.4696], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  58.5876,   96.8668,   26.4865,   76.4608,   11.2434,   21.0702,\n",
      "          59.0755,   29.9381,   15.6964,   45.4011,   -1.0400,    8.1380,\n",
      "        -229.2778,  -45.4171, -153.1528,  -90.6953,   87.9192, -429.4983,\n",
      "        -148.6909, -321.1687, -163.8035], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-189.3757, -205.2804, -193.8487, -190.7776, -198.9504, -167.1608,\n",
      "        -196.5890, -202.9488, -200.8332, -190.5040, -186.0248, -200.9180,\n",
      "        -154.4292, -202.9699, -191.0777, -181.4773, -190.2878, -197.0026,\n",
      "        -180.0127, -158.7515, -120.1748], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-42.3478, -46.5941, -42.4390, -40.8302, -49.8248, -65.1593, -55.8802,\n",
      "        -46.4060, -38.7606, -55.4819, -56.7714, -51.1982, -52.9480, -45.1530,\n",
      "        -74.8018, -80.2814, -74.6853, -61.9467, -61.6227, -92.2806, -59.4284],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-40.7447, -42.3926, -38.0204, -51.0689, -53.1360, -42.0998, -42.7966,\n",
      "        -55.5235, -45.4562, -44.0030, -45.4851, -56.3831, -45.0978, -46.5327,\n",
      "        -39.7667, -43.7554, -47.4666, -41.5847, -44.2864, -49.4052, -29.5670],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-111.9033, -124.5070, -120.1791, -105.9951, -118.7146,  -51.2669,\n",
      "         -87.3224, -101.2317, -129.9751, -137.5415, -145.7200, -122.0314,\n",
      "         -96.0323, -192.6734, -121.9725, -112.4290, -101.6090, -134.0047,\n",
      "         -84.2196, -129.2990,  -63.9676], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-199.7576, -169.6396, -153.9514, -314.9298, -274.5945, -123.3729,\n",
      "        -176.7059, -226.2340, -230.7636, -161.0023, -324.6071, -225.9039,\n",
      "        -332.5958, -240.0919, -384.3094, -514.0640,  -84.7577, -334.1623,\n",
      "        -209.4521, -549.9305, -355.4759], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-66.3448, -71.4385, -73.9960, -57.4639, -61.9452, -31.7758, -55.4827,\n",
      "        -46.6084, -71.2871, -69.6527, -69.1262, -45.7807, -48.6565, -77.1631,\n",
      "        -73.4055, -68.6486, -43.2378, -71.1778, -48.3372, -50.5949, -37.8091],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-43.5891, -51.6440, -47.5308, -31.2339, -38.5002, -38.5812, -39.7536,\n",
      "        -37.0504, -48.0271, -60.6453, -53.2887, -53.4730, -31.9076, -40.1940,\n",
      "        -36.0176, -39.1285, -44.8668, -40.0351, -45.0087, -45.3810, -30.2569],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-58.4722, -63.4781, -60.2009, -58.6529, -66.2761, -57.8954, -60.5022,\n",
      "        -63.0494, -61.8487, -66.5813, -60.0773, -62.8738, -53.4398, -60.7341,\n",
      "        -59.6833, -58.5661, -66.8140, -60.3834, -60.5414, -57.2426, -39.6174],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-25.9297, -26.6738, -28.1251, -24.7195, -24.2003, -22.0590, -28.7518,\n",
      "        -25.0144, -23.9294, -25.6672, -26.0970, -27.2392, -22.6820, -27.0644,\n",
      "        -32.7122, -31.0030, -26.0399, -26.2456, -24.0327, -26.5482, -18.0458],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -78.5199,  -87.2877,  -82.6350,  -71.6750,  -93.0485,  -82.9601,\n",
      "         -76.7523,  -79.4246,  -84.0979, -104.5642,  -83.4915,  -80.7311,\n",
      "         -74.2326,  -81.9623,  -76.2310,  -73.7872,  -99.3980,  -82.7000,\n",
      "         -85.7945,  -81.7277,  -54.8121], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  -8.6608,   -3.6643,    1.2303,  -43.5415,  -50.2039,  -53.1836,\n",
      "         -21.6728,  -54.0805,  -31.5662,  -79.8741,  -49.2691,  -82.3672,\n",
      "         -60.0905,  -45.5614,  -43.3654,  -77.7802, -124.7144, -101.0270,\n",
      "         -49.5590, -161.7380,  -59.8691], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-15.1313, -16.3936, -17.2334, -20.9818, -31.9403, -12.2304, -25.8802,\n",
      "        -20.1397, -20.1953, -19.1751, -41.3523, -12.2368, -29.3722, -10.1923,\n",
      "        -49.2591, -17.5141,  -9.3683, -19.8115, -12.0828,  -6.4346,  -7.8858],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-131.2697, -141.3115, -123.1113, -143.9355, -179.8619, -166.1406,\n",
      "        -138.0512, -152.7937, -141.3531, -192.0030, -202.6803, -177.7709,\n",
      "        -186.3496, -184.6893, -208.0062, -238.3487, -206.5162, -200.6501,\n",
      "        -177.9724, -315.9227, -183.6675], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -88.4732,  -93.4887,  -80.0716, -109.9923, -100.6589, -121.6866,\n",
      "        -122.5011, -131.6539,  -86.7079,  -72.6251,  -89.4612, -131.8945,\n",
      "         -89.5807,  -85.6962, -111.5111, -116.6163, -124.4292,  -99.8248,\n",
      "        -110.3668, -115.4691,  -83.1140], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-19.0351, -18.1546, -17.8066, -25.4124, -23.9291, -17.3397, -20.3510,\n",
      "        -22.5331, -20.0079, -17.0509, -22.0196, -21.0378, -23.6378, -16.9058,\n",
      "        -25.7906, -30.8962, -16.2171, -20.8003, -20.0825, -28.6021, -18.7189],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-123.1709, -121.9782, -138.6131,  -86.7738, -115.2334, -110.8284,\n",
      "        -116.1254,  -95.0879, -104.4216, -169.4935, -125.2421, -125.4353,\n",
      "        -122.0494, -115.6020, -140.8924, -130.1601, -129.8003, -127.9871,\n",
      "        -121.4271, -150.4506, -106.1673], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -68.8575,  -84.2896,  -69.9211,  -41.4141, -124.9518, -175.7838,\n",
      "         -83.7473,  -81.3835,  -60.5381, -210.1016, -164.4906, -142.1581,\n",
      "        -157.1923, -143.9192, -155.8151, -174.4086, -253.9320, -149.5854,\n",
      "        -162.2113, -336.0288, -167.5072], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-178.5876, -200.7900, -156.6187, -208.8813, -264.6566, -307.5716,\n",
      "        -228.9038, -269.7192, -186.7490, -281.8409, -299.2897, -328.6238,\n",
      "        -278.8600, -290.8253, -302.6894, -341.2098, -392.1669, -293.1119,\n",
      "        -292.2426, -511.1530, -278.6328], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  -5.3755,   -8.5730,    0.2801,   -2.8874,  -46.4510,  -74.1284,\n",
      "         -12.1835,  -10.3328,   -2.9669,  -79.5690,  -70.0160,  -33.8429,\n",
      "         -78.2136,  -32.1139,  -74.9908, -104.7301, -101.0044,  -62.3913,\n",
      "         -67.7695, -195.4257,  -98.2299], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-492.0732, -369.5826, -599.9690, -193.4740, -362.8689, -400.3307,\n",
      "        -527.7331, -214.3738, -167.9588, -819.5016, -381.4300, -655.3948,\n",
      "        -644.5068, -483.7921, -394.6510, -372.5998, -615.9188, -319.8251,\n",
      "        -512.7949, -769.3110, -554.0191], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-47.8083, -50.7010, -48.3698, -50.9132, -54.0794, -45.0364, -49.8118,\n",
      "        -52.2407, -50.5062, -50.4622, -49.6994, -51.3351, -45.2464, -48.8216,\n",
      "        -51.7808, -52.9701, -50.1011, -50.0003, -48.4418, -49.5737, -34.4872],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -5.2518,  -7.5805,  -8.7310,  -8.3199, -18.1303,  -2.1989,  -9.2725,\n",
      "        -22.9847,  -9.1645,  -7.9764,  -7.4991,  -7.4126, -11.5166, -12.9580,\n",
      "        -16.8376, -17.3358, -16.7914,  -7.4189,   1.0666, -16.4980, -10.7336],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -30.5895,  -25.3000,  -56.2391, -152.2782,  -52.2058,  -61.2566,\n",
      "         -39.2658, -127.0091,  -80.7467, -272.0574,  -11.6475, -169.0089,\n",
      "         -53.9042,  -35.9017,   -5.5907,  -46.8291, -236.7799, -284.7876,\n",
      "         -87.3354,  -78.1435,  -14.2009], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-32.5138, -31.9181, -33.3134, -33.5601, -30.0130, -32.9968, -41.1181,\n",
      "        -51.0383, -25.4802, -40.1128, -43.2999, -76.9511, -36.5282, -67.0344,\n",
      "        -41.2212, -32.6934, -55.3619, -34.8841, -32.6294, -58.8171, -22.8383],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-34.7876, -38.3332, -35.7927, -36.0616, -37.8392, -34.3817, -39.4117,\n",
      "        -42.0521, -35.3775, -41.1363, -45.4796, -51.0523, -35.1960, -56.0844,\n",
      "        -46.7385, -44.4341, -50.1052, -42.6238, -36.7239, -54.1117, -27.7103],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-26.8388, -23.2083, -30.5784, -41.3024, -37.4679, -10.7731, -32.2255,\n",
      "        -20.3776, -60.6245, -47.9851, -39.1239, -21.2422, -37.0200, -54.6785,\n",
      "        -55.2149, -64.0922, -22.5848, -45.3718, -24.6735, -61.1292, -42.3013],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-3.5475, -3.9159, -5.1797, -3.0311, -3.2723, -2.8305, -4.5331, -2.6657,\n",
      "        -2.7309, -4.4549, -3.4607, -2.9461, -2.7111, -4.1985, -4.8042, -3.4922,\n",
      "        -5.5670, -0.9320, -2.8338, -1.1842,  1.3336], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-22.2857, -36.8286, -27.5670, -17.3246, -18.7364, -21.4082, -23.5151,\n",
      "        -21.5655, -44.3153, -36.1701, -37.2574, -41.4041, -22.9967, -28.8876,\n",
      "        -15.9421, -24.0122, -28.0249, -17.8247, -38.3648, -39.8633, -28.0344],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  36.2082,  -73.9047,    9.2293,  -78.7160,  -64.7414,  -34.9258,\n",
      "          -6.6458, -108.0396, -230.8985,   -8.6686,  -90.9523,  -41.6067,\n",
      "         -86.7064,  -47.8618, -100.1501, -218.6113,  -54.8467, -115.3184,\n",
      "         -23.5493, -225.4951, -100.2604], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-352.4301, -384.4702, -363.5524, -318.1089, -429.7767, -397.1608,\n",
      "        -452.4730, -296.7390, -308.8055, -478.7815, -389.9716, -409.4640,\n",
      "        -420.2057, -524.1398, -338.7086, -405.0917, -623.1865, -378.9338,\n",
      "        -467.9438, -593.5175, -397.8020], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-149.9939, -159.7240, -133.7751, -182.6812, -202.5402, -174.1139,\n",
      "        -164.1218, -190.2523, -163.3618, -204.4469, -266.8108, -239.1277,\n",
      "        -228.1403, -272.4303, -282.7338, -324.8764, -239.1794, -261.3279,\n",
      "        -199.2479, -436.1029, -237.3834], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-31.9568, -14.7392, -40.7653,  -1.1060, -12.4590, -27.1539, -43.9924,\n",
      "         -4.2786, -28.2774, -57.7562, -44.9312, -39.7568, -29.5592, -49.9819,\n",
      "        -21.3821, -36.3330, -59.8943, -27.1443, -37.1827, -84.0729, -39.5220],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -62.6961,  -57.6117,  -59.2367,  -95.0256,  -90.4908,  -33.1365,\n",
      "         -48.8699,  -73.5000, -109.4605,  -89.6646,  -96.4877,  -70.4493,\n",
      "         -86.4219,  -96.5371, -104.0589, -122.8923,  -34.9938,  -99.1749,\n",
      "         -55.1282, -128.3801,  -77.6399], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -4.9324,  -3.8321,  -4.6340,  -8.6638,  -7.6905,  -4.6210,  -5.6668,\n",
      "         -6.2347,  -4.7931,  -4.3007,  -6.8424,  -5.4622,  -9.1008,  -3.1527,\n",
      "         -9.1156, -12.2008,  -4.0321,  -4.3831,  -5.7477, -11.0049,  -5.5471],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -36.6549,   -8.2976,  -42.5603,  -62.0226,  -17.0584,  -21.8768,\n",
      "         -15.0939,   -8.8417,  -39.5145,  -31.0424, -103.1569, -107.3378,\n",
      "          18.2632, -136.9944,  -40.1923,  -95.4272,  -75.2555,  -69.3098,\n",
      "         -92.3773, -135.9872, -108.7600], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-58.3549, -59.6464, -58.1836, -67.5979, -71.6675, -51.3900, -59.6411,\n",
      "        -67.6987, -77.2744, -57.9769, -61.1511, -63.7769, -57.5467, -59.5453,\n",
      "        -64.5067, -66.4280, -52.5537, -58.2912, -56.9245, -58.1682, -43.0128],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-401.4081, -410.4702, -419.4167, -355.7003, -430.4191, -334.4624,\n",
      "        -368.4292, -361.1986, -390.3232, -558.5031, -558.8553, -492.4785,\n",
      "        -475.2803, -587.0927, -585.5355, -604.0519, -467.7021, -550.3146,\n",
      "        -414.1674, -777.8860, -454.2368], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-128.0685, -150.7072, -133.5442, -111.9733, -160.5319, -148.6973,\n",
      "        -128.5837, -136.3059, -138.9230, -212.0066, -202.9978, -180.0836,\n",
      "        -153.4036, -233.1547, -188.0067, -186.7316, -230.3849, -192.4139,\n",
      "        -160.4223, -277.0111, -140.2758], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -12.4396,  -18.0178,  -12.6575,   -9.8386,  -18.8926,  -32.1763,\n",
      "         -23.1344,  -18.8576,   -9.7854,  -37.2415,  -50.5527,  -41.7329,\n",
      "         -33.3753,  -61.7669,  -59.1630,  -61.7161,  -64.2410,  -45.9918,\n",
      "         -31.8657, -103.3746,  -43.6863], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  6.6865, -26.8277,   4.2338,  -4.3488,   7.0694, -61.9215, -38.3508,\n",
      "        -47.4906,  11.7505,  16.3383,  -2.4283, -42.1887,  -1.4165,  -5.5609,\n",
      "         -8.3611, -13.8054, -65.5290,   0.9864, -29.3281, -22.2658,  -5.4858],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -91.7981,  -94.4061,  -93.7845,  -93.1182,  -96.6343,  -80.7681,\n",
      "         -93.9993,  -97.8699,  -92.4485,  -94.9342,  -89.5822, -101.5188,\n",
      "         -83.6088,  -90.5501,  -94.2987,  -92.8422,  -87.3841,  -92.0896,\n",
      "         -87.7877,  -86.8892,  -64.2795], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-19.2360, -20.2686, -20.9385, -21.9478, -16.8572, -20.3784, -21.0502,\n",
      "        -15.7052, -12.5018, -15.3251, -27.0111, -25.4855,  -9.7633, -28.0028,\n",
      "        -38.2267, -38.7849, -31.9861, -24.0368, -38.8599, -27.0024, -42.3912],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-21.8808, -16.5137, -29.0842, -24.3065, -11.3130,  -6.4037, -25.3840,\n",
      "         -8.2384, -17.0431, -23.6065,  -8.7823, -15.3608, -22.9093, -44.5403,\n",
      "         -5.3302, -12.0496,  -1.9402, -12.2939, -14.0435,   1.7424,  -5.1274],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-21.7049, -34.9958, -15.7484, -23.0827, -15.0609, -14.3174, -17.1699,\n",
      "        -30.9878, -33.9484, -37.8350, -70.7099, -73.3012, -21.5275, -53.2209,\n",
      "        -25.9355, -50.1579, -27.7890, -36.0193, -30.4110, -88.2589, -37.7258],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-112.2455, -125.3149, -115.8927, -105.0042, -136.0201, -119.1023,\n",
      "        -111.3778, -114.8591, -120.3853, -156.4110, -140.3208, -128.7051,\n",
      "        -119.4550, -146.2410, -131.7449, -134.0820, -156.0588, -137.8031,\n",
      "        -128.3983, -167.5001, -100.6874], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -87.0494, -114.3237,  -89.9731,  -87.7275,  -87.8254,  -79.9513,\n",
      "        -109.5226, -138.2764,  -96.9283, -125.0685, -176.1782, -217.0851,\n",
      "         -83.5387, -302.8134, -153.0024, -120.4223, -206.2653, -152.9189,\n",
      "         -90.3470, -221.8814,  -66.3111], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-40.2446, -45.5881, -52.1097, -21.1293, -18.7185, -19.9229, -41.7059,\n",
      "        -28.2195, -36.9082, -39.5585, -33.0258, -29.3966, -10.9497, -50.5427,\n",
      "        -43.6147, -25.3556, -48.9382, -36.9146, -23.9330,  -8.1103,  -8.1710],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-13.3939, -17.0183, -13.5341, -13.5528, -18.6609, -29.3314, -22.3976,\n",
      "        -23.0452, -11.8624, -24.7652, -25.4493, -31.9479, -20.3292, -30.6352,\n",
      "        -27.4478, -26.3039, -44.2367, -22.2128, -24.9947, -43.4382, -18.4001],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -180.8124,  -179.3712,  -119.6700,  -267.8864,  -316.7943,  -380.1058,\n",
      "         -267.0819,  -257.3828,  -169.8115,  -331.2381,  -545.7285,  -387.2650,\n",
      "         -510.3521,  -426.9286,  -706.2259,  -886.7083,  -490.0516,  -564.9465,\n",
      "         -411.2934, -1234.8385,  -684.3430], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-359.8004, -391.4772, -375.7960, -339.1963, -390.3646, -299.4201,\n",
      "        -338.0364, -342.2252, -389.2608, -411.2921, -373.0491, -339.7190,\n",
      "        -309.0414, -387.2180, -363.4135, -353.5236, -350.9238, -388.4841,\n",
      "        -341.1140, -333.5690, -245.1528], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -37.0676,  -56.0686,  -27.6406,  -63.4667,  -54.6839,  -26.5348,\n",
      "         -52.2907, -104.7328,  -52.3805,  -83.5716, -178.8516, -206.8147,\n",
      "         -84.5044, -314.2780, -146.2850, -137.8723, -167.5423, -139.2662,\n",
      "         -49.7841, -295.8525,  -78.6083], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-70.5788, -66.6237, -77.0688, -59.8957, -61.4905, -56.6215, -71.4574,\n",
      "        -70.7541, -58.5078, -77.7722, -58.4940, -88.5268, -61.5193, -64.5437,\n",
      "        -63.3195, -52.1005, -62.5155, -56.7586, -60.2296, -53.3020, -40.2261],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -6.2439,  -4.9455,  -5.2560, -11.7662, -12.4196,  -5.8053,  -4.7900,\n",
      "        -10.2023,  -7.8469,  -7.4129,  -6.5204,  -8.3346, -11.1797,  -2.5154,\n",
      "         -2.9819,  -6.5798,  -4.3098,  -2.1266,  -6.9368,  -6.8013,  -2.6418],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-357.7484, -391.8421, -360.5634, -349.0706, -403.7561, -367.8403,\n",
      "        -377.3998, -359.8471, -372.4853, -445.0265, -480.3057, -408.8860,\n",
      "        -392.2389, -483.7341, -532.6894, -558.6785, -466.4709, -503.9867,\n",
      "        -407.2086, -644.5011, -401.7446], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-230.0098, -249.0298, -237.1407, -224.4324, -232.0822, -205.3575,\n",
      "        -245.2892, -229.4872, -236.9188, -230.7325, -242.6518, -231.9937,\n",
      "        -195.5896, -253.2371, -278.5808, -273.1609, -235.4238, -266.1857,\n",
      "        -224.7220, -246.6598, -182.1125], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.6497, -13.0342,  -9.0129, -31.1692, -29.5641, -10.0714, -11.6766,\n",
      "        -29.8882, -20.7656, -20.3705, -37.0295, -38.9411, -34.7021, -42.8089,\n",
      "        -27.9860, -38.8360, -20.6505, -26.9418, -17.9807, -61.8060, -24.6239],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-3.0181, -3.2411, -4.4591, -2.8752, -3.0009, -2.6656, -4.0955, -2.4816,\n",
      "        -2.1969, -3.8077, -3.0278, -2.6982, -2.6856, -3.3976, -4.3773, -3.3938,\n",
      "        -5.0903, -0.4492, -2.5781, -1.3232,  1.2812], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  48.2241,  -29.9081,  -75.9482,    2.3082,  -33.8893, -328.2112,\n",
      "         -34.1710, -112.9131,   41.6985,  -82.5477,  -85.0520, -324.9927,\n",
      "        -383.4035,   56.8731,  -15.8842,  -53.0184, -371.5564,  106.0200,\n",
      "        -433.9272, -105.7937, -148.2770], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-233.1217, -251.3183, -245.4200, -217.8953, -236.3509, -178.6267,\n",
      "        -224.4809, -205.3686, -245.8626, -248.3951, -251.4282, -205.9027,\n",
      "        -198.8121, -258.8227, -276.6378, -273.8675, -210.2395, -270.4760,\n",
      "        -213.6362, -245.3232, -180.5249], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-384.3463, -382.8309, -395.5682, -364.8555, -414.9700, -338.5013,\n",
      "        -361.4817, -387.3082, -377.6334, -454.0640, -374.3256, -424.0543,\n",
      "        -380.0738, -351.3784, -365.6636, -365.2870, -356.6146, -378.0476,\n",
      "        -373.5126, -391.6877, -290.7451], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-28.5573, -31.1418, -30.5535, -28.1511, -29.4786, -26.1457, -31.1665,\n",
      "        -29.9463, -29.0751, -29.5802, -28.2597, -30.1560, -23.2901, -31.0675,\n",
      "        -30.8378, -28.3352, -31.8720, -27.8245, -27.4015, -23.6593, -15.8009],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -42.6617,  -47.8053,  -37.4950,  -48.0562,  -72.9619,  -78.2487,\n",
      "         -49.5581,  -55.8238,  -45.0879,  -89.9871,  -98.0881,  -79.1220,\n",
      "         -89.9798,  -87.5696, -101.4997, -121.3447, -109.8228,  -91.7514,\n",
      "         -80.8093, -188.0101,  -96.7552], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -15.9284, -105.6111,   -7.6080,   23.5103,  -40.4294,   -8.6359,\n",
      "        -153.7094,  -39.4060, -140.6061,  -41.1650, -434.2409,  -94.7678,\n",
      "         -66.8942,  109.8090, -299.3968,  -79.4515,   84.2541, -119.5823,\n",
      "         -97.8328,  -38.8003, -132.7681], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-44.0181, -50.5138, -48.0918, -38.8514, -37.3462, -48.1091, -57.6861,\n",
      "        -48.0841, -40.7559, -40.6377, -42.3041, -51.5556, -28.6822, -52.8533,\n",
      "        -56.0597, -46.3180, -60.8615, -47.4232, -44.9135, -37.2348, -26.4645],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-228.5717, -236.7402, -244.1698, -212.2061, -201.7000, -109.5134,\n",
      "        -207.1308, -148.9152, -235.9812, -207.7574, -271.1425, -148.2786,\n",
      "        -197.4827, -267.3060, -350.0730, -365.0499, -127.4637, -305.6490,\n",
      "        -178.2303, -299.6723, -223.7970], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-33.5860, -39.2554, -41.1723, -18.1832, -34.5148, -29.1861, -28.1076,\n",
      "        -15.2266, -33.8362, -55.8499, -42.5165, -18.9367, -30.4052, -41.1936,\n",
      "        -45.4940, -41.9113, -44.0858, -42.2130, -34.6199, -45.7960, -27.8635],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-74.3192, -80.3312, -72.6463, -82.9760, -83.1639, -83.8036, -88.2432,\n",
      "        -97.2135, -77.0374, -70.9854, -70.1695, -96.3838, -65.0679, -75.1397,\n",
      "        -72.8938, -70.0421, -91.4613, -73.1022, -80.2964, -65.1948, -47.5689],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.4381, -12.3879, -12.6049, -32.5664, -25.6744, -13.0658, -18.2071,\n",
      "        -21.3910, -19.3968, -10.9502, -26.9729, -18.5486, -33.3721, -11.8233,\n",
      "        -38.3689, -54.4378,  -6.2357, -29.9103, -20.1573, -53.0157, -34.9652],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-25.2159, -35.7199, -34.7704, -27.6162, -29.1687, -22.0701, -21.4620,\n",
      "        -23.7936, -34.7846, -34.5857, -37.5748, -21.9740, -31.3291, -38.4934,\n",
      "        -22.1405, -34.2748, -25.5763, -31.8973, -30.0931, -42.1911, -19.3561],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-65.1742, -62.2104, -66.1658, -66.5303, -66.6854, -60.9239, -69.5007,\n",
      "        -80.7402, -58.2425, -72.0850, -62.7388, -98.5299, -66.2281, -70.9395,\n",
      "        -61.1575, -56.0856, -69.9032, -57.7998, -62.7978, -69.1906, -43.4325],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -79.7191,  -90.0673,  -86.9479,  -66.5003,  -88.1897,  -74.6693,\n",
      "         -74.3128,  -71.7370,  -85.4181, -106.5355,  -84.5020,  -74.6284,\n",
      "         -67.9254,  -87.0714,  -76.6290,  -70.4920,  -94.3220,  -83.3424,\n",
      "         -80.7618,  -73.7795,  -49.8567], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -12.4023,  -10.9160,    9.2081,  -38.5386,  -72.9529, -121.6162,\n",
      "         -73.8143,  -36.5917,   10.7228,  -82.3183, -120.7758, -102.4496,\n",
      "        -149.5925, -112.9511, -144.8719, -218.6307, -196.9420, -111.7744,\n",
      "        -127.0447, -385.9040, -205.7732], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -26.2169,  -21.4693,  -19.6217,  -58.1011,  -49.2623,  -42.6953,\n",
      "         -37.4736,  -51.2527,  -43.6466,  -60.2481,  -52.0441,  -61.9733,\n",
      "         -59.9216,  -36.4184,  -69.3860, -104.6092,  -84.1029, -105.0259,\n",
      "         -47.7378, -139.5440,  -69.1783], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -94.6402, -104.0915,  -89.3729, -103.2028, -105.5056, -151.3915,\n",
      "        -138.7659, -129.8634,  -86.3094,  -96.5588, -109.0066, -137.4530,\n",
      "        -103.1122,  -97.3779, -147.8831, -152.8130, -165.0650, -125.5875,\n",
      "        -134.1540, -166.7538, -114.1215], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-55.8322, -59.6309, -58.1130, -56.0973, -58.9465, -45.9219, -56.1653,\n",
      "        -55.1229, -58.7572, -57.6510, -58.0288, -54.4728, -48.7436, -59.8071,\n",
      "        -62.3925, -61.8101, -53.0481, -59.6244, -52.2641, -54.1472, -38.5872],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-41.0007, -39.6307, -44.4707, -36.3112, -32.5963, -38.0616, -49.5925,\n",
      "        -49.6837, -30.9105, -43.8543, -41.0618, -69.4195, -37.2073, -55.8861,\n",
      "        -47.8285, -37.8476, -51.6940, -38.2038, -37.6223, -47.8425, -26.3116],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-146.7111,    7.4002, -172.4673,   -6.9754,  -59.8056,  -77.7319,\n",
      "        -299.5941,  -18.1357, -278.5334, -164.7197, -355.1441,  -99.4794,\n",
      "         -89.1100, -172.3944,  -98.9668, -123.2233, -164.0952,  -92.4504,\n",
      "        -132.7215, -314.6084, -135.0247], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-22.4153, -21.2908, -20.2294, -30.9386, -29.1251, -22.0364, -24.4255,\n",
      "        -28.5013, -23.8339, -19.8105, -25.6109, -26.5459, -28.4813, -19.1904,\n",
      "        -29.2288, -35.7285, -19.9172, -24.4816, -24.6279, -34.0432, -22.6800],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-178.1209, -233.8918, -203.1082, -119.7191, -251.9959, -382.4799,\n",
      "        -204.0965, -247.7177, -138.4037, -396.6953, -220.5704, -309.8871,\n",
      "        -302.2669, -122.8362, -158.3723, -189.7234, -408.9371, -180.2062,\n",
      "        -320.3984, -391.9161, -234.4615], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -51.1115,  -63.1643,  -64.2031,  -13.0128,  -68.9813,  -75.7976,\n",
      "         -38.7171,  -10.5332,  -50.3452, -129.6631,  -86.0992,  -22.3743,\n",
      "         -72.1739,  -60.4404,  -88.9136,  -92.8336, -107.6515,  -85.8662,\n",
      "         -80.3121, -135.9421,  -79.9077], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -75.3890, -106.2026,  -90.1197,  -64.6552,  -77.4201,  -73.9172,\n",
      "         -80.9897,  -66.1900, -110.2433, -110.1490,  -88.5285, -106.7273,\n",
      "         -99.3440,  -90.4614,  -46.8328,  -63.2293,  -93.5422,  -54.8171,\n",
      "        -128.2642, -115.6209,  -96.2898], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -98.3857, -107.4424, -101.2850,  -95.9196, -112.7870,  -98.5165,\n",
      "         -99.2660, -103.3590, -104.9082, -117.1889, -102.8845, -103.5578,\n",
      "         -91.1473, -102.4708, -100.0181,  -98.7408, -113.9044, -104.7878,\n",
      "        -103.4870, -100.8922,  -70.4847], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-225.7380, -249.6807, -223.4601, -233.2274, -284.6310, -239.4304,\n",
      "        -231.3565, -260.9280, -246.2675, -300.1254, -277.4784, -294.6275,\n",
      "        -245.6859, -312.9511, -236.5120, -246.6585, -319.1667, -265.3347,\n",
      "        -259.2689, -328.7688, -193.1742], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.0156, -20.4600, -18.7870, -15.6374, -13.9414, -27.0107, -28.8863,\n",
      "        -24.2821, -13.5827, -14.9038, -15.4978, -27.1426, -10.5471, -21.5642,\n",
      "        -23.5998, -17.5032, -34.8399, -16.6805, -21.2795, -14.9310,  -8.5541],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -539.4598,  -421.2489,  -629.9954,  -274.3862,  -412.4423,  -483.8368,\n",
      "         -626.4569,  -316.8563,  -206.2742,  -858.9318,  -530.2767,  -817.1351,\n",
      "         -746.6444,  -669.1626,  -602.5625,  -600.8148,  -756.5503,  -476.0393,\n",
      "         -599.6432, -1082.2361,  -708.2407], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.6704, -14.8490, -17.5420, -30.8231, -14.3477, -45.2089, -27.5855,\n",
      "        -15.3048, -33.3494, -33.5420, -14.3190, -16.5784, -18.4663, -46.7247,\n",
      "         -6.8468, -23.6911, -11.6153, -36.8378, -18.4606,  -6.2328,  -5.8695],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-22.0797, -24.7032, -26.9056, -12.6712, -21.0564, -21.9476, -25.7616,\n",
      "        -11.7604, -17.1389, -36.0188, -30.5684, -22.6603, -24.7102, -37.7179,\n",
      "        -34.9697, -33.8527, -39.8913, -28.5340, -26.2197, -45.1360, -24.7134],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -28.9211,  -30.7687,  -28.2564,  -29.8335,  -41.1434,  -36.3812,\n",
      "         -29.1514,  -26.0063,  -29.8375,  -50.5077,  -58.1071,  -35.9901,\n",
      "         -51.2826,  -49.6370,  -66.1794,  -77.8001,  -50.9198,  -56.0002,\n",
      "         -42.9511, -103.3130,  -56.4030], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-194.0455, -210.0215, -124.6843, -308.1069, -375.9329, -457.4728,\n",
      "        -292.0893, -409.9147, -209.5373, -352.8267, -443.3916, -522.1898,\n",
      "        -451.8142, -404.8029, -442.7473, -556.8622, -577.8673, -417.0054,\n",
      "        -424.5859, -913.0931, -471.2556], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-26.3541, -31.6967, -30.4465, -17.2304, -30.9934, -38.7655, -38.4346,\n",
      "        -19.3654, -20.3069, -44.6637, -33.0356, -31.5135, -31.2598, -45.0745,\n",
      "        -32.8699, -33.8976, -64.9734, -30.6642, -40.5180, -52.9211, -30.5649],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.3231, -30.7518, -28.3613, -22.7086, -27.4339, -23.9065, -25.2234,\n",
      "        -23.2876, -32.9167, -33.1258, -25.1270, -30.0645, -32.2708, -28.9387,\n",
      "        -18.9948, -20.1152, -30.8866, -17.0483, -37.1828, -31.9280, -24.3676],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -38.7499,  -29.7824,  -41.4866,  -27.7692,  -36.9520,  -55.1000,\n",
      "         -46.3161,  -40.7428,  -17.7719,  -66.1077,  -50.4674,  -72.8825,\n",
      "         -65.0746,  -38.8675,  -66.1743,  -67.9162,  -63.6022,  -46.6371,\n",
      "         -53.0317, -107.1414,  -63.0388], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-139.9645, -171.0835, -168.9440, -125.1919, -139.5486,  -97.0378,\n",
      "        -128.1763, -118.9661, -198.0353, -204.3127, -132.8039, -144.3383,\n",
      "        -124.5390, -160.7253, -111.1819, -108.2586, -166.1134, -171.0577,\n",
      "        -158.1641, -129.4179,  -99.6637], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -28.4638,  -35.9720,  -11.8687,  -44.6182,  -92.0382, -158.2851,\n",
      "         -99.0229,  -72.9956,   -6.0676, -104.8144,  -92.1480, -132.3580,\n",
      "        -130.8574, -107.9777,  -80.1145, -127.7891, -241.8921,  -77.2595,\n",
      "        -144.0168, -285.1946, -149.8771], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 2 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-270.0689, -298.3354, -252.2476, -319.9682, -325.2207, -319.1484,\n",
      "        -321.2640, -403.1406, -293.6099, -280.1818, -289.3224, -430.2522,\n",
      "        -255.1277, -354.8711, -253.0934, -240.0122, -385.2407, -284.0811,\n",
      "        -304.3405, -293.7162, -175.0859], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([   0.8205,   -9.0876,    6.2407,   -3.5711,   -0.9236,  -98.4533,\n",
      "         -61.7771,  -60.0382,   18.5667,    2.7662,   10.0248,  -72.0533,\n",
      "           0.8515,    2.1391,  -11.2670,    4.9220, -114.1840,    2.7777,\n",
      "         -50.2191,  -19.7094,   -7.0967], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -33.5025,  -31.8669,  -42.8698,  -47.3877,  -23.7079,  -14.2198,\n",
      "         -39.8641,  -19.7164,  -39.1529,  -39.4126,  -25.8368,  -25.1305,\n",
      "         -36.0801, -101.6640,  -12.0505,  -27.4321,  -11.2382,  -35.2878,\n",
      "         -25.8736,   -6.1987,   -7.5753], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-47.8566, -48.8338, -45.5517, -56.3701, -58.6655, -52.3098, -52.4932,\n",
      "        -59.5052, -49.9444, -49.9174, -50.4645, -58.6831, -52.5032, -44.0535,\n",
      "        -52.7612, -58.3012, -53.4054, -50.3746, -53.6746, -59.6303, -41.1851],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-492.4312, -557.7714, -511.0907, -423.4476, -616.3902, -599.0939,\n",
      "        -571.9603, -439.4660, -474.7987, -751.3715, -642.7075, -577.4836,\n",
      "        -598.0621, -740.3613, -600.6732, -663.8409, -885.5364, -636.2809,\n",
      "        -663.7454, -951.9218, -586.4209], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -40.9340,  -47.6695,  -39.5273,  -28.4800,  -58.0431,  -98.6539,\n",
      "         -65.0371,  -23.1369,  -27.6997, -102.3599, -138.7496,  -61.9923,\n",
      "        -113.6797, -105.7395, -200.9767, -233.9711, -142.8437, -150.3150,\n",
      "        -103.8301, -320.0595, -178.0727], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-44.3401, -41.1212, -49.9694, -29.9924, -52.5297, -36.6142, -37.7578,\n",
      "        -29.0428, -59.1605, -75.6752, -57.3542, -34.2369, -40.7800, -49.3104,\n",
      "        -26.5084, -33.4035, -57.1021, -43.8921, -45.9501, -55.5373, -27.5352],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-140.8944, -146.4663, -144.0688, -142.3822, -144.0208, -120.0347,\n",
      "        -145.6610, -149.4226, -142.8638, -139.2538, -136.3724, -153.6925,\n",
      "        -121.5835, -143.2304, -146.1199, -141.4340, -130.8750, -143.5200,\n",
      "        -131.3633, -126.4184,  -96.0579], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-71.7085, -77.0447, -73.4045, -73.7934, -75.6726, -64.5469, -76.2342,\n",
      "        -77.0199, -74.9623, -71.2256, -72.2178, -76.3654, -61.2481, -76.3120,\n",
      "        -77.8210, -75.8257, -73.2232, -75.4718, -69.4182, -66.6030, -48.4636],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -90.6308,  -86.1520,  -58.9914, -133.6640, -208.7181, -203.2084,\n",
      "         -91.4024, -125.5528, -103.4350, -221.7585, -244.3149, -162.7672,\n",
      "        -275.3963, -137.9774, -253.0517, -356.6645, -231.3836, -231.6762,\n",
      "        -216.5747, -545.3388, -305.7921], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -60.1863,  -81.3282,  -74.8362, -102.7677,  -57.2797,  -26.4544,\n",
      "         -35.4947,  -95.3801, -111.2477, -127.2608,  -82.2601, -111.9565,\n",
      "        -110.6931, -102.2516,  -77.2251,  -81.6063,  -11.5172,  -96.1959,\n",
      "         -92.8814, -128.4955,  -90.3467], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-156.3446, -146.0304, -146.6535, -162.4814, -186.1858, -153.9612,\n",
      "        -179.2569, -188.7738, -181.0323, -140.8799, -150.0399, -160.4806,\n",
      "        -181.2401, -119.2035, -156.5240, -167.7399, -149.7451, -182.8987,\n",
      "        -218.4891, -167.8708, -185.0765], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-137.1419, -150.3173, -137.2687, -153.3430, -172.1591, -132.4475,\n",
      "        -158.5176, -168.2154, -155.8658, -140.3700, -162.4845, -152.0846,\n",
      "        -134.0399, -136.8407, -165.3096, -124.5814, -140.3613, -145.5457,\n",
      "        -134.4473, -102.3945,  -81.8150], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-43.6163, -48.8004, -43.4355, -47.2331, -43.0152, -55.0652, -61.2123,\n",
      "        -58.9963, -41.1013, -39.1011, -49.0281, -65.2523, -38.4464, -58.4881,\n",
      "        -62.5317, -58.4126, -68.0905, -52.6721, -50.8044, -59.4391, -36.7296],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-2.2443, -2.2545, -3.4055, -2.6473, -2.6040, -2.4245, -3.4557, -2.2124,\n",
      "        -1.4162, -2.8614, -2.3949, -2.3359, -2.6482, -2.2266, -3.7532, -3.2498,\n",
      "        -4.3932,  0.2567, -2.2042, -1.5264,  1.2046], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -74.5845,  -52.8603,  -70.0514,  -28.0345,  -25.6240,  -37.4910,\n",
      "         -43.3490,  -70.0378, -197.2004,   47.4981,  -10.1694,  -54.7169,\n",
      "         -58.7483, -106.9959,  -61.3887, -113.3718,  -78.8115,  -73.4079,\n",
      "        -523.5906, -369.8731, -112.6027], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([   9.6077,    4.6191,   19.4239,  -14.1637,    1.9577,   10.9091,\n",
      "          -5.4802,  -27.4607,    6.2417,   -9.1917,  -92.6993,  -92.4319,\n",
      "         -40.5556, -161.0603,  -95.0952, -103.9359,  -67.0678,  -71.2305,\n",
      "          -5.2921, -205.2581,  -59.8478], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -89.6615,  -37.8190,  -99.1250,  -56.2865,  -98.2877,  -61.9170,\n",
      "        -143.4851,  -32.3101, -162.4294, -121.3127, -241.0422,  -61.3880,\n",
      "         -72.4368, -122.7578, -120.8383, -114.5343, -113.6999,  -86.5047,\n",
      "        -103.9050, -179.5181, -107.7364], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.2701, -10.7011, -14.9696, -12.4176,  -9.8288, -10.4195,  -4.6784,\n",
      "        -20.5390, -11.1075, -11.2505,  -6.1887,  -5.6028, -30.1817, -14.7664,\n",
      "         -3.1736, -17.4102,  -7.5994,  -8.2311, -19.0350, -12.0954,   2.6504],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-136.1732, -208.7857, -165.1811, -110.6544, -127.4542, -130.3533,\n",
      "        -145.9727, -114.9439, -218.2606, -205.9004, -176.9964, -214.5688,\n",
      "        -181.3026, -168.9549,  -70.2699, -114.6560, -164.5921,  -94.4449,\n",
      "        -253.0602, -232.8323, -197.1597], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-172.3993, -191.2175, -184.8517, -148.0189, -206.0608, -161.8909,\n",
      "        -191.2891, -136.0420, -166.2527, -233.3828, -169.7438, -169.4125,\n",
      "        -173.2963, -232.5976, -122.8628, -136.8427, -256.0357, -159.0979,\n",
      "        -198.0506, -191.1578, -134.9327], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-27.6290, -29.0818, -30.4364, -26.4287, -28.4387, -14.6153, -23.2618,\n",
      "        -21.7921, -29.9116, -29.5799, -28.6742, -20.8039, -22.8615, -30.2551,\n",
      "        -29.0142, -28.3419, -18.8923, -27.0391, -21.1163, -21.2872, -14.4249],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -234.3078,  -205.3789,  -131.8255,  -442.9734,  -388.9251,  -347.1738,\n",
      "         -323.0359,  -411.6780,  -250.2817,  -246.9762,  -587.4955,  -522.6456,\n",
      "         -573.7825,  -497.4273,  -733.4323,  -954.3882,  -402.6690,  -597.0694,\n",
      "         -411.1454, -1250.8849,  -701.1360], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-18.6101, -19.7734, -20.1722, -18.9594, -19.0229, -15.2866, -20.0289,\n",
      "        -18.2624, -18.6803, -18.5462, -19.2765, -18.2219, -16.1494, -20.1580,\n",
      "        -22.5128, -21.7454, -18.8252, -18.2726, -17.1020, -17.3863, -10.9720],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-36.0466, -39.1947, -42.0076, -28.2037, -27.6672, -16.9057, -34.4359,\n",
      "        -20.1950, -35.3294, -34.8005, -40.7809, -21.7191, -25.1558, -45.5462,\n",
      "        -54.4503, -50.8773, -25.6792, -44.3148, -26.1988, -37.4323, -26.4907],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  1.8552,  -3.3113, -12.4257, -27.9902,  -2.6116, -22.3570,  -7.8191,\n",
      "        -28.8716,  13.0891, -16.2549,  -0.8855,  -5.7603, -39.8037,  -7.4629,\n",
      "        -12.7535, -40.5160, -19.1666, -30.9137, -36.7758, -33.0306, -10.0844],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  12.9100,  -71.4626,    4.6634,  -30.1042, -344.5027, -236.5030,\n",
      "        -386.9296,  -86.2241,   -7.3204, -251.9917, -578.3291, -190.4102,\n",
      "        -415.3202, -428.4925, -555.4313, -121.9622, -610.6680, -205.6798,\n",
      "        -222.2098, -479.6743, -217.2035], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-104.9216, -113.9117, -106.6119,  -90.2331, -135.4970, -159.2690,\n",
      "        -131.9250, -108.9745,  -90.2768, -176.3324, -149.1134, -153.3230,\n",
      "        -151.1710, -152.6623, -152.8352, -169.0688, -218.8786, -145.4943,\n",
      "        -161.1294, -258.9167, -152.5149], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-39.4728, -51.2306, -42.2717, -27.3214, -32.7615, -37.2212, -37.0333,\n",
      "        -33.4862, -44.8535, -55.5145, -53.8993, -50.6554, -24.6091, -37.6910,\n",
      "        -32.0491, -38.3581, -44.2125, -38.9156, -43.6960, -44.7676, -28.7882],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-497.6407, -489.7180, -430.9408, -607.7817, -695.4594, -717.7076,\n",
      "        -745.8692, -812.9142, -579.2533, -425.6910, -559.8624, -696.5515,\n",
      "        -651.1592, -345.9393, -622.4108, -524.2573, -680.2264, -599.1737,\n",
      "        -787.5818, -547.5243, -591.4776], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-117.3741, -143.6805, -122.7604,  -77.3509, -209.7553, -209.6491,\n",
      "         -93.0306,  -98.6854, -132.6729, -326.1103, -257.7066, -158.5290,\n",
      "        -230.1576, -221.6475, -217.7703, -252.7125, -313.5734, -232.6724,\n",
      "        -219.6690, -454.0005, -229.1967], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-170.9795, -144.6271, -103.0173, -319.2076, -257.6507, -221.9861,\n",
      "        -232.3708, -287.7862, -179.8489, -124.1035, -344.7140, -325.5062,\n",
      "        -357.6866, -267.1564, -453.2830, -594.8561, -210.7925, -363.7112,\n",
      "        -261.3709, -711.0239, -428.1912], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-19.2395, -28.5814, -26.0649,  -3.7981,  -3.8891,  -5.6892, -18.1359,\n",
      "         -6.0746, -21.0175, -24.0300, -26.2936, -17.7533,   2.9442, -24.9263,\n",
      "        -16.7681,  -8.8290, -15.0214, -15.4840, -11.2487,  -0.6080,   0.0522],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -43.1592,   24.0107,  -50.3856,   -4.2130,  -14.5243,  -18.2440,\n",
      "        -102.8429,   16.0563, -109.1732,  -47.7029, -165.2439,  -32.9408,\n",
      "           4.7463,  -84.5336,  -23.2573,  -52.0721,  -75.6345,  -12.5582,\n",
      "         -57.7077, -128.4288,  -66.3882], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-26.3488, -47.3111, -33.8836, -16.2574, -23.7179, -26.6606, -30.2123,\n",
      "        -20.8817, -73.4399, -50.0477, -53.5629, -56.2435, -27.9039, -40.1080,\n",
      "        -16.9280, -31.2126, -37.8436, -16.4238, -53.6557, -59.1937, -43.9297],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-20.9105, -20.4962, -18.9656, -33.9544, -18.1782, -15.8272, -26.9731,\n",
      "        -24.8233, -16.5644, -30.7469, -45.1704, -32.2231, -32.4234, -50.4399,\n",
      "        -65.9980, -76.1643, -26.2005, -57.4453, -22.8489, -84.9254, -45.0825],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-13.9691, -15.0168, -16.2704, -12.8697, -14.5329,  -7.8746, -12.1124,\n",
      "        -11.2866, -14.8300, -16.0942, -13.5584, -10.9183, -10.7419, -15.3812,\n",
      "        -12.7227, -11.0204, -11.6327, -10.8743, -10.4578,  -6.8604,  -3.3713],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -92.4785,  -57.1225, -115.7920,  -30.7132,  -47.0424,  -59.4762,\n",
      "         -98.7343,  -27.6135,  -14.8739, -143.7956,  -76.0297, -124.8322,\n",
      "        -129.7767,  -81.5684, -111.3684, -107.7486,  -86.9659,  -67.9737,\n",
      "         -85.4154, -177.3272, -126.2902], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -7.7640,  -8.9498,  -5.7665, -10.3496, -10.4141, -32.8320, -11.6470,\n",
      "        -11.3576, -16.1019, -13.7159,  -8.8982,  -7.5985, -10.6359,  -6.7392,\n",
      "         -9.5057, -11.8910,  -6.4287, -10.1634, -15.0206,  -8.2594,  -5.2212],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-274.9615, -267.5443, -311.8344, -174.3747, -279.5199, -240.8725,\n",
      "        -226.9036, -193.6127, -230.4429, -466.0190, -348.3365, -319.3314,\n",
      "        -334.5023, -340.2627, -348.0204, -337.0352, -333.3651, -328.8395,\n",
      "        -286.9740, -498.0004, -296.6429], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-54.6157, -62.7180, -57.0427, -54.4690, -54.8021, -50.6214, -63.0931,\n",
      "        -65.8860, -56.8358, -59.5793, -68.2035, -78.8712, -45.6071, -93.0535,\n",
      "        -69.6730, -60.9850, -77.7248, -66.3455, -53.7225, -70.3839, -35.6405],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-21.7177, -26.1067, -20.5710, -25.5434, -26.9207, -43.1853, -36.9536,\n",
      "        -41.5303, -19.8843, -25.8652, -25.5383, -48.0411, -22.7024, -33.2085,\n",
      "        -27.1924, -23.0904, -55.6572, -23.7342, -34.1575, -34.4772, -16.1371],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.8017,  -8.6507, -13.2195, -12.3401, -10.0248,  -5.3577,  -9.7240,\n",
      "         -9.8343, -10.9988, -11.6102,  -7.2548, -10.1284, -15.8830, -15.2967,\n",
      "         -3.8382,  -8.4910,  -2.7081,  -3.7351,  -7.9072,  -4.0911,  -1.3692],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-33.4248, -36.0040, -37.6841, -29.3344, -30.0913, -16.3433, -30.1272,\n",
      "        -23.2638, -34.7002, -34.6105, -38.5321, -24.9416, -26.2461, -43.6853,\n",
      "        -44.7399, -42.6500, -24.9364, -38.8635, -25.0488, -34.6155, -22.3030],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-132.7369, -157.4778, -139.8579, -103.0183, -137.9898, -134.7687,\n",
      "        -167.2520,  -96.4540, -120.3055, -185.9127, -160.0168, -169.5574,\n",
      "        -134.2851, -197.1111, -100.5358, -136.7831, -224.5771, -128.4990,\n",
      "        -174.6645, -214.2054, -143.5103], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -99.2186, -116.1622, -108.8152, -100.9138, -121.8722, -105.2773,\n",
      "        -131.1857,  -86.5469, -108.9624, -123.3216,  -94.4186, -118.4891,\n",
      "        -139.5513, -135.5139,  -75.5043,  -99.6357, -154.9426,  -80.5931,\n",
      "        -154.8005, -156.0256, -125.5763], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-215.1087, -233.2068, -224.4942, -204.5841, -230.9267, -178.3126,\n",
      "        -205.2429, -204.3812, -231.0037, -240.7322, -224.5934, -203.1145,\n",
      "        -185.6669, -232.0907, -225.9512, -221.3250, -208.2660, -234.8481,\n",
      "        -203.6942, -206.0174, -150.7773], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-279.1467, -324.4233, -342.3807, -164.5882, -188.9513, -109.1986,\n",
      "        -243.8574, -134.6545, -282.7427, -298.2932, -278.7582, -153.0500,\n",
      "        -120.8838, -378.7815, -329.0196, -245.9754, -207.7564, -313.3564,\n",
      "        -174.1760, -140.1205, -117.6900], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-104.2358, -109.8655, -107.9535, -101.9572, -112.1400,  -88.4937,\n",
      "        -101.4410, -102.7868, -108.7040, -114.9840, -107.1803, -104.5032,\n",
      "         -94.7981, -107.3044, -110.0197, -109.4786,  -99.7411, -110.2450,\n",
      "         -99.7261, -103.7389,  -75.1857], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-4.1561e+01, -3.7146e+01, -4.8432e+01, -9.5533e+01, -5.4804e+01,\n",
      "        -3.6567e+01, -3.6136e+01, -8.5265e+01, -7.7412e+01, -1.9630e+02,\n",
      "        -2.0743e+01, -8.8517e+01, -2.0780e+01, -5.5328e+01, -8.3219e+00,\n",
      "        -3.1732e+01, -1.4947e+02, -1.9475e+02, -3.5442e+01, -6.3216e+01,\n",
      "        -1.5348e-01], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -36.0036,  -52.3911,  -35.5936,  -16.0611,  -77.9753, -125.9999,\n",
      "         -54.6903,  -41.9302,  -30.8959, -148.2756, -142.8954,  -96.5780,\n",
      "        -117.2463, -131.9505, -154.1878, -174.6019, -202.7970, -134.1236,\n",
      "        -118.1821, -315.3640, -150.3346], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -30.5957,  -42.9616,  -22.3255,  -44.5998,  -41.6299, -108.4878,\n",
      "         -84.5865,  -99.3979,  -22.0372,  -38.5168,  -52.9987, -128.3389,\n",
      "         -41.8915,  -84.1141,  -62.4058,  -51.0994, -148.9488,  -51.6837,\n",
      "         -76.0609, -103.5243,  -42.0361], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -79.2063,  -83.8503,  -84.7263,  -68.5603,  -89.9968,  -79.9320,\n",
      "         -78.0388,  -74.5729,  -77.8707, -106.4533,  -78.1760,  -82.5124,\n",
      "         -77.7670,  -77.8322,  -70.9295,  -69.0873,  -96.3831,  -75.9562,\n",
      "         -85.0027,  -80.7867,  -56.8672], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-66.6684, -71.5446, -67.2407, -69.5429, -76.7013, -67.0033, -69.7496,\n",
      "        -73.9935, -70.6582, -73.8212, -69.3055, -73.2872, -63.3757, -67.9701,\n",
      "        -70.0640, -71.0974, -74.9242, -70.3842, -70.1367, -70.0556, -48.9567],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-151.6181, -157.4265, -140.5677, -176.9894, -181.4124, -177.0823,\n",
      "        -176.8897, -182.8828, -155.6259, -158.2351, -189.7251, -189.4727,\n",
      "        -180.2837, -166.1253, -224.5501, -252.1138, -188.4127, -204.1485,\n",
      "        -184.3333, -273.8149, -183.1959], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.1950, -10.0414, -12.0979,  -9.4395,  -8.8602,  -6.5363, -11.6139,\n",
      "         -6.5007,  -9.0602,  -8.5263, -13.1104, -11.6148,  -9.4994, -12.6449,\n",
      "        -19.3936, -22.4240,  -8.6266, -12.8888,  -8.5240, -16.5259, -11.6307],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-261.3298, -272.0437, -277.5742, -242.9384, -220.1785, -147.2818,\n",
      "        -260.2620, -178.2232, -260.1264, -224.4203, -315.4409, -182.8762,\n",
      "        -226.5124, -310.4469, -435.1774, -451.5369, -169.8865, -365.9790,\n",
      "        -216.4300, -376.0905, -279.3912], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-39.6155, -36.9913, -47.6917, -21.0516, -31.3667, -37.9462, -39.9833,\n",
      "        -33.5021, -25.1473, -64.7670, -43.5747, -60.8162, -42.9486, -51.4541,\n",
      "        -47.2811, -36.2335, -55.8293, -37.8628, -39.0836, -60.4672, -32.1441],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-35.2019, -36.0000, -33.6289, -43.0357, -43.4140, -34.7263, -37.3074,\n",
      "        -44.4953, -38.1297, -34.4562, -35.2861, -42.2979, -36.3673, -32.7078,\n",
      "        -34.3533, -37.7169, -35.3869, -33.6906, -36.5966, -35.3060, -24.3624],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -62.2254,  -51.1698,  -47.9119,  -97.9716,  -61.9762,  -36.1581,\n",
      "         -78.3341,  -54.5247,  -57.2738,  -29.9056, -143.5249,  -73.1901,\n",
      "        -120.1611, -117.4070, -227.1991, -278.9100,  -37.4092, -162.3486,\n",
      "         -72.0783, -293.8727, -181.0628], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-21.4133, -25.2588, -26.3022, -13.7817, -21.5872, -17.5638, -19.1148,\n",
      "        -15.1985, -22.1456, -31.5802, -21.3152, -16.5842, -14.4620, -25.6955,\n",
      "        -18.6919, -12.9318, -27.2835, -18.7590, -19.1870, -11.4522,  -5.7778],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -75.4836,  -73.8876,  -94.1300,  -83.6566, -140.5992,  -73.6618,\n",
      "         -85.7853,  -90.7270, -304.7705, -100.8414, -116.4318,  -84.5340,\n",
      "         -83.0521, -111.1297, -159.4294, -132.8287,  -68.1981,  -64.1024,\n",
      "         -78.3138,  -97.7395,  -85.8376], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-139.2892,   14.6125, -180.9877, -191.5775,  -32.6013,  -94.7437,\n",
      "        -225.6871,   -4.8586, -209.0407,  -75.6798, -403.0896, -259.3352,\n",
      "         132.7945, -374.5485,  -98.9468, -214.1526, -291.9643,    2.0933,\n",
      "        -251.9746, -278.0532, -269.6527], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-160.1895, -162.8581, -160.5268, -171.5639, -165.8852, -105.2836,\n",
      "        -151.5258, -142.3014, -170.4879, -146.8794, -182.3899, -136.5484,\n",
      "        -155.2756, -169.8318, -215.7966, -235.3841, -107.1677, -198.0873,\n",
      "        -141.6839, -205.4539, -152.0513], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.5042, -10.2717, -18.7413, -27.6756, -11.6195,  -8.6719, -19.9303,\n",
      "        -10.1425,  -7.5840, -62.6318,  -1.0934, -13.0558, -12.9446, -16.1776,\n",
      "         -3.4844, -16.9894, -38.1402, -48.6965, -14.2471, -23.0469, -12.2558],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-26.4148, -34.3980, -27.3170, -17.2094, -43.3569, -81.7796, -44.7825,\n",
      "        -38.8426, -21.0426, -63.6523, -39.8401, -47.2311, -43.7464, -25.0550,\n",
      "        -44.9237, -45.7306, -98.8854, -40.4013, -64.3331, -80.1161, -45.6207],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-210.9678, -236.0994, -216.1609, -198.5670, -293.8623, -174.6077,\n",
      "        -147.5322, -195.8345, -254.2347, -368.3608, -341.0876, -251.9791,\n",
      "        -275.5851, -369.7172, -263.5410, -288.3383, -286.8295, -303.7279,\n",
      "        -232.5676, -439.0771, -220.0041], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-19.4036, -19.2721, -19.5134, -23.7237, -27.0701, -12.0782, -13.6616,\n",
      "        -21.4953, -23.5613, -23.1589, -18.1526, -18.6956, -20.3013, -16.5624,\n",
      "         -9.5270, -11.6597, -12.4116, -12.8427, -16.1446,  -9.7836,  -5.6572],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-178.9996, -260.0688, -213.7720, -109.1643, -157.9509, -190.0569,\n",
      "        -221.9725, -257.4205, -194.2661, -307.7509, -302.5642, -416.2812,\n",
      "         -94.3515, -599.7295, -213.0230,  -79.8011, -481.6309, -254.4830,\n",
      "        -180.0660, -272.1319,  -28.7331], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6543, -1.5023, -2.6023, -2.4735, -2.3015, -2.2407, -2.9680, -2.0072,\n",
      "        -0.8209, -2.1400, -1.9124, -2.0596, -2.6197, -1.3339, -3.2774, -3.1400,\n",
      "        -3.8618,  0.7948, -1.9191, -1.6813,  1.1463], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-19.7892, -17.0026, -22.5039, -14.4048, -24.6634, -19.3157, -14.0776,\n",
      "        -13.4394, -16.0816, -36.8962, -22.4885, -19.6898, -29.4796, -12.7138,\n",
      "        -20.8425, -23.3751, -21.0697, -17.6503, -22.3213, -33.9342, -20.5707],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-16.7903, -16.7917, -19.1939, -10.7618, -22.6486, -29.0271, -17.9033,\n",
      "        -18.4246, -12.3373, -36.9703, -20.7486, -27.5598, -25.6886, -16.4427,\n",
      "        -17.7770, -16.6088, -37.1385, -15.5219, -25.9088, -34.1308, -17.0613],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -6.7516,  -7.8020,  -8.9558,  -4.7275,  -7.1565,  -6.4149,  -7.0469,\n",
      "         -4.7781,  -6.2181, -10.5148,  -7.1907,  -5.3769,  -5.6298,  -7.8762,\n",
      "         -7.6361,  -5.8467, -10.5812,  -4.5667,  -6.5679,  -4.7364,  -0.8694],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-117.8981, -115.3887, -114.3725, -102.4670, -161.8739, -233.5880,\n",
      "        -149.8873, -152.0988,  -90.0322, -211.7705, -142.4991, -191.7081,\n",
      "        -188.3706,  -78.5253, -158.4209, -176.5201, -247.2018, -143.6696,\n",
      "        -203.4596, -279.1774, -180.0986], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -33.1352,   -5.4787,  -34.3882,  -19.1368,   -4.2724,  -53.2073,\n",
      "         -71.9201,  -50.6458,   23.9703,  -16.6017,   25.4906, -100.2133,\n",
      "         -40.6659,   12.1950,    3.9668,   20.5094,  -49.6605,   28.7924,\n",
      "         -42.9623,   -7.5779,  -23.2138], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-25.9788, -27.0116, -27.2248, -26.6861, -30.6973, -23.6494, -24.9639,\n",
      "        -27.1615, -27.1830, -31.1908, -26.3682, -27.2936, -25.8491, -25.0248,\n",
      "        -24.6325, -24.9349, -27.1292, -23.7679, -25.8357, -24.4848, -15.7007],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-11.0657, -10.7166, -11.6503, -13.4046, -13.4327,  -7.3053, -10.2383,\n",
      "        -10.5694, -11.6901, -11.4196, -12.4747,  -9.6892, -12.6123, -10.4285,\n",
      "        -13.6463, -15.5211,  -8.1267, -10.0856,  -9.8093, -12.8124,  -7.2670],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-51.3592, -50.8318, -51.8030, -56.5387, -58.5080, -33.0513, -43.9163,\n",
      "        -50.2559, -55.8674, -52.7590, -52.2845, -48.3140, -50.7231, -48.8382,\n",
      "        -49.8921, -54.5118, -33.0334, -50.1718, -43.6869, -47.8676, -34.8213],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.8340, -22.9089, -21.2164, -33.4574, -35.7527, -21.8423, -21.0672,\n",
      "        -31.6732, -28.1542, -27.4982, -28.6123, -30.3636, -32.4919, -23.5155,\n",
      "        -22.8380, -30.0469, -21.8371, -23.7161, -25.8567, -34.4237, -20.1300],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-24.9166, -25.4229, -25.0032, -28.7008, -27.8508, -21.8651, -26.8089,\n",
      "        -27.0890, -25.7099, -23.4074, -26.9681, -26.2415, -25.3123, -24.9702,\n",
      "        -31.3501, -33.6599, -23.4415, -26.5300, -24.6180, -29.6018, -19.8524],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-67.0775, -71.9243, -69.8848, -66.5783, -71.0891, -55.1686, -66.6275,\n",
      "        -65.8210, -70.9874, -70.5415, -69.5094, -65.0863, -58.1351, -71.9652,\n",
      "        -73.3009, -72.2322, -63.9516, -71.6113, -62.7949, -63.9221, -45.8882],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -64.0467,  -80.0985,  -67.7496,  -89.5796,  -74.2084,  -65.8450,\n",
      "         -58.1648,  -67.4283,  -48.5760,  -66.0320,  -98.6804, -100.3960,\n",
      "         -21.4342, -104.4272, -107.2505,  -88.9437,  -99.2501,  -61.3110,\n",
      "         -85.5394,  -49.4311,  -75.5987], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-41.1835, -35.0844, -47.9454, -27.5648, -33.1128, -29.1309, -37.5756,\n",
      "        -27.8144, -28.3962, -54.5249, -39.3199, -43.5652, -44.6527, -32.1699,\n",
      "        -50.0218, -45.1998, -31.0495, -37.1132, -34.9942, -50.7016, -36.1596],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -18.4963,   -5.3856,  -17.9247,  -12.0880,  -25.8311,  -17.6686,\n",
      "         -17.0753,  -10.4991,  -19.8567,  -12.4018,  -24.3764,  -11.5015,\n",
      "         -25.7008,   -4.6321,  -26.4332,  -30.6173,  -30.4013,  -31.2233,\n",
      "        -101.6914,  -21.3357, -113.2676], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-347.3531, -314.0074, -415.6979, -161.2793, -344.4578, -302.0518,\n",
      "        -284.6041, -173.8967, -237.3912, -637.8384, -332.1490, -360.0635,\n",
      "        -424.0343, -323.1528, -276.9905, -260.4902, -419.8673, -289.4637,\n",
      "        -366.5184, -483.8416, -330.0846], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-102.7676,  -83.9618,  -97.5005,  -55.1276,  -64.8243,  -75.9927,\n",
      "         -94.9664, -116.3784, -253.5279,   40.5355,  -62.0069,  -87.7675,\n",
      "         -98.3394, -141.8178, -138.5634, -153.7302, -124.7341, -117.1638,\n",
      "        -636.7382, -445.6661, -144.6780], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-190.7077, -192.8883, -187.4676, -146.9391, -273.7752, -431.1922,\n",
      "        -253.0318, -250.7459, -138.2540, -387.2130, -239.0761, -327.1436,\n",
      "        -320.4368, -123.6178, -260.3057, -285.5790, -468.3626, -241.2509,\n",
      "        -363.8075, -493.6115, -312.6317], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-35.9486, -34.1228, -40.8215, -47.4308, -34.1506, -18.1445, -36.9978,\n",
      "        -30.7108, -44.0786, -41.9695, -29.6432, -31.4608, -33.6382, -62.1028,\n",
      "        -25.5248, -36.1032, -24.8110, -46.2209, -27.9399, -21.1685, -15.3503],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-41.3687, -44.5523, -42.6496, -42.5469, -46.1172, -40.0361, -43.7569,\n",
      "        -45.0560, -43.3008, -44.8365, -42.2377, -44.7721, -37.5609, -42.9958,\n",
      "        -43.6115, -42.8649, -46.1137, -42.0625, -42.0437, -39.9315, -27.3396],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -44.7733,  -53.4737,  -42.7919,  -49.4018,  -41.5628,  -91.3425,\n",
      "         -86.4595,  -82.4743,  -35.2650,  -32.8945,  -37.9968,  -88.9615,\n",
      "         -32.5330,  -49.5588,  -60.5905,  -48.2167, -104.5942,  -47.3807,\n",
      "         -67.7785,  -48.5286,  -33.6479], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -59.9197,  -73.2971,  -54.8321,  -69.2643,  -69.2855, -126.3098,\n",
      "        -108.5575, -119.4873,  -53.3124,  -63.8534,  -65.6700, -137.1026,\n",
      "         -56.3374,  -89.1825,  -75.2079,  -61.1556, -157.2084,  -69.1173,\n",
      "         -96.7688,  -90.4925,  -48.2334], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -28.1048,  -66.4688,  -54.9751,  -55.3399,  -30.0699,  -27.5350,\n",
      "         -82.2277,  -14.5503,  -31.8419,  -29.4271,  -85.2741,  -61.1666,\n",
      "          17.7070, -112.5103, -131.0351,  -46.7582, -130.1066,  -64.6272,\n",
      "         -73.3773,   13.6807,  -57.3554], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -67.1872,  -54.2532,  -69.6064,  -33.7412,  -41.5743, -117.5860,\n",
      "         -43.3258, -104.2891,  -59.4160,  -67.1185,  -38.5199,  -50.6589,\n",
      "        -111.6973,  -69.3983, -124.7088,  -56.7578,  -38.5178,  -55.8120,\n",
      "         -31.2882,  -28.8281,  -31.1764], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-149.1605, -169.5289, -117.1266, -181.0567, -272.2397, -361.8766,\n",
      "        -214.3044, -246.7779, -149.1007, -314.3433, -331.4629, -324.9317,\n",
      "        -331.4025, -269.8466, -354.2243, -429.7444, -456.8591, -324.9147,\n",
      "        -333.5850, -687.3936, -370.7188], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-138.6151, -121.4100, -140.9680, -101.7382, -186.0190, -140.3667,\n",
      "        -213.8407, -158.5155, -187.3576, -124.1377, -184.8114,  -86.2564,\n",
      "        -208.5230,  -70.7696, -226.3528, -134.9586, -138.5571, -212.0641,\n",
      "        -264.6121, -117.0014, -235.8373], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.0160, -16.0496, -25.4154, -45.4507, -20.9659, -16.6094, -32.7304,\n",
      "        -13.4534, -25.5240, -22.3801, -23.6001, -13.9082, -44.7084, -65.8169,\n",
      "        -26.3628, -55.5036,   0.1448, -35.5853, -26.9775, -39.1370, -32.2668],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.8836, -23.4097, -20.6250, -17.7829, -20.4980, -18.6286, -18.7318,\n",
      "        -19.9371, -18.9818, -22.4005, -20.9039, -18.8057, -18.5652, -20.0536,\n",
      "        -19.3364, -20.7991, -22.1358, -17.4587, -19.1838, -19.9782, -11.1453],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-22.3405, -23.2445, -22.7619, -25.4265, -23.8547, -16.7423, -23.8956,\n",
      "        -23.2405, -23.0499, -22.1989, -30.3570, -26.3012, -24.2663, -32.9493,\n",
      "        -35.6034, -37.5882, -23.2659, -29.0430, -21.3062, -38.1540, -21.3170],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-18.9418, -20.2927, -20.5431, -23.9424, -18.3437,  -8.5218, -15.9506,\n",
      "        -18.5076, -18.6164, -33.2565, -28.8487, -21.4507, -19.2659, -37.3857,\n",
      "        -30.9067, -32.5855, -18.4007, -33.7879, -14.3575, -35.6173, -15.9614],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ 101.3189,  113.5044,  112.9683,  158.5291, -118.0404, -239.8398,\n",
      "        -264.1334,  339.5922,  338.7409, -186.0661,  113.4297,   33.5153,\n",
      "        -326.9905, -207.4526,  335.8243,   11.2225, -773.6874,  217.0984,\n",
      "        -369.1969, -641.5795, -435.3222], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.5576,  -8.8778, -10.3374,  -7.9749,  -7.2572,  -5.2596,  -9.6278,\n",
      "         -5.5213,  -7.5466,  -8.1420, -10.3104,  -5.8995,  -7.7746, -10.1798,\n",
      "        -15.3131, -15.0725,  -7.9661,  -9.2087,  -7.0450, -11.1251,  -5.9842],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  22.4283,   17.8900,   25.6812,   26.4944,  -28.7280,  -32.6678,\n",
      "           4.2873,   23.1468,   32.2237,  -62.0241,  -17.9505,  -27.0284,\n",
      "         -50.1648,  -57.7135,   43.7650,   13.5311, -114.9143,   16.6287,\n",
      "         -41.0188, -117.6848,  -37.8289], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-144.8676, -161.3701, -154.7124, -124.0983, -157.8015, -146.6026,\n",
      "        -145.7752, -129.8250, -149.7197, -186.2017, -164.2758, -136.1550,\n",
      "        -135.2778, -159.8729, -175.8206, -172.5899, -178.4519, -173.8025,\n",
      "        -156.6328, -181.6914, -124.7121], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-103.7350, -145.0182, -124.2037,  -48.2447, -159.5391, -271.1210,\n",
      "        -160.1195, -116.1447,  -58.7595, -261.7439, -115.2604, -169.2631,\n",
      "        -191.7349,  -84.3632,  -68.2276, -101.0146, -337.4725,  -90.8810,\n",
      "        -234.1436, -258.5495, -166.0336], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-186.5966, -148.7775, -192.8207, -172.6372, -185.9174, -136.8914,\n",
      "        -157.6385, -188.7258, -147.3739, -231.0130, -160.8094, -252.8104,\n",
      "        -221.8723, -132.7262, -151.9665, -154.4612, -118.3310, -142.8846,\n",
      "        -161.8359, -204.8503, -149.7632], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.5221, -12.4664, -14.3781,  -5.6470,  -6.4932,  -5.1253, -10.9905,\n",
      "         -4.7480,  -9.3480, -11.9097, -10.3152,  -5.9566,  -4.1782, -14.7646,\n",
      "        -13.9091,  -9.3331, -11.3577,  -9.1554,  -6.6616,  -3.6914,  -1.0070],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -82.7937,  -81.9386,  -80.1275,  -96.2730,  -93.0713,  -57.3369,\n",
      "         -77.4966,  -80.1121,  -89.7612,  -75.8409,  -94.9857,  -74.9913,\n",
      "         -88.2920,  -82.3295, -108.4298, -124.2872,  -53.8528,  -99.8188,\n",
      "         -76.1978, -111.7302,  -81.1159], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-11.2779, -11.7037, -13.1858, -10.6191, -10.0138,  -6.5883, -11.8233,\n",
      "         -7.6046, -10.5605, -10.8787, -13.2638,  -7.8645, -10.1042, -13.1629,\n",
      "        -18.4104, -18.3007,  -9.4485, -12.3459,  -9.1357, -13.7724,  -8.0285],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-306.1850, -333.5713, -303.1329, -305.5737, -369.7620, -359.0179,\n",
      "        -360.6564, -314.4665, -301.4102, -390.0891, -383.3409, -367.7620,\n",
      "        -360.2729, -405.2540, -400.8813, -445.4045, -468.7862, -395.8873,\n",
      "        -389.4960, -555.3879, -359.2860], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -64.3831,  -18.6915,  -70.3589,  -56.9680,  -25.3375,  -43.1097,\n",
      "         -99.8840,  -19.3549,  -90.0165,  -48.2899, -158.4686,  -97.1285,\n",
      "          22.5690, -131.9187,  -60.5407,  -97.9154, -110.8169,  -29.4791,\n",
      "         -91.4174, -124.7694, -104.1939], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.4929,  -1.2180, -13.6268, -12.2234, -14.9525,  -7.5876,  -2.9629,\n",
      "         -8.6142, -14.8993, -12.0446, -24.5554, -23.4045, -35.2219, -33.8782,\n",
      "        -31.9840, -32.7537,  -7.5828, -66.8823, -35.7303, -63.4722, -34.3578],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-32.5554, -37.5025, -35.1687, -35.5051, -63.7062, -35.2615, -39.2605,\n",
      "        -32.0739, -44.1783, -56.4113, -73.4812, -20.1065, -58.4168, -27.9400,\n",
      "        -77.7559, -47.8226, -36.6378, -49.6331, -37.6705, -47.4209, -32.8568],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -81.4861, -102.7845,  -83.2665,  -81.4481,  -85.0385,  -99.6390,\n",
      "        -109.8295, -127.7702,  -85.1114, -105.9371, -126.8778, -174.4955,\n",
      "         -72.0501, -203.7777, -116.1747,  -90.4425, -179.7052, -116.3805,\n",
      "         -94.6887, -150.7442,  -55.4573], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -56.7969,  -49.9543,  -69.4329,  -16.6896,  -70.3340,  -66.5083,\n",
      "         -67.6542,    4.4440,  -19.5783, -140.7581,  -87.8569,  -62.0185,\n",
      "        -118.9691, -104.0493,  -81.4250, -109.5875, -137.2575,  -73.0576,\n",
      "         -96.8678, -212.8548, -128.2134], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -91.0213,  -86.3680, -151.8033, -204.5296, -112.0736,  -98.8551,\n",
      "         -72.0265,  -77.2517,  -80.3675,  -82.8841,  -92.8830, -108.5444,\n",
      "         -79.2929, -158.8722, -121.2015, -116.9112, -106.7621,  -66.3834,\n",
      "        -136.5791,  -69.0575,  -79.7857], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-110.9238, -128.6515, -135.5422,  -91.6664, -121.3073, -106.9843,\n",
      "        -127.7560,  -87.3148, -124.9212, -165.3702, -100.6340, -147.6885,\n",
      "        -171.7865, -140.2450,  -76.0659,  -86.3764, -151.8274,  -72.7062,\n",
      "        -177.3499, -176.0117, -144.6373], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-51.6698, -59.9964, -56.7273, -44.0728, -53.9166, -52.4298, -55.4523,\n",
      "        -54.4049, -53.5651, -64.9707, -54.6966, -60.2545, -39.7627, -68.0827,\n",
      "        -51.8108, -41.9035, -72.5756, -53.4404, -52.5900, -45.4014, -26.3746],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-31.8973, -36.1141, -35.0922, -25.5492, -31.7376, -29.4135, -30.1526,\n",
      "        -28.8009, -32.5846, -43.0703, -35.3780, -35.5589, -26.7771, -30.9285,\n",
      "        -28.6733, -28.5588, -35.4950, -29.4333, -32.4976, -31.2357, -20.0385],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -45.5628,  -43.8412,  -32.9337,  -77.0464, -139.3438,  -99.1147,\n",
      "         -55.6833,  -72.4982,  -63.8756, -172.5864,  -93.6610, -125.2226,\n",
      "        -144.7526, -121.4699,   -9.8652,  -75.0424, -216.3824,  -99.7884,\n",
      "        -121.4838, -240.7475, -107.4774], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -42.2959,  -28.8255,  -44.4621,  -33.7754,   16.2236,  -44.1968,\n",
      "        -160.4738,   60.4765,   36.3703,   43.0958,  -67.8580,   18.1868,\n",
      "         -88.3363,  -83.0207, -239.5897, -313.4728,  -91.1154, -126.0682,\n",
      "         -91.5583, -297.2645, -243.1335], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-11.0186,  -2.1074, -12.9043,  -0.1946, -11.1156,  -7.3942, -12.9324,\n",
      "         -0.5244, -12.1506,  -6.4998, -13.8802,   1.2288, -16.4069,  -1.3720,\n",
      "        -20.4359, -20.1059, -16.5023, -22.1797, -62.5715, -13.3326, -70.9577],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-106.8910,  -97.3815,  -97.8094, -135.6191, -117.5137,  -61.2328,\n",
      "        -100.0770,  -88.8991, -112.6748,  -81.6511, -145.4962,  -81.0484,\n",
      "        -138.0354, -104.3757, -197.3741, -242.1410,  -41.3829, -162.2210,\n",
      "         -99.8838, -221.5277, -159.5851], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.8721, -11.6999, -16.1842, -12.5650, -37.2446, -18.1569, -18.0243,\n",
      "        -36.2946, -10.9775, -21.5944, -21.0142, -20.6798, -23.6651, -42.4187,\n",
      "        -33.0171, -23.9699, -24.7745, -23.3837, -16.9310, -23.3420, -19.5000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-19.5240, -26.2705, -22.1342, -13.7440, -26.2999, -22.8612, -19.5313,\n",
      "        -18.0981, -44.0521, -38.9435, -35.4591, -33.2724, -18.8652, -27.2299,\n",
      "        -15.8776, -21.3580, -32.3162, -14.7131, -27.9109, -34.8453, -19.6496],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -70.5754,  -79.8839,  -76.9906,  -79.8129,  -57.7173,  -42.3551,\n",
      "         -67.4928,  -70.3261,  -66.1660, -120.6358,  -97.8949,  -81.3265,\n",
      "         -56.0989, -136.8020, -110.8939, -107.6999,  -78.3518, -130.2774,\n",
      "         -56.0092, -115.4746,  -58.3922], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-70.6499, -76.1800, -74.1970, -68.9454, -68.0871, -58.0407, -75.8766,\n",
      "        -68.4192, -71.8462, -66.9328, -73.5152, -69.0793, -57.2400, -79.1568,\n",
      "        -87.5228, -84.3311, -68.1982, -79.5516, -65.1984, -70.5112, -51.5953],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -75.2056,  -85.3494,  -79.2997,  -69.6139,  -89.3411,  -76.5927,\n",
      "         -73.0920,  -82.3374,  -82.5048, -101.6697,  -82.1224,  -88.5164,\n",
      "         -67.9235,  -93.8965,  -66.5761,  -59.4625, -101.0059,  -77.1546,\n",
      "         -79.3409,  -73.3134,  -42.8161], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -16.5241,  -31.9447,  -22.1723,   -9.9319,  -16.4523,    2.3746,\n",
      "         -13.9342,  -34.9515,  -25.4470,  -52.5183,  -73.6714,  -86.4523,\n",
      "         -15.9078, -159.6363,  -40.4528,  -16.5963,  -81.0920,  -48.0285,\n",
      "          -8.9816,  -86.7424,    2.2203], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-46.6037, -50.9476, -48.1313, -47.6948, -47.5940, -47.2501, -54.4906,\n",
      "        -53.6292, -46.9724, -44.7280, -45.3612, -54.1854, -37.7160, -50.4126,\n",
      "        -51.4601, -47.5639, -54.9651, -47.5026, -47.0866, -40.8777, -29.0942],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-11.8306, -16.0536, -10.8943, -18.6215,  -7.6543, -17.3237, -11.2297,\n",
      "        -10.3856,  -9.6655,  -8.9232, -11.7932, -14.8851, -16.8470, -12.5665,\n",
      "        -12.5957, -16.9020, -18.7331,  -4.3906, -12.4634, -15.1481,  -8.8602],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-110.2018, -118.2446, -110.4177, -116.8149, -116.2057, -105.8295,\n",
      "        -122.5744, -125.5082, -114.3457, -104.3443, -110.1584, -124.6304,\n",
      "         -94.7869, -116.3310, -121.8167, -119.5662, -116.8479, -118.1031,\n",
      "        -110.2176, -106.3461,  -78.8667], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-538.7198, -552.8970, -584.3817, -415.8376, -592.3293, -536.3917,\n",
      "        -484.1435, -505.8310, -505.7348, -820.7270, -586.1221, -651.5527,\n",
      "        -571.5721, -590.9562, -517.1740, -475.3396, -670.9152, -556.0104,\n",
      "        -570.8000, -685.6329, -430.4345], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-18.1924, -18.5610, -21.1291, -18.7195, -23.7378, -10.2025, -15.1274,\n",
      "        -16.6391, -31.9597, -21.6644, -19.1156, -14.7659, -16.1170, -20.7824,\n",
      "        -18.7856, -16.1391, -12.7571, -13.3906, -13.7824,  -9.9351,  -6.7304],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -14.3994,  -17.5288,   -7.0182,  -15.1720,  -40.5606,  -90.9567,\n",
      "         -43.3054,  -22.1187,   -2.5102,  -66.1978,  -89.3778,  -51.3416,\n",
      "         -87.2786,  -52.1184, -132.8927, -165.0030, -118.9556,  -95.1760,\n",
      "         -82.3887, -247.5297, -134.8096], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-260.3295, -271.8779, -219.9951, -349.4678, -316.6080, -399.0128,\n",
      "        -377.8202, -432.2430, -257.2014, -204.6440, -260.0232, -429.5735,\n",
      "        -283.5327, -235.7043, -318.1015, -346.3836, -390.9365, -294.5416,\n",
      "        -350.1062, -362.7166, -264.2280], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-143.3074, -156.3414, -146.7204, -139.5720, -165.9537, -144.9592,\n",
      "        -143.4331, -149.8422, -153.3481, -173.0381, -151.8713, -150.1793,\n",
      "        -135.5197, -148.5950, -147.3154, -147.5008, -166.3146, -155.8352,\n",
      "        -152.6781, -153.8987, -107.9445], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-16.2016, -16.2612, -21.1596, -10.5519,  -5.9848,  -9.5035, -18.6901,\n",
      "        -17.8638, -13.7994, -13.8152, -10.2290, -16.2000,  -4.9612, -15.4131,\n",
      "        -14.7257,  -7.3399, -26.9649,  -9.9361,  -9.3915,  -0.3407,  -0.7583],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ 164.1309,  140.5988,  104.6277,   77.2488,  -62.7467,   95.7239,\n",
      "         118.9005,   78.0807, -579.7740,  140.9326,   82.3173,  102.0307,\n",
      "         -30.1945,    0.7205, -109.1624,   13.7187,  205.2679,  180.7788,\n",
      "          14.1255,   34.5187,  -20.1725], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -56.6240,  -76.4471,  -63.5349,  -38.6609,  -71.8154,  -89.6407,\n",
      "         -67.5026,  -77.2084,  -60.2468, -114.2242,  -90.0617, -110.6293,\n",
      "         -56.8489, -132.0139,  -65.5135,  -44.6015, -154.8410,  -76.9242,\n",
      "         -80.4609, -107.7500,  -36.8714], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -63.7365,  -74.1457,  -66.5795,  -52.9603,  -84.6797, -102.8476,\n",
      "         -73.1008,  -71.0457,  -64.0702, -106.1175,  -76.7761,  -77.3338,\n",
      "         -74.1819,  -64.5507,  -74.7034,  -74.8818, -123.5015,  -76.9822,\n",
      "         -93.7352, -104.3504,  -64.8391], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-300.7446, -320.1798, -305.0304, -306.3015, -325.6375, -250.5832,\n",
      "        -295.0275, -302.8545, -323.0512, -312.5625, -314.3078, -296.9536,\n",
      "        -269.1148, -317.2306, -326.4350, -331.8163, -277.1807, -332.8346,\n",
      "        -286.5128, -304.3501, -226.0867], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-38.8986, -18.1535, -45.6655, -35.9208,   0.3052,   1.7348, -30.0530,\n",
      "        -75.6384, -36.5256, -17.5468,  -5.5455, -45.4480, -15.5938,  -3.9092,\n",
      "         -4.1229,   4.3255, -75.4520,  -2.9488,  -4.5013,  17.4175,   1.4698],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-125.8473, -160.6960, -128.6608,  -97.8740, -129.7061, -135.0815,\n",
      "        -116.7118, -126.6662, -146.9523, -196.6183, -187.0935, -183.6264,\n",
      "        -106.5145, -157.8425, -113.5719, -132.6777, -175.3819, -145.8902,\n",
      "        -152.3239, -193.8395, -110.6799], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -70.3578,   -4.6199,  -87.4188,   10.5855,  -33.0510,  -54.4321,\n",
      "        -120.6945,    5.6035, -114.2320, -118.1936, -157.4748,  -45.4258,\n",
      "         -58.6376,  -76.7676,  -48.6140,  -73.8291, -101.3418,  -52.1034,\n",
      "         -78.6606, -185.6253,  -85.4259], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-102.8949, -102.3685, -100.9495, -116.9465, -109.7424,  -53.3822,\n",
      "         -91.2579,  -88.0878, -112.2008,  -97.0170, -140.8004,  -93.4122,\n",
      "        -115.2239, -136.8948, -164.8436, -186.7829,  -63.1660, -146.4824,\n",
      "         -88.4691, -182.3616, -118.3706], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-3.7340, -2.5098, -3.6706, -8.8337, -6.4243, -3.1235, -5.0084, -7.8758,\n",
      "        -5.4010, -4.8877, -3.0726, -7.8000, -5.4196, -2.7160, -3.3297, -5.5234,\n",
      "        -7.1926, -5.0485, -3.4758, -4.7403,  0.2114], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-74.2894, -79.2101, -74.6144, -78.9870, -80.8498, -70.7344, -80.3022,\n",
      "        -82.8622, -77.6788, -73.8050, -75.9983, -81.9258, -67.1479, -77.2756,\n",
      "        -82.3052, -82.6860, -78.0874, -79.5323, -74.9068, -75.5649, -54.5192],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-35.6794, -36.7686, -36.5686, -41.6358, -41.0846, -22.0861, -27.4047,\n",
      "        -30.1608, -36.9391, -37.9235, -42.8476, -32.3939, -32.7586, -40.0230,\n",
      "        -44.1676, -50.0751, -26.0880, -38.0373, -33.6461, -41.7314, -33.1452],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-158.3728, -168.8051, -168.5724, -149.3195, -155.7086,  -95.5097,\n",
      "        -142.0890, -131.4595, -170.3926, -159.5699, -165.4040, -128.3621,\n",
      "        -126.2398, -177.2608, -177.0150, -173.2032, -114.0620, -175.2278,\n",
      "        -128.1215, -139.3272, -106.0790], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-16.7705, -19.7457, -19.4240, -13.1230, -17.3328, -21.4292, -20.7704,\n",
      "        -16.8746, -15.5342, -22.9632, -18.7246, -19.2725, -14.3880, -20.8731,\n",
      "        -21.6069, -18.2407, -29.5035, -17.6124, -19.8454, -19.4850, -10.3763],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  469.0341,   371.0991,   357.3500,   262.5331,  -144.4775,   304.7700,\n",
      "          368.6672,   275.2762, -1435.4215,   411.1434,   289.9655,   292.3154,\n",
      "         -110.4808,   141.9209,  -265.8005,   135.9941,   540.7330,   668.3502,\n",
      "           26.8143,   151.2958,   -13.0413], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-13.1741, -14.2782, -14.7653, -13.1238, -13.3397, -11.9544, -15.1096,\n",
      "        -13.5510, -12.7992, -13.5367, -13.0574, -13.7547, -10.8409, -14.4258,\n",
      "        -15.3103, -13.7817, -15.5508, -11.4731, -12.3966, -10.4045,  -5.7208],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-104.6065, -147.1999, -131.7084, -123.7960, -132.5857, -108.7665,\n",
      "        -131.7491, -106.4984, -178.8796, -131.1549,  -88.2387, -147.6709,\n",
      "        -207.7694, -123.8395,  -59.2666,  -88.3273, -123.1756,  -49.9185,\n",
      "        -227.5127, -181.5355, -182.7094], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.5327, -12.3778, -14.3038,  -5.8182,  -6.3923,  -5.1650, -11.2062,\n",
      "         -4.6489,  -9.2334, -11.6016, -10.6681,  -5.8852,  -4.5169, -14.7845,\n",
      "        -14.9588, -10.6943, -11.2203,  -9.7221,  -6.8038,  -4.9865,  -1.9126],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-140.7528, -151.0522, -133.2489, -163.1599, -171.7647, -154.9791,\n",
      "        -155.4510, -186.5665, -152.9085, -151.2087, -153.7166, -192.2559,\n",
      "        -143.0666, -162.7916, -144.1180, -149.3260, -176.2054, -153.3066,\n",
      "        -156.5926, -168.0052, -108.4478], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -28.6305,  -29.8197,  -28.1767,  -39.7736,  -57.3582,  -47.3546,\n",
      "         -27.8387,  -43.4965,  -47.3332,  -97.4949,  -65.5549,  -68.1488,\n",
      "         -59.7025,  -68.8284,  -57.1970,  -76.7221, -112.4862, -100.3988,\n",
      "         -52.5505, -140.7698,  -55.9034], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -51.4776,  -45.0160,  -43.5265,  -73.4877,  -55.5962,  -36.1333,\n",
      "         -58.8953,  -48.4491,  -50.3695,  -32.2275,  -87.5144,  -51.7081,\n",
      "         -80.4687,  -65.4010, -131.5408, -161.5132,  -29.9154,  -98.1902,\n",
      "         -55.9435, -160.1671, -105.3830], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -91.1917,  -81.4764, -102.5564,  -69.6217,  -65.8032,  -64.8512,\n",
      "         -94.3407, -108.9006,  -62.9110, -112.2862,  -81.7798, -172.8425,\n",
      "         -80.3450, -131.6440,  -72.2901,  -38.1296,  -99.9583,  -65.1311,\n",
      "         -68.1353,  -80.4317,  -36.6340], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-67.7919, -73.6386, -67.5664, -76.0439, -81.6566, -69.0899, -79.0129,\n",
      "        -84.6557, -74.1814, -67.7485, -74.3099, -79.4502, -64.2501, -67.8335,\n",
      "        -76.6674, -63.8282, -74.4738, -69.3150, -68.8508, -54.8285, -41.2945],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -6.6743,  -1.6319,  -6.1215, -31.2093,  -7.6743,  -3.9714, -11.8568,\n",
      "        -24.2132, -15.0402, -37.2829,  -2.0386, -24.9602,  -4.6114,  -9.4496,\n",
      "        -10.6937, -22.6572, -33.4955, -55.8119,  -4.0159, -25.3635,  -3.7833],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-5.6757, -0.7901, -8.3024, -2.4806, -1.0548,  0.3238, -4.0974, -3.0795,\n",
      "         1.1760, -7.4529, -0.1471, -9.9091, -7.4125,  1.1963, -1.8921,  0.6638,\n",
      "         2.0137,  4.2570, -0.7117,  0.1609,  0.6537], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-9.0150e+01, -1.3230e+02, -1.0659e+02, -2.0702e+01, -2.1858e+01,\n",
      "        -4.0272e+01, -5.4280e+01, -5.4211e+01, -1.1548e+02, -1.4515e+02,\n",
      "        -1.3378e+02, -1.2609e+02,  1.4495e-01, -6.5966e+01, -7.3211e+00,\n",
      "        -2.7646e+01, -8.5321e+01, -6.0432e+01, -7.5548e+01, -4.9281e+01,\n",
      "        -3.5550e+01], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-64.2370, -69.2938, -67.3178, -63.0664, -66.1198, -53.5637, -65.7889,\n",
      "        -63.3716, -67.0867, -65.8728, -65.9530, -63.1942, -53.7736, -69.9886,\n",
      "        -71.7015, -69.2362, -62.8646, -68.6997, -59.9883, -59.9079, -43.1798],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-176.6867, -195.0256, -180.1160, -147.1687, -238.7967, -293.9010,\n",
      "        -198.6621, -202.7025, -168.8583, -304.3390, -209.4624, -231.4356,\n",
      "        -225.9328, -158.7962, -200.1996, -208.4057, -336.7201, -210.2878,\n",
      "        -266.9440, -313.2095, -200.3193], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-139.9724, -143.2515, -144.7761, -139.4635, -124.9298,  -86.6051,\n",
      "        -142.7169, -105.0466, -139.6154, -117.5732, -170.8339, -105.9076,\n",
      "        -131.2265, -160.5302, -235.7108, -251.3917,  -93.4692, -196.2576,\n",
      "        -121.9505, -214.5704, -157.4424], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.4195, -23.9672, -22.8883, -28.1602, -38.7950, -19.6820, -13.7718,\n",
      "        -27.7268, -30.2542, -37.8776, -25.9059, -26.1127, -30.0574, -22.5544,\n",
      "         -8.2923, -12.4476, -22.9750, -17.0891, -24.0874, -20.7896,  -9.6932],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-42.5654, -40.0928, -50.9133, -22.6864, -39.4730, -41.5555, -40.5606,\n",
      "        -30.5959, -28.9125, -73.9910, -45.8251, -55.8170, -49.7690, -49.5466,\n",
      "        -45.1977, -38.5953, -60.0234, -39.3768, -45.2321, -65.3269, -37.7334],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -95.4704,  -91.1496,  -96.5829, -125.4228, -151.6000,  -95.1349,\n",
      "        -103.6798, -130.9911, -219.2205,  -98.4602, -110.6841, -116.2200,\n",
      "        -109.4958, -104.7237, -130.8387, -126.7206,  -84.5192,  -86.7412,\n",
      "         -99.5227, -105.2272,  -85.8509], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -81.1985,  -89.3930,  -81.1185,  -82.2443,  -92.8316,  -94.1373,\n",
      "         -94.5076, -117.5662,  -79.0229, -116.9075, -120.2735, -164.3949,\n",
      "         -94.8114, -165.0869, -106.3025,  -95.0964, -150.7964, -105.8623,\n",
      "         -94.9546, -162.7870,  -71.5507], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-77.5773, -84.9934, -84.7430, -69.3835, -73.2760, -52.7652, -74.6717,\n",
      "        -66.8874, -81.3654, -79.3543, -77.5399, -67.1900, -55.8740, -89.2882,\n",
      "        -83.5773, -75.2578, -67.3048, -81.4354, -64.0613, -57.8099, -43.4094],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-338.5615, -404.6127, -393.1370, -185.9171, -394.4335, -414.4189,\n",
      "        -306.8486, -212.0605, -345.1046, -617.8337, -439.8789, -256.9508,\n",
      "        -348.4383, -394.9142, -440.1364, -418.9279, -559.0983, -457.0238,\n",
      "        -428.8452, -550.6193, -348.5888], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -69.3956,  -40.4367,  -68.6194,  -56.6062,  -88.8411,  -67.5998,\n",
      "         -54.2941,  -46.4273,  -66.3620,  -55.3206,  -89.8229,  -67.9418,\n",
      "         -59.4952,  -50.8075,  -86.2013,  -93.3339, -119.3437,  -99.6382,\n",
      "        -314.4224,  -55.5767, -341.5847], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-62.6777, -64.7794, -64.2385, -64.0620, -67.8092, -53.9360, -62.6285,\n",
      "        -64.4546, -64.3815, -66.5197, -63.9224, -65.6149, -59.0085, -62.6242,\n",
      "        -67.2933, -68.1852, -59.0815, -64.7806, -60.2791, -63.9201, -45.7940],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-18.6171, -21.3122, -23.9745, -16.8158,  -9.0268, -11.8052, -25.9575,\n",
      "        -18.3684, -19.9756, -20.7924, -14.9669, -24.3368,  -1.7947, -30.0784,\n",
      "        -25.8671, -18.8399, -35.1348, -32.4459, -12.5030,  -9.9218,  -2.6066],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-129.1214, -150.3339, -175.8191,  -89.8362, -249.6723, -176.6309,\n",
      "        -292.1017,  -13.4900, -134.3130, -354.5444, -172.4346,  -69.1109,\n",
      "        -216.8299, -186.3706, -142.3157,  -78.4961, -501.1114, -247.8557,\n",
      "        -229.8430, -200.2132, -155.9494], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-59.0971, -63.9598, -60.6980, -60.1022, -65.6441, -56.8933, -61.8187,\n",
      "        -64.4133, -62.3698, -64.2312, -59.6915, -63.9913, -52.4848, -61.6969,\n",
      "        -60.2413, -58.6258, -65.3131, -60.4134, -59.7297, -55.0041, -38.6771],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -66.0123,  -63.4689,  -75.2827,  -47.2972,  -50.9802,  -55.5002,\n",
      "         -70.2569,  -66.7452,  -48.8137,  -83.7075,  -64.0221, -100.6764,\n",
      "         -58.6579,  -85.0207,  -69.0095,  -50.3118,  -77.9739,  -58.9427,\n",
      "         -57.5361,  -70.4166,  -40.0665], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.6804, -15.6066, -15.9866, -15.3350, -15.3699, -13.0953, -16.4789,\n",
      "        -15.2854, -14.5183, -14.7522, -14.9752, -15.2796, -12.9944, -15.6416,\n",
      "        -17.5038, -16.7389, -16.2260, -13.5143, -13.9722, -13.3166,  -7.8463],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-126.5485, -101.6946, -133.2219, -110.3452,  -62.2365,  -90.6754,\n",
      "        -168.0682, -157.3107,  -59.7009, -133.3931, -199.0466, -306.9062,\n",
      "        -165.2974, -285.8127, -271.2636, -246.2274, -170.9550, -189.6467,\n",
      "        -110.7874, -363.1624, -180.4002], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.0958, -19.2236, -18.5800, -16.9164, -19.7655, -17.2335, -18.5099,\n",
      "        -19.1764, -17.7233, -22.4133, -21.2862, -22.1400, -16.9620, -25.1571,\n",
      "        -20.6212, -19.0971, -25.4360, -18.2116, -18.1282, -22.3089, -10.2969],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-202.1438, -153.8685, -162.7879, -277.8408, -310.1258, -199.1932,\n",
      "        -291.6517,  -68.3264, -139.4134, -196.9841, -260.2097, -103.4259,\n",
      "        -440.0927, -180.5099, -337.1016, -590.9275, -240.3525, -285.8502,\n",
      "        -339.0901, -702.3924, -546.1310], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-12.7550,  -8.1640,  -8.4351, -33.7367, -22.3727,  -9.2823,  -6.5728,\n",
      "        -21.0779, -12.6290, -31.4114, -15.1728, -11.3417, -25.2404,  -3.1133,\n",
      "        -14.8517, -30.4252,   1.5332, -26.8977, -13.1095, -29.3414, -18.9637],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-67.5060, -72.4741, -69.1969, -69.8702, -71.7408, -59.0132, -70.4723,\n",
      "        -72.9932, -71.3055, -66.9860, -66.5298, -71.8366, -56.6101, -71.4562,\n",
      "        -69.1223, -66.6394, -67.0302, -68.5190, -63.9055, -57.1644, -41.8965],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-47.1861, -41.1287, -53.7948, -35.1804, -32.8829, -35.5209, -50.1182,\n",
      "        -44.6837, -31.3804, -51.3618, -36.6359, -64.9788, -41.2842, -42.5814,\n",
      "        -46.6055, -35.3197, -40.0408, -34.8247, -37.3792, -38.0678, -27.7725],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-74.1811, -81.7381, -77.8781, -70.5705, -81.7118, -70.8809, -75.2205,\n",
      "        -77.0088, -78.6747, -86.0618, -74.6254, -77.3564, -63.2301, -79.1777,\n",
      "        -72.1770, -67.4152, -84.4434, -75.4421, -74.4494, -64.7700, -45.5128],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-14.2272, -17.1466, -18.7126,  -6.7291, -10.0414, -14.9847, -17.6465,\n",
      "         -9.1564, -11.3126, -19.3714, -14.8391, -12.4803,  -8.3178, -18.8126,\n",
      "        -20.5536, -14.6796, -23.7092, -14.6044, -14.1629, -12.5763,  -6.5485],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-28.9197, -31.0457, -30.4195, -29.4365, -31.2085, -26.6614, -30.8556,\n",
      "        -30.1785, -29.7434, -30.6486, -29.8596, -30.1227, -25.9044, -30.6093,\n",
      "        -32.5063, -31.7342, -31.4891, -29.3172, -28.5417, -28.1442, -18.7124],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-11.9407, -11.7639, -11.7426, -15.6290, -14.8226, -12.6927, -14.2417,\n",
      "        -15.1765, -11.9229, -11.0977, -13.0619, -14.5789, -14.1054, -10.7993,\n",
      "        -15.5817, -17.5141, -13.3900, -11.2610, -13.1680, -15.7581,  -9.2965],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -46.3127,  -52.6002,  -49.1879,  -41.7205,  -51.9002,  -45.3460,\n",
      "         -47.9353,  -43.4994,  -47.9741,  -66.3230,  -72.6383,  -57.2389,\n",
      "         -53.3800,  -81.2182,  -78.4406,  -79.4466,  -70.8893,  -70.8632,\n",
      "         -52.7376, -100.1526,  -53.1455], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-395.7903, -440.9358, -420.8752, -338.8222, -432.1245, -395.5742,\n",
      "        -392.6073, -354.6242, -412.4714, -508.5957, -446.6113, -370.5548,\n",
      "        -367.0892, -436.2571, -470.4897, -462.0109, -478.9993, -475.6383,\n",
      "        -425.0326, -488.6588, -339.8224], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-152.7462, -111.0125, -185.9613,  -63.7566, -105.8561, -116.3626,\n",
      "        -168.4650,  -63.0162,  -47.2349, -241.9243, -120.5503, -202.4254,\n",
      "        -202.8398, -151.8376, -138.8501, -135.8893, -180.8927, -103.0346,\n",
      "        -155.4185, -252.1033, -181.3919], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-11.3379, -10.5959, -12.7530, -17.6762, -15.6050,  -6.9263, -10.6811,\n",
      "        -13.2255, -22.3813, -21.2483, -17.5391, -12.0806, -14.4067, -19.3124,\n",
      "        -22.8024, -24.0917,  -9.3245, -18.6543,  -9.5513, -22.0674, -11.6063],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-202.0168, -211.5033, -208.3833, -201.9761, -181.0380, -118.2698,\n",
      "        -205.1110, -160.6507, -205.3584, -182.1024, -275.7821, -184.4996,\n",
      "        -195.6323, -293.8190, -358.5323, -376.2373, -159.7616, -304.3830,\n",
      "        -175.2624, -357.3687, -233.9666], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-36.3228, -37.6231, -37.3135, -36.9173, -42.6357, -38.0058, -37.4030,\n",
      "        -40.0517, -36.6023, -44.0298, -37.1705, -41.7121, -37.5756, -34.1556,\n",
      "        -36.7161, -37.3598, -42.2959, -35.3502, -39.1621, -39.8275, -26.4751],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -50.1033,  -62.1066,  -62.2111,  -35.3585,  -35.2946,  -48.5380,\n",
      "         -74.9147,  -50.3209,  -50.4581,  -66.6749,  -71.2708,  -70.7207,\n",
      "         -28.4015,  -98.7429,  -98.6236,  -66.1383, -103.2981,  -91.6731,\n",
      "         -46.5062,  -68.9752,  -31.1860], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.9338, -18.6904, -18.9465, -19.4158, -19.1687, -15.2859, -19.4672,\n",
      "        -18.4688, -18.0942, -17.5182, -18.9597, -18.1900, -16.8834, -18.7281,\n",
      "        -22.2386, -22.5180, -17.9212, -17.8593, -17.0653, -18.7016, -11.8206],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -26.4612,  -40.8064,  -67.9074,  -47.0696,  -27.5884,  -62.5270,\n",
      "         -30.6883,  -60.1473,  -47.3903,   -1.4484,  -67.6734,  -93.7767,\n",
      "        -186.9467,  -34.9957,  -62.0365, -124.1958,  -37.9567,   -1.5220,\n",
      "        -127.1164,  -75.9937,  -72.4056], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-25.1878, -20.0969, -28.7237, -18.5672, -22.1754, -22.7482, -29.1174,\n",
      "        -18.6551, -13.2348, -33.0292, -19.2909, -31.9738, -31.2470, -19.5730,\n",
      "        -22.9782, -23.6518, -28.1834, -15.8819, -26.2279, -32.4287, -23.6234],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.5138, -10.2116, -11.8445,  -4.8745, -11.2230, -10.3584,  -7.1468,\n",
      "         -4.5066,  -8.9785, -19.7229,  -8.9779,  -5.5921,  -9.2025, -12.3413,\n",
      "         -4.4407,  -3.4188, -15.7279,  -5.7393, -10.3915,  -5.5532,  -0.9014],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-401.4140, -420.2841, -399.6559, -428.2536, -455.2959, -315.5514,\n",
      "        -372.2234, -413.5207, -443.9630, -416.8393, -414.5266, -395.1732,\n",
      "        -372.2076, -408.2073, -400.1505, -421.5079, -333.9022, -428.5683,\n",
      "        -373.7004, -386.8095, -290.6513], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-25.1350, -27.0955, -27.0561, -24.9442, -25.3583, -21.3244, -27.1343,\n",
      "        -25.1256, -25.4238, -25.1710, -25.4691, -25.2280, -20.7495, -27.5538,\n",
      "        -29.0709, -27.3595, -26.1199, -25.0765, -23.2967, -22.1318, -14.6865],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-103.8711,   42.1150, -156.9312, -155.9353,  -19.9748,  -41.8621,\n",
      "        -155.7240,   35.5061, -173.8913,  -60.5445, -302.9205, -158.9098,\n",
      "          90.8862, -286.0466,  -47.5314, -149.0984, -183.8911,   20.2331,\n",
      "        -181.6218, -213.0873, -190.9993], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -79.3457,  -45.0081,  -98.7864, -237.2073, -130.7218, -147.0103,\n",
      "        -128.0573, -179.9500,  -93.3820,  -66.5801,  -96.7911, -223.3112,\n",
      "        -150.2274, -179.7670, -133.6242, -158.0983, -185.5117, -133.8830,\n",
      "        -147.6404, -225.2427, -100.0753], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([   2.4532,    9.3669,   30.1722,  -35.8676,  -60.9466, -131.4999,\n",
      "        -142.9375,   -8.1401,   69.6227,  -37.0227,  -75.1785, -120.1339,\n",
      "        -164.7638, -158.9877,  -85.2993, -197.1734, -283.4745,  -59.1667,\n",
      "        -157.2694, -417.2979, -240.5506], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-161.6120, -164.7955, -138.6977, -204.1428, -227.6105, -267.9030,\n",
      "        -274.2232, -287.3318, -180.7633, -131.0069, -193.2471, -252.1214,\n",
      "        -213.7334, -116.2237, -235.6821, -184.0774, -259.8484, -200.9489,\n",
      "        -266.9283, -189.6294, -193.7856], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-183.9803, -191.2196, -175.1846, -212.9098, -210.2008, -180.3079,\n",
      "        -200.0717, -221.0156, -195.8282, -171.7468, -190.0468, -214.3330,\n",
      "        -180.0776, -181.8429, -204.4445, -219.0162, -183.2480, -202.4083,\n",
      "        -191.2585, -206.2435, -152.6515], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -55.4862,  -43.3778,  -40.9517,  -92.6883,  -87.5590,  -41.5672,\n",
      "         -48.8940,  -62.6004,  -63.2939,  -46.2043,  -74.7462,  -52.9354,\n",
      "         -99.6717,  -36.4321,  -82.0667, -127.2865,  -19.5178,  -74.0240,\n",
      "         -64.4975, -132.5420,  -94.9899], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-101.0622, -106.9977, -104.3764,  -98.3691,  -93.8092, -101.6312,\n",
      "        -122.1569, -131.9348,  -91.5880, -108.0953, -111.4315, -168.7148,\n",
      "         -88.9527, -153.6541, -116.5240,  -96.4716, -141.0739, -108.3448,\n",
      "        -100.2481, -123.2695,  -66.8952], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-46.5929, -54.9763, -49.4661, -40.9268, -52.6070, -64.1907, -60.9323,\n",
      "        -54.5362, -44.1849, -63.6453, -55.2465, -66.1895, -45.4270, -68.7955,\n",
      "        -56.0487, -51.0794, -90.4464, -54.3185, -60.7278, -68.8611, -38.8913],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -21.5694,  -21.7021,  -40.6425,  -49.5698,  -40.6761,  -74.5663,\n",
      "         -42.7514,  -56.1197,  -55.9878,  -89.6012,  -63.6274,  -93.7741,\n",
      "        -125.0015,  -40.2218,  -89.0374, -153.6530, -153.8114, -138.7972,\n",
      "         -99.0573, -189.8569,  -95.1505], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -93.6512, -108.2687,  -97.8621,  -84.8044, -120.1164, -101.3210,\n",
      "         -95.0179,  -98.1299, -100.5286, -147.6475, -127.0770, -125.4695,\n",
      "        -105.6859, -157.7365, -100.2810, -100.5360, -159.9075, -114.8254,\n",
      "        -112.7113, -157.3271,  -81.6038], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-72.7177, -77.1545, -74.7476, -74.8953, -76.0107, -57.9170, -73.3590,\n",
      "        -73.3065, -76.9065, -71.1895, -74.5633, -71.8341, -62.5697, -77.6391,\n",
      "        -80.6250, -80.3242, -65.2394, -77.7525, -66.6570, -68.4895, -50.2514],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -95.2494, -148.2184,  -87.3264, -113.1265, -100.3206, -326.4769,\n",
      "        -318.4979, -291.5705,  -64.3470,  -71.6640, -177.3846, -348.4590,\n",
      "         -84.5476, -233.0839, -276.3413, -128.2582, -436.5676, -167.7460,\n",
      "        -206.3196, -191.4893,  -88.5896], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -67.4719,  -94.4054,  -52.3386,  -84.3823,  -92.9766, -268.4327,\n",
      "        -188.7812, -212.1971,  -44.2881,  -82.8280,  -59.3043, -242.5028,\n",
      "         -69.3272,  -87.3402,  -76.7967,  -45.3608, -316.6262,  -70.1467,\n",
      "        -175.8195, -119.1939,  -61.8160], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-204.3693, -222.3362, -210.9390, -193.7387, -225.8428, -198.8136,\n",
      "        -206.7656, -198.6911, -214.4301, -240.4395, -226.1390, -201.8302,\n",
      "        -193.7783, -218.3598, -241.0295, -244.1841, -229.3972, -240.0436,\n",
      "        -215.4080, -247.6859, -174.7954], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.5677,  -8.2088,  -7.7851, -20.4560, -21.8884,  -9.1462,  -6.5223,\n",
      "        -17.8691, -14.4464, -12.3830, -10.8544, -14.2125, -19.1759,  -3.6753,\n",
      "         -2.7359,  -9.7992,  -4.7617,  -4.8891, -11.6409, -11.5734,  -6.1759],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -42.6439,  -44.2699,  -49.4315,  -45.5102,  -75.6480,  -22.9718,\n",
      "         -56.3053,  -51.9010,  -52.8179,  -68.2617, -106.8008,  -53.5899,\n",
      "         -76.3046,  -59.1088, -109.0059,  -34.6413,  -32.0435,  -56.3095,\n",
      "         -28.2568,  -38.0422,  -20.6180], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-42.7430, -46.6673, -45.7120, -42.0396, -47.3945, -30.3535, -37.9291,\n",
      "        -43.6461, -47.8169, -48.5585, -40.9534, -43.3240, -33.2531, -49.0253,\n",
      "        -32.0937, -27.6094, -39.4679, -37.0948, -35.7737, -23.0586, -14.7462],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-35.7739, -39.1515, -38.1053, -33.3304, -41.8860, -37.4015, -35.8880,\n",
      "        -36.4361, -37.2914, -46.6631, -37.6648, -37.4063, -34.3283, -36.7405,\n",
      "        -35.8358, -34.6051, -45.0797, -35.8974, -38.5925, -36.8606, -23.7014],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.7739,  -7.1589,  -7.1897, -15.8484, -16.9799,  -7.4907,  -6.0526,\n",
      "        -13.8553, -11.4060, -10.3639,  -8.9485, -11.2531, -14.8059,  -3.9216,\n",
      "         -3.3425,  -8.1371,  -5.0395,  -3.9027,  -9.3358,  -8.7952,  -4.1742],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-109.9386, -115.9271, -110.7013, -116.7898, -118.8904,  -89.9130,\n",
      "        -109.9616, -115.6542, -117.9828, -107.6074, -111.9714, -112.2209,\n",
      "         -97.4239, -114.5511, -116.7487, -118.7375,  -97.7977, -116.8547,\n",
      "        -102.5884, -103.8938,  -77.3392], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-293.2181, -276.3917, -335.2238, -153.7672, -328.6365, -424.6443,\n",
      "        -404.0295, -164.6036, -142.7746, -508.9965, -203.7938, -328.7823,\n",
      "        -395.7601, -240.5012, -166.5432, -211.1401, -590.3979, -188.9454,\n",
      "        -447.7840, -443.8286, -361.0129], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -90.2568,  -96.5090,  -95.0873,  -88.7965,  -89.9699,  -45.8574,\n",
      "         -79.3004,  -75.0680,  -98.5525,  -96.7674, -119.8524,  -87.0031,\n",
      "         -84.4806, -138.8912, -128.5549, -130.6185,  -73.1466, -120.3500,\n",
      "         -72.8547, -131.5456,  -78.2296], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.1068,  -8.4553,  -6.1691,  -7.3738,  -9.3777, -20.2683,  -9.3693,\n",
      "         -8.5146, -12.5896,  -7.8206,  -6.9627,  -5.7176,  -5.2029,  -5.1491,\n",
      "        -12.1874,  -7.9711,  -7.4724,  -8.0790, -14.3141,  -6.9581,  -4.1613],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-332.8989, -384.8871, -310.8796, -339.3107, -499.5165, -569.2471,\n",
      "        -420.5987, -438.5216, -336.8787, -589.0012, -539.9615, -569.2797,\n",
      "        -514.2589, -565.4016, -495.9769, -563.1636, -786.1869, -515.6946,\n",
      "        -557.0815, -920.6608, -503.0083], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -86.7654,  -90.2467,  -92.1789,  -81.7154,  -87.0157,  -62.8312,\n",
      "         -85.0283,  -74.5191,  -88.1765,  -91.0697,  -96.7444,  -76.6557,\n",
      "         -79.6608,  -92.1905, -112.4045, -108.4683,  -71.4134,  -99.9665,\n",
      "         -77.1351,  -95.9977,  -70.3498], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-368.5172, -351.2244, -448.5825, -215.6811, -300.5150, -302.6915,\n",
      "        -341.9735, -246.2455, -323.3727, -639.6313, -356.6933, -410.6889,\n",
      "        -331.1986, -375.3647, -428.2393, -379.5933, -517.8823, -523.7018,\n",
      "        -341.2275, -501.6673, -305.7967], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-18.1746, -19.3877, -19.6776, -18.5937, -18.7004, -15.5545, -19.8916,\n",
      "        -18.3897, -18.1952, -18.1671, -18.5790, -18.3815, -15.6762, -19.6114,\n",
      "        -21.5341, -20.5819, -19.1436, -17.4821, -16.9650, -16.4684, -10.2902],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -60.3531,  -59.8130,  -59.1701,  -68.0744,  -58.2559,  -46.7288,\n",
      "         -67.7208,  -54.1395,  -58.8831,  -46.7210,  -76.1974,  -53.9199,\n",
      "         -65.2490,  -65.0716, -109.3910, -121.8950,  -45.8643,  -86.8237,\n",
      "         -59.3274, -108.5519,  -77.4748], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -54.9667,  -49.5126,  -39.0294,  -85.3635,  -99.9395,  -91.5754,\n",
      "         -67.5151,  -75.3193,  -56.4568,  -79.9931, -100.1540,  -84.8223,\n",
      "        -123.4944,  -62.3375, -110.4997, -158.2489,  -95.9252,  -98.0593,\n",
      "        -100.8254, -213.4935, -131.3799], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-21.6953, -22.6808, -21.8648, -24.8495, -24.5892, -22.4902, -25.1999,\n",
      "        -25.9542, -21.9505, -21.0534, -22.4362, -25.5863, -21.4094, -21.6533,\n",
      "        -25.6097, -26.3510, -24.9533, -21.5820, -22.9454, -23.6808, -15.3643],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-58.7375, -63.5430, -64.2625, -50.6367, -54.9866, -45.0624, -59.3922,\n",
      "        -43.2744, -58.3603, -63.7458, -69.5834, -45.7687, -52.1574, -68.6451,\n",
      "        -88.1453, -88.1633, -56.4228, -75.9261, -55.2314, -79.6234, -55.3577],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -47.0220,  -34.1976,  -30.8987,  -87.9676,  -78.8157,  -35.2041,\n",
      "         -38.4278,  -62.2533,  -57.1108,  -34.4930,  -67.5862,  -50.3314,\n",
      "         -90.8713,  -25.6553,  -75.4089, -118.3324,   -6.8370,  -66.3530,\n",
      "         -54.2381, -121.5208,  -85.0385], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-24.9225, -28.0307, -29.9404, -17.2433, -28.8354, -16.7534, -27.3517,\n",
      "        -10.4364, -21.9116, -37.1303, -21.3549, -18.0144, -23.2978, -38.0890,\n",
      "         -9.3794, -11.0550, -38.8286, -15.1951, -26.3104, -18.9386, -12.8095],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-181.5322, -201.9156, -202.8154, -137.1744, -209.5241, -137.0508,\n",
      "        -133.2978, -119.7050, -202.2086, -273.1414, -210.6590, -122.8634,\n",
      "        -171.0089, -200.1930, -184.8252, -183.1614, -181.0547, -208.0778,\n",
      "        -173.9774, -201.2149, -135.3546], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-225.6410, -252.8534, -224.0404, -206.8691, -346.6100, -277.4932,\n",
      "        -179.6326, -209.8746, -260.0673, -450.0948, -408.9021, -278.1407,\n",
      "        -362.7515, -372.4212, -359.4814, -419.8329, -396.9916, -379.6205,\n",
      "        -321.5200, -645.5108, -345.3962], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -5.3965,  -5.3694,  -6.2213,  -6.4840,  -2.9279,  -3.3943,  -9.3107,\n",
      "         -3.4496,  -3.1978,  -3.6231, -14.3765,  -7.5918,  -9.0644, -15.8458,\n",
      "        -24.7062, -26.4368,  -8.9938, -13.8776,  -5.9282, -27.3765, -13.1232],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -8.4142, -10.5048,  -9.6846,  -8.5990,  -9.5086, -28.4710, -10.9636,\n",
      "         -9.9496, -18.5269, -14.5215, -14.2687,  -6.3179, -19.7594, -12.1749,\n",
      "        -10.3253, -22.0506,  -3.3250,  -9.5490, -10.8324, -15.3308,  -6.4525],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-115.8516, -122.3131, -122.9243, -111.4621, -118.0207,  -65.2062,\n",
      "         -98.5636,  -95.4580, -127.0123, -119.2153, -120.7384,  -91.4195,\n",
      "         -95.0843, -127.1409, -122.9328, -122.5047,  -76.9046, -124.5987,\n",
      "         -91.6460,  -98.2690,  -74.4086], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  -3.0490,  -36.9208,  -18.7008,   -1.4937,  -15.6187,  -10.9945,\n",
      "          -9.0813,   -5.9043, -146.8358,  -25.4863,  -46.0606,  -32.0040,\n",
      "         -18.5399,  -22.5552,  -20.6554,  -27.2210,   -2.9663,   17.5885,\n",
      "         -52.7175,  -42.0344,  -47.9275], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-35.5826, -37.5470, -39.6856, -31.8513, -31.6131, -17.7941, -32.6683,\n",
      "        -23.6114, -36.3175, -34.6323, -40.2261, -23.7605, -28.8182, -41.8730,\n",
      "        -50.1604, -49.6929, -23.6280, -42.2459, -27.1701, -38.4889, -27.0989],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-150.5581, -130.5402, -137.8772, -156.0208, -217.5919, -228.5977,\n",
      "        -152.5699, -197.9147, -124.6881, -274.3022, -220.0260, -278.4685,\n",
      "        -276.7547, -162.8629, -206.8530, -248.7805, -255.8954, -195.5586,\n",
      "        -228.0547, -422.9210, -242.9883], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-45.6806, -46.8375, -45.9656, -49.7805, -48.7018, -42.4179, -61.7775,\n",
      "        -42.6917, -39.7675, -38.2061, -39.8692, -47.3494, -45.5451, -52.1545,\n",
      "        -45.0393, -53.1434, -56.9450, -41.4500, -50.9114, -51.0257, -41.4820],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -59.6959,  -77.5284,  -68.4666,  -37.6810,  -73.9454,  -68.5773,\n",
      "         -56.3268,  -55.4559,  -65.2987, -127.6384, -120.8175,  -96.3905,\n",
      "         -74.9870, -158.5867, -105.9126,  -96.2617, -141.3789, -107.4303,\n",
      "         -77.7113, -170.1135,  -69.5937], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -20.7896,  -26.8061,  -21.2443,  -17.4598,  -42.3767,  -43.5731,\n",
      "         -49.3982,  -16.5340,   -7.8914,  -57.3210,  -41.3313,  -56.8320,\n",
      "         -53.0909,  -93.2825,  -13.1852,  -29.6011, -114.3464,  -24.0829,\n",
      "         -56.2804, -103.7178,  -48.1329], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-264.9004, -286.5636, -276.6140, -245.2715, -279.6795, -233.7726,\n",
      "        -262.2924, -235.6016, -276.0592, -302.5809, -300.4404, -240.3936,\n",
      "        -247.1945, -290.4429, -336.7862, -342.9468, -272.2981, -324.6172,\n",
      "        -266.8716, -334.6183, -238.4177], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-122.4359, -104.8000, -127.8142, -110.0398,  -98.3394,  -86.1731,\n",
      "        -173.8396,  -58.7857,  -65.9759, -113.7595, -151.2841, -131.4900,\n",
      "        -172.4970, -185.8595, -222.3460, -268.7983, -147.0880, -165.1317,\n",
      "        -140.6870, -317.6183, -222.7267], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-20.9930, -21.9093, -21.4698, -23.6451, -23.2581, -20.0982, -23.6882,\n",
      "        -23.9449, -21.3163, -20.1953, -21.6991, -23.5305, -20.1677, -21.2931,\n",
      "        -24.8284, -25.3545, -22.5997, -20.7698, -21.1887, -21.9528, -14.2401],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-214.0295, -234.3243, -212.1176, -216.1341, -301.3093, -226.5219,\n",
      "        -177.7169, -233.1713, -249.8565, -334.0122, -273.4034, -250.6160,\n",
      "        -257.5959, -261.9378, -202.0979, -224.4472, -283.7491, -250.1923,\n",
      "        -250.6544, -321.0800, -186.3819], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-17.5545, -18.6905, -18.9940, -18.0782, -18.1512, -15.1121, -19.2747,\n",
      "        -17.8563, -17.5518, -17.5363, -17.9772, -17.8323, -15.2837, -18.8703,\n",
      "        -20.8778, -20.0319, -18.5646, -16.8170, -16.4523, -16.0560,  -9.9584],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-221.2357, -242.5789, -217.8548, -261.1866, -301.8206, -217.3354,\n",
      "        -271.0691, -280.8966, -260.8146, -225.0088, -301.5334, -240.3760,\n",
      "        -244.4913, -213.2654, -320.5251, -225.4009, -217.4216, -255.2210,\n",
      "        -219.7099, -181.5790, -149.1606], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  -0.8345,   -3.5475,    2.3427,    1.7429,  -37.2991,  -44.8904,\n",
      "           4.1623,   -3.5896,   -3.6024,  -66.0587,  -48.8321,  -21.6357,\n",
      "         -55.6662,  -25.2784,  -35.3972,  -56.0023,  -68.9432,  -35.4860,\n",
      "         -43.0895, -129.8053,  -57.8525], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.6001, -12.1584, -13.2549, -13.8192, -19.9987, -12.6519, -10.4979,\n",
      "        -19.9533, -14.3809, -17.2157, -18.6670, -20.8726, -25.9538, -21.1220,\n",
      "        -25.1903, -23.9858, -15.9811, -35.9593, -17.0365, -38.5097, -15.7390],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-286.5834, -197.0574, -345.5686, -159.1949, -130.6200, -102.6142,\n",
      "        -242.2617, -212.2356, -136.2032, -317.7472, -147.2769, -398.6520,\n",
      "        -239.3427, -185.3974, -188.6246,  -89.4846,  -79.4081, -125.3063,\n",
      "        -142.3051, -112.4098, -122.4003], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -3.5537, -55.1307, -20.2367, -16.2817,   4.7574,  -5.4734,  -3.7121,\n",
      "        -52.1072, -20.0152,  -0.8733, -32.2526, -19.8709, -21.5809, -26.5324,\n",
      "          0.4351, -30.5458, -36.3189,  -7.7908,  -4.6876, -34.7443,  -6.9661],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -334.9935,  -379.7858,  -341.2113,  -302.2757,  -363.6004,  -343.6957,\n",
      "         -370.2833,  -287.2614,  -333.5746,  -496.7104,  -669.5048,  -447.1695,\n",
      "         -471.8828,  -723.4243,  -810.5640,  -867.8055,  -571.1401,  -693.8248,\n",
      "         -427.3221, -1118.9330,  -607.2054], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-37.0277, -38.5186, -41.2146, -30.9252, -26.8709, -35.3114, -47.7429,\n",
      "        -40.7678, -28.6713, -36.7664, -39.1935, -55.4563, -29.4808, -53.0048,\n",
      "        -52.0213, -42.3358, -49.6697, -40.0084, -34.7414, -45.1599, -26.1063],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -45.3643,  -30.0054,  -28.6664,  -84.5837,  -45.1408,  -30.5759,\n",
      "         -65.9387,  -37.5149,  -36.5360,   -4.3399, -116.7225,  -46.9592,\n",
      "        -108.3056,  -71.4784, -209.8020, -267.7567,  -13.9270, -139.9885,\n",
      "         -60.9864, -272.1072, -176.3172], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-307.0034, -324.5418, -292.7026, -346.6619, -373.1143, -275.2845,\n",
      "        -301.4721, -342.7609, -339.4563, -362.8106, -429.0833, -386.1341,\n",
      "        -363.8872, -444.6892, -435.7636, -481.5266, -351.5792, -428.3685,\n",
      "        -331.0527, -572.2059, -340.0712], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-165.1158, -199.4139, -164.2576, -169.9524, -152.9075, -224.1934,\n",
      "        -249.9178, -251.2466, -155.8890, -163.3978, -220.6479, -315.4401,\n",
      "        -136.7734, -323.7768, -260.2367, -220.6274, -330.8133, -232.7344,\n",
      "        -200.6750, -280.5114, -142.2517], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -66.0122,  -68.9194,  -62.7953,  -71.8286,  -92.3402,  -85.9525,\n",
      "         -67.2136,  -73.1452,  -70.5113,  -93.2111,  -84.1826,  -73.2522,\n",
      "         -89.2198,  -59.6353,  -85.7780, -102.0419,  -90.8253,  -84.2860,\n",
      "         -88.5750, -123.7866,  -80.9928], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-48.2549, -47.1930, -52.5937, -27.9750, -40.8706, -75.3207, -35.5117,\n",
      "        -69.1653, -46.8585, -64.7731, -44.9287, -54.0728, -70.7427, -71.4658,\n",
      "        -77.2075, -38.7878, -54.8957, -47.6473, -33.3666, -42.5942, -23.3209],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-10.4609,  -9.5928, -10.8039, -10.5265, -10.8751,  -7.7940,  -9.7023,\n",
      "        -11.0354, -14.2305,  -7.5708,  -9.3697, -10.5341, -10.8372, -12.0148,\n",
      "        -11.4113, -14.0538, -10.6429,  -8.5684, -22.9452, -19.2293,  -7.4096],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  16.8789,    0.4755,   12.4658,   28.3550,  -95.4102,  -83.5715,\n",
      "          -1.5662,    2.0130,   -9.2793, -119.2683,  -93.5647,   23.3228,\n",
      "         -89.8449,   46.2157,  -59.2276,   18.6924,  -96.2942,  -16.5289,\n",
      "         -52.9307,  -59.7565,  -23.7471], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -59.0491,   37.5255, -104.8753,   74.9364,  -62.2979,  -38.7514,\n",
      "        -153.4238,   26.7490, -103.6240, -192.2428, -242.1743,  -42.7761,\n",
      "        -113.6359,   -1.1923,  -94.0584,   74.9778,  -73.0998,    9.2212,\n",
      "         -41.5075,  -71.0985,  -16.2174], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-130.6400, -126.0242, -115.7446, -140.5233, -184.3034, -276.9974,\n",
      "        -186.9579, -210.2945, -101.2494, -201.9877, -144.7163, -247.5009,\n",
      "        -206.3927,  -79.9171, -162.7242, -184.8856, -280.0974, -147.9308,\n",
      "        -231.2904, -289.2686, -190.2340], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 4 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -392.8082,  -831.6158,  -515.5907,  -320.8679,  -252.7172,  -291.7910,\n",
      "         -422.9835,  -292.1960,  -938.1618,  -603.1133,  -762.1952,  -806.5872,\n",
      "         -583.5249,  -663.1625,  -188.1760,  -481.5096,  -391.1190,  -292.7047,\n",
      "         -991.8286, -1026.8149,  -874.1256], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-124.9551, -152.4401, -114.7225, -130.5752, -203.9262, -237.1221,\n",
      "        -151.8796, -200.0244, -137.9061, -254.1083, -229.5962, -263.9356,\n",
      "        -201.5871, -253.7550, -186.6986, -198.0802, -340.9291, -204.7981,\n",
      "        -219.3973, -375.1144, -175.8488], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 5 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -63.8037,  -99.5401,  -80.2232,  -14.8749,  -93.1144, -120.1745,\n",
      "         -66.4331,  -72.4172,  -70.3546, -201.4299, -154.1347, -145.6088,\n",
      "         -85.9957, -225.6552, -105.8781,  -74.4064, -252.2476, -125.6827,\n",
      "        -112.6414, -221.7701,  -68.6017], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-26.3942, -36.3570, -35.9533, -15.6388, -44.4624, -24.8889, -36.6597,\n",
      "        -19.0620, -35.5682, -50.4467, -60.6240, -10.1575, -32.4330, -31.4797,\n",
      "        -65.3644, -16.5737, -37.8080, -37.2910, -23.3539,  -7.3376,  -5.7684],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -4.6265,  -3.9395,  -4.8371,  -4.6840,  -8.4345, -11.7583,  -7.0432,\n",
      "         -0.6576,  -2.3484, -11.5733, -14.3893,  -2.1871, -16.0380,  -3.6608,\n",
      "        -23.8893, -30.3948, -13.2095, -14.0624, -12.5723, -36.2995, -20.9085],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ 120.1764,   68.7446,   86.1396,   68.4493,  -74.9883,   88.9377,\n",
      "          84.9610,   75.7621, -387.3455,  116.2196,   85.9478,   98.9367,\n",
      "         -46.4511,   28.8960,  -86.7295,   70.1473,  153.1425,  195.6301,\n",
      "         -37.3566,   79.1272,   -8.8116], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-125.2905, -129.1625, -130.7655, -124.3046, -122.3120,  -69.4426,\n",
      "        -112.7462,  -96.9747, -132.9532, -118.3054, -142.4100,  -93.8456,\n",
      "        -112.5948, -138.9405, -169.1218, -178.4335,  -76.7765, -154.3360,\n",
      "        -102.2868, -149.0963, -110.6487], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -95.4520,  -74.5351,  -95.5053,  -67.2623,  -84.0257, -141.9526,\n",
      "         -42.2876, -143.6714,  -97.6126, -102.0995,  -56.4961,  -69.8316,\n",
      "        -165.2299,  -78.5510, -139.0192,  -65.8041,  -25.3891,  -70.2035,\n",
      "         -41.8975,  -33.8862,  -41.7139], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-151.4121, -166.3946, -146.5571, -166.9565, -160.5176, -191.0408,\n",
      "        -198.6952, -207.1610, -150.0979, -135.4183, -143.7470, -210.1641,\n",
      "        -128.3456, -157.8260, -167.8089, -157.7053, -209.3391, -160.2635,\n",
      "        -173.9635, -149.2213, -110.6768], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-7.8114e+01, -5.3694e+01, -8.0884e+01, -4.3874e-01, -8.9297e-01,\n",
      "        -1.5272e+01, -3.1123e+01, -4.8802e+01, -2.3047e+02,  7.0662e+01,\n",
      "         4.7389e+00, -3.3016e+01, -4.2830e+01, -1.2972e+02, -6.1142e+01,\n",
      "        -1.1466e+02, -7.6768e+01, -7.5887e+01, -6.3257e+02, -4.3119e+02,\n",
      "        -1.1739e+02], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -75.8808,  -80.9058,  -73.7636,  -86.9159,  -81.8630,  -62.4273,\n",
      "         -82.5897,  -87.7334,  -80.6450,  -75.7423, -105.0207, -101.8425,\n",
      "         -80.8661, -121.5666, -116.4777, -121.6619,  -86.4458, -105.5698,\n",
      "         -75.6106, -134.6813,  -77.0788], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -79.5366,  -81.8401,  -91.9977,  -63.3784,  -49.0915,  -52.8354,\n",
      "         -86.3870,  -83.6400,  -75.4802,  -68.2232,  -64.9035,  -78.4470,\n",
      "         -41.6707,  -80.8425,  -83.2222,  -63.8599, -102.0202,  -73.9234,\n",
      "         -57.7284,  -40.8069,  -36.7408], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -3.7693,  -2.3258,  -2.9356,  -8.0681,  -0.9685,  -3.1216, -10.3960,\n",
      "         -2.2562,  -0.2715,   1.4947, -18.4100,  -7.5783, -13.2735, -16.3329,\n",
      "        -36.3057, -42.5167,  -7.1460, -19.7665,  -6.7680, -45.1316, -24.0681],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-189.6365, -204.5661, -161.2080, -249.1679, -216.3081, -331.2738,\n",
      "        -311.2639, -334.7369, -174.0384, -138.0315, -185.5310, -341.6830,\n",
      "        -195.1094, -179.9831, -254.6672, -261.0146, -337.3160, -220.9865,\n",
      "        -273.5383, -273.7434, -198.0637], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-27.4924, -24.8633, -22.2261, -43.6429, -42.6394, -22.0716, -23.9091,\n",
      "        -38.6917, -33.3460, -28.2401, -38.3705, -38.3445, -42.7307, -31.8427,\n",
      "        -33.6608, -46.4152, -20.8639, -32.8553, -29.5764, -54.0621, -31.1002],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-118.4566, -126.0926, -130.0319, -104.5624, -108.3214,  -56.1773,\n",
      "        -101.1395,  -80.9167, -126.0285, -118.8055, -128.8810,  -79.7898,\n",
      "         -91.3152, -138.2624, -146.8188, -143.5739,  -72.5331, -138.2147,\n",
      "         -87.9837, -111.1833,  -83.9882], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-40.0718, -49.3208, -41.0006, -63.1060, -41.6079, -48.0561, -34.2451,\n",
      "        -43.8829, -22.3444, -36.8392, -57.4435, -75.1979,  -6.9592, -71.1736,\n",
      "        -66.8256, -70.7237, -72.7183, -36.3038, -62.7145, -45.6332, -61.5988],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-246.4539, -283.0141, -254.5705, -183.9400, -421.5281, -375.6389,\n",
      "        -231.9199, -170.1511, -252.3059, -584.9628, -428.4330, -277.4427,\n",
      "        -448.5361, -408.7649, -321.7054, -422.6820, -582.5463, -381.2721,\n",
      "        -431.5251, -767.7064, -431.2023], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -1.0701,  -4.4328,  -4.6563,  -2.3773, -23.5910,  -9.3563,  -5.0075,\n",
      "        -20.8553,  -4.7951, -18.9722,  -8.3649,  -4.3467, -14.3563,  -6.6591,\n",
      "        -16.9677, -17.8523, -29.5772,  -8.4822,  -2.7359, -30.0091, -18.6964],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-202.7017, -214.2582, -198.1467, -224.9755, -231.2767, -186.7285,\n",
      "        -208.4437, -238.3018, -220.5891, -199.9512, -198.0275, -229.9831,\n",
      "        -182.8959, -203.0134, -190.5640, -194.2970, -197.2245, -204.7734,\n",
      "        -199.6944, -177.8959, -134.5478], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-12.8556, -12.5642, -12.4300, -16.4520, -15.7381, -12.9843, -14.8140,\n",
      "        -16.0073, -13.0049, -12.1506, -14.3019, -14.9085, -15.1138, -12.5438,\n",
      "        -16.8800, -18.9447, -13.9068, -12.4336, -13.9067, -17.1255, -10.0268],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([   9.0007,    6.7088,  -11.0218,  -66.1787,  -13.4680,   11.5621,\n",
      "         -33.0162,  -63.9379,  -33.9896,  -25.9651,  -51.7146, -116.3772,\n",
      "         -10.1862, -101.3210, -138.6257,  -48.1929, -112.6305, -265.9106,\n",
      "         -16.0712,  -68.1930,    5.1022], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -77.9282,  -46.1912,  -87.6094,  -94.3154,  -49.7324,  -55.0946,\n",
      "         -72.3586,  -41.0440,  -88.5129,  -70.0596, -141.6313, -131.7794,\n",
      "           9.8317, -165.9388,  -57.1671, -108.7763, -121.8405,  -60.8639,\n",
      "        -117.8423, -130.3755, -115.6007], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-5.9299, -6.9533, -8.4232, -3.7327, -4.4940, -3.5729, -6.5026, -3.4943,\n",
      "        -5.1344, -7.3680, -5.4092, -4.0616, -2.8262, -7.8034, -6.7255, -3.9354,\n",
      "        -7.7129, -3.1051, -3.9849, -0.5588,  1.5693], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-41.3547, -43.1671, -65.3214, -83.8178, -50.2349, -44.9966, -39.2077,\n",
      "        -37.9773, -38.4975, -37.4410, -47.9800, -46.4504, -40.3166, -67.6680,\n",
      "        -63.2463, -59.4598, -48.2276, -36.2506, -58.3231, -38.8053, -36.4852],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-22.8367, -46.4757, -24.8868, -36.7718, -34.0742, -40.5945, -32.0196,\n",
      "        -49.6332, -30.0991, -22.2476, -36.2337, -38.0702, -35.9320, -28.7784,\n",
      "        -26.1195, -43.0239, -36.6415, -27.7012, -35.6886, -44.3375, -24.3899],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-109.1753, -116.4664, -111.7713, -111.7599, -113.1832,  -89.4643,\n",
      "        -111.7708, -111.6914, -115.2174, -106.1373, -111.2038, -109.9597,\n",
      "         -92.4168, -117.1743, -120.6574, -119.1395, -100.5298, -117.9119,\n",
      "        -101.1672, -102.2764,  -76.2430], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -5.5865,  -8.4426, -13.1732, -19.9252, -10.3214, -14.6062,  -7.7531,\n",
      "        -14.2581,  -4.4676, -13.9663, -17.9277, -29.6400, -29.7763,  -8.4990,\n",
      "        -19.0300, -20.8161, -18.1460,  -9.8072, -30.1348, -17.1113, -17.6425],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-137.6285, -151.2105, -149.5869, -118.5251, -171.8035, -105.9196,\n",
      "        -110.2468, -104.5930, -158.0808, -198.1872, -172.8006,  -98.4920,\n",
      "        -143.0545, -144.7983, -157.5810, -145.6570, -129.4478, -160.4413,\n",
      "        -132.3270, -152.2855, -105.2036], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-208.9278, -172.8331, -189.9079, -219.3353, -369.0158, -271.8615,\n",
      "        -332.7935,   14.5481, -108.5583, -327.8739, -218.0000,  -61.7267,\n",
      "        -488.8961, -193.8625, -193.5882, -478.9232, -425.0662, -214.3312,\n",
      "        -429.6999, -706.0648, -569.6865], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -122.8418,  -145.1577,  -228.6802,  -171.3936,  -271.2810,   -15.7264,\n",
      "         -693.4665,  -216.1876,  -431.2468,  -203.2803,  -913.3538,  -106.0579,\n",
      "         -180.5374,  -355.0828, -1231.5686,  -291.8816,  -429.9463,  -789.9827,\n",
      "           29.6549,   -48.4205,    31.0987], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  61.0542,  146.7698,   38.0062,  104.8991,   37.2773,   45.0077,\n",
      "          68.0463,   69.7463,   -4.9950,   51.9390,    6.2619,   18.9906,\n",
      "        -115.4784,   -9.8318,   25.1947,   25.1701,   49.9762, -252.4287,\n",
      "        -219.6526, -191.4860, -222.8282], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-23.5529, -24.3032, -23.2571, -27.7946, -27.5394, -24.2667, -26.8066,\n",
      "        -28.5331, -24.2055, -22.8386, -24.6822, -27.8269, -24.3057, -22.8914,\n",
      "        -27.6290, -29.4091, -26.0646, -23.7855, -25.1170, -27.0364, -17.7766],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -88.1695,  -88.8465,  -90.7905,  -88.3456,  -91.4814,  -66.3761,\n",
      "         -84.1454,  -82.0870,  -89.2470,  -91.5484,  -93.0064,  -85.0332,\n",
      "         -84.9959,  -88.0991, -103.5004, -107.5987,  -70.6015,  -96.6175,\n",
      "         -80.5435,  -99.6920,  -72.7740], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-20.1981, -26.5653, -21.9275, -17.2325, -19.4126, -28.7006, -35.4315,\n",
      "        -24.5669, -15.5856, -33.5436, -50.7680, -49.3484, -30.0669, -77.2914,\n",
      "        -59.7460, -58.2624, -66.5518, -47.1482, -31.9187, -90.7464, -38.5553],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -56.4258,  -31.9519,  -17.2972, -141.5624, -132.3460,  -58.2039,\n",
      "         -44.7405, -126.9276,  -72.7454,  -94.2131, -196.6036, -182.9321,\n",
      "        -209.5376, -174.6461, -187.5946, -275.6161,  -94.2176, -164.8872,\n",
      "        -100.8647, -430.9888, -212.0175], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -41.2697,  -29.8930,  -22.5655,  -87.8831,  -75.5851,  -27.8279,\n",
      "         -46.1019,  -61.1855,  -46.7265,  -33.6235,  -87.1066,  -76.8935,\n",
      "        -102.3825,  -84.1126,  -87.6017, -137.3453,  -37.8892,  -77.1275,\n",
      "         -57.1134, -180.8207, -103.2913], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-7.0418, -8.3710, -9.9371, -4.0602, -5.0642, -3.9194, -7.4218, -3.8811,\n",
      "        -6.2562, -8.7277, -6.3187, -4.5823, -2.8799, -9.4860, -7.6223, -4.1423,\n",
      "        -8.7145, -4.1194, -4.5221, -0.2668,  1.6793], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -53.1682, -130.5838,  -59.5136,  -27.2353, -137.7045,  -65.9246,\n",
      "         -27.9850, -163.2745, -107.5255, -365.5493, -456.1087, -461.5027,\n",
      "        -190.5338, -823.4601, -260.9549, -204.7099, -515.9415, -315.0431,\n",
      "        -119.4125, -740.3125, -139.2502], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([  61.6398,   16.1745,  -38.8339, -286.1415,   47.1892, -226.7924,\n",
      "         -26.1747, -186.7307,  157.6744, -300.7716, -135.7289, -411.0985,\n",
      "        -351.2256,    1.5130, -228.4447, -294.6898, -233.4411, -264.8761,\n",
      "        -413.5688, -272.5415, -305.0121], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-156.3154, -158.6393, -158.6583, -162.2023, -163.8149, -111.6671,\n",
      "        -146.9220, -152.0114, -163.6842, -154.0663, -159.5032, -152.0780,\n",
      "        -143.6682, -157.2933, -169.3681, -175.4534, -116.3291, -166.3986,\n",
      "        -137.5498, -155.1203, -117.4465], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-116.9558, -130.2313, -123.9408, -109.5674, -113.7149, -105.5029,\n",
      "        -127.5959, -122.9003, -120.3810, -116.5672, -110.9993, -124.8123,\n",
      "         -83.5895, -131.8252, -119.1381, -103.0256, -127.9443, -119.1155,\n",
      "        -109.3194,  -84.0137,  -63.9999], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -88.8310, -131.5990,  -90.9049, -185.2608, -122.5515, -118.0375,\n",
      "         -62.8522, -101.5818,  -31.0496,  -89.6535, -209.9812, -211.8683,\n",
      "          17.7870, -210.4367, -234.1447, -200.7639, -202.9804,  -84.8234,\n",
      "        -176.7794,  -93.4597, -199.0561], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -76.7523, -104.7941,  -96.1832,  -33.9754,  -68.5085,  -52.2206,\n",
      "         -70.5147,  -50.2170,  -81.4439, -156.8652, -176.6371, -122.1975,\n",
      "         -78.6577, -264.1863, -172.3939, -145.6206, -175.1388, -161.6849,\n",
      "         -78.1551, -239.7846,  -89.0263], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-16.8423, -17.8619, -17.8979, -17.9758, -18.4580, -16.0754, -18.8389,\n",
      "        -18.3082, -16.9054, -17.4266, -17.3529, -18.2207, -15.6955, -17.4740,\n",
      "        -19.6094, -19.2659, -19.1970, -15.9140, -16.8451, -16.4706, -10.0273],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -95.8859, -105.0978, -118.8318,  -54.0583,  -54.4164,  -35.0870,\n",
      "         -89.2407,  -38.8000,  -85.8825,  -96.6314,  -99.5676,  -53.7223,\n",
      "         -49.7196, -125.8764, -138.8241, -113.6174,  -64.5205, -114.4218,\n",
      "         -59.4218,  -76.8357,  -60.0886], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -0.3044,   3.2544,  -0.9285,   7.5427,   2.6266,   2.1196,  -0.9253,\n",
      "         -1.7276, -25.3932,  20.4134,   0.9416,   5.2856,  -5.0516,  -4.6271,\n",
      "        -11.3400,  -5.4106,  -5.4485,   0.4359, -88.0484, -52.0279,  -6.8201],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-13.3939, -17.0183, -13.5341, -13.5528, -18.6609, -29.3314, -22.3976,\n",
      "        -23.0452, -11.8624, -24.7652, -25.4493, -31.9479, -20.3292, -30.6352,\n",
      "        -27.4478, -26.3039, -44.2367, -22.2128, -24.9947, -43.4382, -18.4001],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-5.4411, -2.2234, -7.6209, -5.9315, -3.0607, -1.8651, -5.7506, -3.4802,\n",
      "        -1.8702, -6.9668, -1.6726, -7.1079, -8.1274, -5.7006, -2.0191, -2.8024,\n",
      "        -0.0710,  0.8758, -2.9898, -0.8486, -0.0698], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -18.5243,  -16.0462,  -20.5857,  -14.5056,  -10.9605,  -12.0709,\n",
      "         -23.5953,   -8.8269,   -9.2676,  -26.4761,  -49.6189,  -31.4845,\n",
      "         -36.4114,  -53.1046,  -74.9676,  -80.6101,  -29.6034,  -49.9354,\n",
      "         -21.7874, -102.4223,  -53.1220], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-13.1924,  -6.9940, -17.0509, -21.5114,  -8.0659,  -4.6603, -16.0236,\n",
      "         -7.6163, -10.8512, -14.2534,  -4.2280, -11.7046, -20.5683, -31.0821,\n",
      "         -0.4342, -10.3078,   4.9882,  -6.6060,  -9.3822,  -0.0374,  -3.8274],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -96.7252,  -94.6439,  -94.3241,  -55.6506, -135.6287,  -92.7412,\n",
      "        -179.9959, -122.2570, -165.8535,  -96.1113, -218.0438,  -87.4036,\n",
      "        -173.0728,  -40.6114, -209.7753, -108.7887,  -91.2671, -190.0103,\n",
      "        -232.2367, -117.0648, -225.2943], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -71.6370,  -86.5681,  -82.6277,  -44.2183,  -72.1191,  -77.2828,\n",
      "         -75.6310,  -47.0712,  -69.0831, -121.1495, -124.9123,  -76.7022,\n",
      "         -81.4878, -138.5916, -146.6012, -142.9993, -130.9676, -128.1736,\n",
      "         -88.3878, -185.5855,  -99.5360], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.6144, -1.4515, -2.5481, -2.4618, -2.2811, -2.2282, -2.9350, -1.9933,\n",
      "        -0.7807, -2.0913, -1.8798, -2.0410, -2.6178, -1.2736, -3.2452, -3.1326,\n",
      "        -3.8259,  0.8312, -1.8999, -1.6917,  1.1423], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-1.8011e+00, -1.5196e+01,  1.0286e+01,  5.9766e-02, -5.8711e+01,\n",
      "        -1.5831e+02, -6.6244e+01, -4.4768e+01,  1.9739e+01, -1.0737e+02,\n",
      "        -9.9006e+01, -1.1087e+02, -1.1150e+02, -9.4583e+01, -1.0880e+02,\n",
      "        -1.4382e+02, -2.4426e+02, -8.8299e+01, -1.2877e+02, -3.2051e+02,\n",
      "        -1.5320e+02], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-60.0406, -64.1396, -62.0427, -61.8029, -67.3645, -50.0367, -60.8774,\n",
      "        -60.4034, -64.7724, -63.6815, -66.5839, -57.9369, -56.2035, -62.8975,\n",
      "        -70.6748, -67.2065, -56.3221, -65.5316, -56.8465, -59.3814, -42.6881],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-30.5995, -35.9483, -36.8778, -20.7552, -19.7791, -25.2354, -37.9629,\n",
      "        -22.9227, -26.9587, -28.8344, -31.0603, -26.6672, -15.3006, -41.2999,\n",
      "        -45.5696, -35.8650, -37.8044, -35.3322, -26.2135, -24.7597, -17.1196],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-6.0750, -7.1384, -8.6208, -3.7755, -4.5684, -3.6181, -6.6226, -3.5448,\n",
      "        -5.2808, -7.5455, -5.5280, -4.1296, -2.8332, -8.0231, -6.8426, -3.9624,\n",
      "        -7.8437, -3.2375, -4.0550, -0.5206,  1.5836], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-102.1400,  -89.2548, -123.9667,  -49.4625,  -82.2566,  -90.0057,\n",
      "        -113.8237,  -53.0235,  -54.6610, -170.6805,  -94.8403, -129.0480,\n",
      "        -121.4276, -121.5572, -103.6908,  -98.8063, -152.7641,  -97.6840,\n",
      "        -110.1904, -168.6825, -110.3625], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 3 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-15.2162, -15.9880, -16.3003, -16.3891, -16.3345, -13.5315, -16.8917,\n",
      "        -16.0761, -15.2034, -15.1519, -15.7229, -15.9136, -14.0636, -15.8885,\n",
      "        -18.2457, -18.0019, -16.2871, -14.2515, -14.6152, -14.6714,  -8.8201],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-51.9988, -51.9457, -55.2553, -48.6177, -51.5741, -43.9755, -51.8650,\n",
      "        -45.7744, -56.5545, -58.3479, -59.2340, -54.0400, -36.3973, -61.9949,\n",
      "        -48.2649, -53.6007, -59.1119, -48.9004, -53.2447, -53.9625, -38.6643],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -76.6412,  -86.2672,  -79.4800,  -74.9669,  -73.6941,  -86.9095,\n",
      "         -97.2629,  -92.5542,  -74.7251,  -71.5770,  -72.1259,  -95.8510,\n",
      "         -56.0660,  -85.9831,  -86.1184,  -74.2857, -102.8949,  -79.7704,\n",
      "         -80.9946,  -64.1449,  -47.1802], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-34.4926, -37.6943, -37.0271, -32.4903, -35.6053, -32.2617, -37.1077,\n",
      "        -33.4227, -34.8128, -37.9727, -36.7259, -34.2683, -30.0240, -38.0202,\n",
      "        -41.8004, -40.0143, -39.4099, -37.4098, -34.4813, -36.3698, -24.3807],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -95.5850,  -91.1628, -104.6020,  -73.7030,  -94.8636,  -93.3850,\n",
      "         -91.9663,  -84.6736,  -79.4126, -131.6100,  -93.4524, -109.5695,\n",
      "        -102.2240,  -81.0153, -101.1173,  -95.8140, -102.6607,  -92.3418,\n",
      "         -98.3190, -117.6121,  -82.7713], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([-81.6673, -85.6624, -83.1024, -85.9705, -88.4053, -61.7703, -78.3366,\n",
      "        -81.7918, -88.2711, -81.3388, -84.1514, -78.8134, -72.8551, -85.2669,\n",
      "        -86.8682, -89.0628, -67.7255, -86.4659, -73.7430, -76.8368, -56.6203],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode will have ended forcefully\n",
      "It took 27 steps in testing\n",
      "Episode ended naturally\n",
      "Predicted value of all actions: tensor([ -58.0926,  -62.8437,  -47.7408,  -75.8693,  -88.8960, -102.3563,\n",
      "         -78.9293,  -89.9846,  -59.0655,  -90.1429, -115.8507, -115.1481,\n",
      "        -107.4890, -108.3017, -129.8257, -152.6781, -132.8114, -113.5190,\n",
      "        -101.4115, -218.7289, -117.4329], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "It took 1 steps in testing\n",
      "Episode ended naturally\n",
      "Reached required number of test episodes\n",
      "Training ended, it took 1894.17 seconds for 1 epochs with 1000 jets totalling 17325 steps\n"
     ]
    }
   ],
   "source": [
    "directory = '/beegfs/desy/user/rinckeph/work/Plots_training/1011/01/'\n",
    "# loop over training data\n",
    "steps_done = 0\n",
    "\n",
    "episode_lengths = []\n",
    "rewards = []\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i_epoch in range(epochs):\n",
    "    # go through training data once per epoch\n",
    "    episode_counter = 0\n",
    "    train_ntracks_used = []\n",
    "    train_vertex_error = []\n",
    "    train_pflags = []\n",
    "    train_steps = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        if X_train[i, 0, 0] == 0 or X_train[i, 1, 0] == 0:\n",
    "            continue\n",
    "        episode_counter += 1\n",
    "        #print(f\"Currentlyin  episode {episode_counter}\")\n",
    "        if episode_counter > num_episodes:\n",
    "            print(\"Reached required number of episodes\")\n",
    "            break\n",
    "        print(f\"Currently at event: {i}\" )\n",
    "        Env = VO.TrackEnvironment(X_train[i])\n",
    "        state = Env.state\n",
    "        if len(memory) > 100000:\n",
    "            print(\"Memory update\")\n",
    "            #l = len(memory)\n",
    "            #idx = np.random.choice(l, l,replace=False)\n",
    "            #memory = memory[idx]\n",
    "            memory_r = random.sample(memory, len(memory))\n",
    "            memory = memory_r[:50000]\n",
    "            \n",
    "        pflags_till_valid = -1\n",
    "        steps_till_done = 0\n",
    "        for t in count():\n",
    "            print(f\"Steps done: {steps_done}\")\n",
    "            true_SV = np.array([y_train[i,0], y_train[i,1], y_train[i,2]])\n",
    "            #print(f\"SV: {true_SV}\")\n",
    "            actions = select_action_DQN(state)\n",
    "            next_state, vertex_x, uncertainty, n, pflag, dflag = -1, -1, -1, -1, True, False\n",
    "            actions_index = 0\n",
    "            action = 0\n",
    "            while pflag:\n",
    "                pflags_till_valid += 1\n",
    "                action = actions[actions_index]\n",
    "                if t > max_episode_length:\n",
    "                    action = n_actions - 1\n",
    "                    print(\"Episode will have ended forcefully\")\n",
    "                next_state, vertex_x, uncertainty, n, pflag, dflag = Env.take_action(action)\n",
    "                actions_index += 1\n",
    "            train_pflags.append(pflags_till_valid)\n",
    "            steps_till_done += 1\n",
    "            #print(Env.vertex.track_indices)\n",
    "            #print(Env.state)\n",
    "            # set up reward, if a vertex can be computed, set it to change in displacement\n",
    "            reward = -10000 # if no vertex, this should be positive\n",
    "            if type(vertex_x) == np.ndarray:\n",
    "                reward = - np.sum((vertex_x-true_SV)**2)\n",
    "                if dflag:\n",
    "                    reward += 4\n",
    "            if pflag:\n",
    "                reward -= 10000\n",
    "\n",
    "            rewards.append(reward)\n",
    "            print(f\"Reward for action {action}: {reward}\")\n",
    "            memory.append((state, action, reward, next_state, dflag))\n",
    "            state = next_state\n",
    "            #if i < 10:\n",
    "             #   print(Env.state)\n",
    "            print(Env.vertex.track_indices)\n",
    "            optimise_model_memory(MINI_BATCH)\n",
    "            optimise_model_memory(MINI_BATCH)\n",
    "            #print(\"Did model update\")\n",
    "            if steps_done%TARGET_UPDATE == 0 and steps_done!= 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                gamma = GAMMA\n",
    "                print(\"Did target update\")\n",
    "            if dflag:\n",
    "                print(\"Done flag\")\n",
    "                train_steps.append(steps_till_done)\n",
    "                high_displacement = False\n",
    "                if type(vertex_x) == np.ndarray:\n",
    "                    if np.sum((vertex_x-true_SV)**2) > 36000:\n",
    "                        high_displacement = True\n",
    "                if i < 10 or high_displacement:\n",
    "                    print(\"Trying to plot\")\n",
    "                    error_str = \"no vertex\"\n",
    "                    if type(vertex_x) == np.ndarray:\n",
    "                        error_str = f\"{LA.norm(vertex_x-true_SV):.2f}\"\n",
    "                    uncer_str = \"uncertainty\"\n",
    "                    if uncertainty != None:\n",
    "                        uncer_str = f\"{uncertainty:.4f}\"\n",
    "                    textstr = str(dflag)+f\"\\nreward: {reward} \\nerror: {error_str} \\nuncertainty: {uncer_str}\"\n",
    "                    fig, ax = I.helices_plot(Env.track_data,\n",
    "                                 Env.vertex.track_indices, textstr=textstr, \n",
    "                                 pocas=Env.vertex.pocas, barrel=True,\n",
    "                                 reco_vertex=vertex_x,true_vertex=true_SV)\n",
    "                    fig.savefig(f\"{directory}helix_train_{i}_ep_{i_epoch}.pdf\")\n",
    "                    plt.close()\n",
    "                    print(\"Closed figure\")\n",
    "                if type(Env.vertex.x) == np.ndarray:\n",
    "                    train_vertex_error.append(LA.norm(Env.vertex.x-true_SV))\n",
    "                #print(\"Optional stuff done\")\n",
    "                train_ntracks_used.append(len(Env.vertex.track_indices))\n",
    "                episode_lengths.append(t)\n",
    "                if dflag:\n",
    "                    print(\"Episode ended naturally\")\n",
    "                else:\n",
    "                    print(\"Episode ended forcefully\")\n",
    "                #print(\"Break loop now\")\n",
    "                break\n",
    "    plt.hist(train_ntracks_used, bins=range(n_actions//2))\n",
    "    plt.xlabel(\"Num tracks used\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(\"Train sample\")\n",
    "    plt.savefig(f\"{directory}RL_train_ntracks_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "        \n",
    "    plt.hist(train_vertex_error)\n",
    "    plt.xlabel(\"Vertexing error\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(\"Train sample\")\n",
    "    plt.yscale('log')\n",
    "    plt.savefig(f\"{directory}RL_train_displacement_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.hist(train_pflags, bins=range(n_actions//2))\n",
    "    plt.xlabel(\"Number of attempts for valid action\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(\"Train sample\")\n",
    "    plt.savefig(f\"{directory}RL_train_pflags_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.hist(train_steps, bins=range(max_episode_length))\n",
    "    plt.xlabel(\"Number of steps before done\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(\"Train sample\")\n",
    "    plt.savefig(f\"{directory}RL_train_nsteps_epo{i_epoch}.pdf\")\n",
    "    plt.close()   \n",
    "\n",
    "    # test performance on test data once per episode (no random actions!)\n",
    "    episode_counter = 0\n",
    "    print(\"Should I do testing?\")\n",
    "    if not run_test:\n",
    "        print(\"No :/\")\n",
    "        break\n",
    "    print(\"Yes!\")\n",
    "    test_ntracks_used = []\n",
    "    test_vertex_error = []\n",
    "    test_decay_length = []\n",
    "    test_pflags = []\n",
    "    test_steps = []\n",
    "    print(\"Started testing\")\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if episode_counter > num_test_episodes:\n",
    "            print(\"Reached required number of test episodes\")\n",
    "            break\n",
    "        episode_counter += 1\n",
    "        Env = VO.TrackEnvironment(X_test[i])\n",
    "        state = Env.state\n",
    "        test_counter = 0\n",
    "        n_pflags = -1\n",
    "        n_steps = 0\n",
    "        for t in count():\n",
    "            steps_done += 1\n",
    "            state = state.flatten().to(device)\n",
    "            agent_out = policy_net(state)\n",
    "            actions = policy_net(state).argsort()\n",
    "            actions = [int(i) for i in actions]\n",
    "            if t == 0 or action == n_actions-1:\n",
    "                print(f\"Predicted value of all actions: {agent_out}\")    \n",
    "            next_state, vertex_x, uncertainty, n, pflag, dflag = -1, -1, -1, -1, True, False\n",
    "            actions_index = 0\n",
    "            action = 0\n",
    "            while pflag:\n",
    "                n_pflags += 1\n",
    "                action = actions[actions_index]\n",
    "                if t > max_episode_length:\n",
    "                    action = n_actions - 1\n",
    "                    print(\"Episode will have ended forcefully\")\n",
    "                next_state, vertex_x, uncertainty, n, pflag, dflag = Env.take_action(action)\n",
    "                actions_index += 1\n",
    "            test_pflags.append(n_pflags)\n",
    "            n_steps += 1\n",
    "            true_SV = np.array([y_test[i,0], y_test[i,1], y_test[i,2]])\n",
    "            test_decay_length.eppen(y_test[i,3])\n",
    "            if dflag:\n",
    "                print(f\"It took {t+1} steps in testing\")\n",
    "                test_steps.append(n_steps)\n",
    "                if i < 20:\n",
    "                    error_str = \"no vertex\"\n",
    "                    if type(vertex_x) == np.ndarray:\n",
    "                        error_str = f\"{LA.norm(vertex_x-true_SV):.2f}\"\n",
    "                    uncer_str = \"uncertainty\"\n",
    "                    if uncertainty != None:\n",
    "                        uncer_str = f\"{uncertainty:.4f}\"\n",
    "                    textstr = f\"\\nerror: {error_str} \\nuncertainty: {uncer_str}\"\n",
    "                    fig, ax = I.helices_plot(Env.track_data,\n",
    "                                 Env.vertex.track_indices,\n",
    "                                 textstr=textstr, \n",
    "                                 pocas=Env.vertex.pocas, barrel=True,\n",
    "                                 reco_vertex=vertex_x,true_vertex=true_SV)\n",
    "\n",
    "                    fig.savefig(f\"{directory}helix_test_{i}_ep_{i_epoch}.pdf\")\n",
    "                    plt.close()\n",
    "\n",
    "                if type(Env.vertex.x) == np.ndarray:\n",
    "                    test_vertex_error.append(LA.norm(Env.vertex.x-true_SV))\n",
    "                #else:\n",
    "                    #test_vertex_error.append(1000000)\n",
    "                test_ntracks_used.append(len(Env.vertex.track_indices))\n",
    "                if dflag:\n",
    "                    print(\"Episode ended naturally\")\n",
    "                else:\n",
    "                    print(\"Episode ended forcefully\")\n",
    "                break\n",
    "\n",
    "    plt.hist(test_ntracks_used, bins=range(n_actions//2))\n",
    "    plt.xlabel(\"Num tracks used\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(f\"Test sample, episode: {i_epoch}\")\n",
    "    plt.savefig(f\"{directory}RL_test_ntracks_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "        \n",
    "    plt.hist(test_vertex_error)\n",
    "    plt.xlabel(\"Vertexing error\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(f\"Test sample, episode: {i_epoch}\")\n",
    "    plt.yscale('log')\n",
    "    plt.savefig(f\"{directory}RL_test_error_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.hist(test_pflags, bins=range(n_actions//2))\n",
    "    plt.xlabel(\"Number of attepts till valid action\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(f\"Test sample, episode: {i_epoch}\")\n",
    "    plt.savefig(f\"{directory}RL_test_npflags_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "                      \n",
    "    plt.hist(test_steps, bins=range(max_episode_length))\n",
    "    plt.xlabel(\"Number of steps till done\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.title(f\"Test sample, episode: {i_epoch}\")\n",
    "    plt.savefig(f\"{directory}RL_test_nsteps_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.scatter(test_decay_length, test_vertex_error)\n",
    "    plt.xlabel(\"Decay length (cm)\")\n",
    "    plt.ylabel(\"Vertex error (cm)\")\n",
    "    plt.title(f\"Test sample, epoch: {i_epoch}\")\n",
    "    plt.savefig(f\"{directory}RL_test_decay_error_scatter_epo{i_epoch}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Training ended, it took {time.time()-start_time:.2f} seconds for {epochs} epochs with {num_episodes} jets totalling {steps_done} steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "4571\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(len(episode_lengths)),episode_lengths)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Actions\")\n",
    "plt.savefig(f\"{directory}RL_epilength.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(len(rewards)), rewards)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(f\"{directory}RL_rewards.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myanaconda]",
   "language": "python",
   "name": "conda-env-myanaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
